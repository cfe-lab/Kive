"""
copperfish.models
"""

from django.db import models;
from django.contrib.auth.models import User;
from django.contrib.contenttypes.models import ContentType;
from django.contrib.contenttypes import generic;
from django.db.models.signals import pre_save, post_save;
from django.dispatch import receiver;
from django.core.exceptions import ValidationError;
from django.core.validators import MinValueValidator;

import operator;		# Python math functions
import hashlib;			# To calculate MD5 hash

class Datatype(models.Model):
	"""
	Abstract definition of a semantically atomic type of data.
	Related to :model:`copperfish.CompoundDatatype`
	"""

	# Implicitly defined
	#   restricted_by (self/ManyToMany)
	#   compoundDatatypeMember_set (ForeignKey)

	name = models.CharField(max_length=64)

	# auto_now_add: set date_created to now on instantiation (editable=False)
	date_created = models.DateTimeField('Date created',
										auto_now_add = True)

	description = models.TextField()

	# Are we only using python interpreters? Not necesarilly, but we will be using
	# python to VALIDATE the data...
	Python_type = models.CharField(	'Python variable type',
									max_length=64,
									help_text="Python type, such as String, Int, Date");

	# Still todo: CHECK FOR CIRCULARLY DEFINED RESTRICTIONS (Into clean())
	restricts = models.ManyToManyField(	'self',
										symmetrical=False,
										related_name="restricted_by",
										null=True,
										blank=True,
										help_text="Captures hierarchical is-a classifications among Datatypes");

	# upload_to: path where file is stored
	verification_script = models.FileField(	"Verification script",
											upload_to='VerificationScripts',
											help_text="Validates inputs labelled as a DataType in actually being that DataType")

	def __unicode__(self):
		"""Describe a Datatype by it's name"""
		return self.name;

class CompoundDatatypeMember(models.Model):
	"""
	An actual data type member of a particular CompoundDatatype.
	Related to :model:`copperfish.Dataset`
	Related to :model:`copperfish.CompoundDatatype`
	"""

	compounddatatype = models.ForeignKey(	"CompoundDatatype",
											related_name="members",
											help_text="Links this DataType member to a particular CompoundDataType");

	datatype = models.ForeignKey(	Datatype,
									help_text="Specifies which DataType this member actually is");

	column_name = models.CharField(	"Column name",
									max_length=128,
									help_text="Gives datatype a name - removes the need for collumn indices");

	# MinValueValidator(1) constrains column_idx to be >= 1 (Else, raise ValidationError)
	column_idx = models.PositiveIntegerField(	validators=[MinValueValidator(1)]
												help_text="The column number of this DataType");

	# Define unique database indices to ensure uniqueness on particular tuples
	# A compoundDataType cannot have 2 of the same column name or column number
	class Meta:
		unique_together = 	(("compounddatatype", "column_name"),
							("compounddatatype", "column_idx"));

	# {} returns the variable in the order it was given
	def __unicode__(self):
		"""Describe a CompoundDatatypeMember with it's column number, datatype name, and column name"""
		return u"{}: <{}> [{}]".format(	self.column_idx,
										unicode(self.datatype),
										self.column_name);

class CompoundDatatype(models.Model):
	"""
	A structured collection of datatypes generated by, or necessary for, a Transformation.
	Related to :model:`copperfish.CompoundDatatypeMember`
	Related to :model:`copperfish.Dataset`
	"""

	# Implicitly defined:
	#   members (CompoundDatatypeMember/ForeignKey)
	#   Conforming_datasets (Dataset/ForeignKey)

	def __unicode__(self):
		""" Create an ordered string representation of this CompoundDatatype's members """

		string_rep = u"(";
		all_members = self.members.all();

		# Get the column number for each DataType member in this CompoundDatatype
		member_indices = [member.column_idx for member in all_members];

		# len() returns the number of objects, range(x) returns a sequence from 0 to x
		# So, get the column index of each Datatype member, along with the Datatype member itself
		members_with_indices = [ (member_indices[i], all_members[i]) for i in range(len(all_members))];

		# Sort Datatype members using the index as a basis (Specified by operator.itemgetter(0))
		members_with_indices = sorted(	members_with_indices,
										key=operator.itemgetter(0));

		# Add the unicode of the Datatype member to the string representation
		for i, colIdx_and_member in enumerate(members_with_indices):
			colIdx, member = colIdx_and_member;
			string_rep += unicode(member);

			# Add a comma if we are not at the end of the list
			if i != len(members_with_indices) - 1:
				string_rep += ", ";

		string_rep += ")";
		return string_rep;

	# Model.clean() is executed prior to save() to validate model contents
	def clean(self):
		"""Check if Datatype members have consecutive indices from 1 to n"""
		column_indices = [];

		# += is shorthand for extend() - concatenate a list with another list
		for member in self.members.all():
			column_indices += [member.column_idx];

		# Check if the sorted list is exactly a sequence from 1 to n
		if sorted(column_indices) != range(1, self.members.count()+1):
			raise ValidationError("Column indices are not consecutive starting from 1");


class Dataset(models.Model):
	"""
	Datasets uploaded by users.
	Related to :model:`copperfish.PipelineStep`
	Related to :model:`copperfish.CompoundDatatype`
	"""

	# Implicitly defined
	#   descendent_datasets (self/ManyToMany)

	# Activating the admin panel creates a Users model
	user = models.ForeignKey(	User,
								help_text="User that uploaded this dataset.");

	name = models.CharField(	"Dataset name",
								max_length=128,
								help_text="Description of this dataset.");

	description = models.TextField("Dataset description");

	date_created = models.DateTimeField(	auto_now_add=True,
											help_text="Date of dataset upload.");
	
	# What Pipeline step did this Dataset come from, and from which output?
	# Does null or blank mean that it came from no Pipeline? (WHICH ONE IS IT?) [Both needed for Foreign key]


	# Null - if a Dataset was uploaded but not created...
	pipeline_step = models.ForeignKey(	"PipelineStep",
									  	related_name="data_produced",
									  	null=True,
										blank=True);

	# Within a pipeline step, which output is it?
	pipeline_step_output_name = models.CharField(	"???",
													max_length=128,
													blank=True,
													help_text="???");

	# Datasets are a compound data type
	compounddatatype = models.ForeignKey(	CompoundDatatype,
											related_name="conforming_datasets");

	# If the dataset is derived from a transformation, it comes from precursor datasets
	parent_datasets = models.ManyToManyField(	'self',
												related_name="descendent_datasets",
												null=True,
												blank=True);

	dataset_file = models.FileField(upload_to="Datasets",
									help_text="Path where datasets are stored");

	MD5_checksum = models.CharField(	max_length=64,
										help_text="Used to check data integrity");


	def __unicode__(self):
		"""Display the Dataset name, user, and date created."""
		return "{} (created by {} on {})".format(	self.name,
												 	unicode(self.user),
												 	self.date_created);

	# Before saving, generate the MD5 hash for this Dataset
	def clean(self):
		"""If a file specified, populate the MD5 checksum."""

		try:
			md5gen = hashlib.md5();
			md5gen.update(self.dataset_file.read());
			self.MD5_checksum = md5gen.hexdigest();
		except ValueError as e:
			print(e);
			print("No file found; setting MD5 checksum to the empty string.");
			self.MD5_checksum = "";
	

class CodeResource(models.Model):
	"""
	A CodeResource is any file tracked by ShipYard.
	Related to :model:`copperfish.CodeResourceRevision`
	"""

	# Implicitly defined
	#   revisions (codeResourceRevision/ForeignKey)

	name = models.CharField("Resource name",
							max_length=128,
							help_text="A name for this resource");

	description = models.TextField("Resource description");

	def __unicode__(self):
		return self.name;

class CodeResourceRevision(models.Model):
	"""
	A particular revision of a code resource.

	Related to :model:`copperfish.CodeResource`
	Related to :model:`copperfish.CodeResourceDependency`
	"""

	# Implicitly defined
	#   descendents (self/ForeignKey)
	#   dependencies (CodeResourceDependency/ForeignKey)
	#   needed_by (CodeResourceDependency/ForeignKey)
	#   method_set (Method/ForeignKey) ???

	coderesource = models.ForeignKey(
			CodeResource,
			related_name="revisions");	
		
	revision_name = models.CharField(
			max_length=128,
			help_text="Identifier for the particular revision of this resource");

	revision_DateTime = models.DateTimeField(
			auto_now_add=True,
			help_text="Date this resource revision was uploaded");

	revision_parent = models.ForeignKey('self',
										related_name="descendants",
										null=True,
										blank=True);

	revision_desc = models.TextField(
			"Revision description",
			help_text="A description for this particular resource revision");

	content_file = models.FileField(
			"File contents",
			upload_to="CodeResources",
			null=True,
			blank=True,
			help_text="The actual contents of this code resource revision");

	MD5_checksum = models.CharField(
			max_length=64,
			blank=True,
			help_text="Used to validate the contents of this resource revision");

	def __unicode__(self):
		"""Represent with CodeResource and CodeResourceRevision name"""
		
		# This CodeResourceRevision may have no coderesource yet
		# Admin page interface... will cause a CodeResource to be created without being save()ed
		# and then a code resource REVISION will be instantiated (but not save()ed) ...
		if not hasattr(self, "coderesource"):
			return u"[no code resource set] {}".format(self.revision_name);
		
		string_rep = self.coderesource.name + u" " + self.revision_name;
		return string_rep;

	# Why wouldn't a file be specified?????? - this could be a resource without an actual
	# file, just dependencies....
	def clean(self):
		"""If there is a file specified, fill in the MD5 checksum."""
		try:
			md5gen = hashlib.md5();
			md5gen.update(self.content_file.read());
			self.MD5_checksum = md5gen.hexdigest();
		except ValueError as e:
			#print(e);
			#print("No file found; setting MD5 checksum to the empty string.");
			self.MD5_checksum = "";

class CodeResourceDependency(models.Model):
	"""
	Dependencies of a CodeResource - themselves also CodeResources.

	Related to :model:`copperfish.CodeResource`
	"""

	coderesourcerevision = models.ForeignKey(CodeResourceRevision,
											 related_name="dependencies");

	# The dependency is defined by a particular code resource revision
	requirement = models.ForeignKey(CodeResourceRevision,
	                                related_name="needed_by");
	
	# Where to put it (relative to the sandbox).
	# FIXME: should we use a FilePathField?
	where = models.CharField(
			max_length=100,
			help_text="Where a code resource dependency must exist - relative to the sandbox??? or the code resource?");

	def __unicode__(self):
		"""Represent as [x] requires [y] as [z]."""
		return u"{} requires {} as {}".format(
				unicode(self.coderesourcerevision),
				unicode(self.requirement),
				self.where);

# General description of Method and Pipeline familiies
class TransformationFamily(models.Model):
	"""
	TransformationFamily is an abstract class that describes
	what is common between MethodFamily and PipelineFamily.

	Extends :model:`copperfish.MethodFamily`
	Extends :model:`copperfish.PipelineFamily`
	"""

	name = models.CharField(
		"Transformation family name",
		max_length=128);

	description = models.TextField(
		"Transformation family description"
		);

	def __unicode__(self):
		""" Describe a transformation family with it's name """
		return self.name;

	class Meta:
		abstract = True;

class MethodFamily(TransformationFamily):
	"""
	MethodFamily groups revisions of Methods together.

	Inherits :model:`copperfish.TransformationFamily`
	Related to :model:`copperfish.Method`
	"""

	# Implicitly defined:
	#   members (Method/ForeignKey)

	pass

class PipelineFamily(TransformationFamily):
	"""
	PipelineFamily groups revisions of Pipelines together.

	Inherits :model:`copperfish.TransformationFamily`
	Related to :model:`copperfish.Pipeline`
	"""

	# Implicitly defined:
	#   members (Pipeline/ForeignKey)

	pass


class Transformation(models.Model):
	"""
	Abstract class that defines what is common of transformation
	revisions (ie, Method and Pipeline revisions)

	Extends :model:`copperfish.Method`
	Extends :model:`copperfish.Pipeline`
	"""

	revision_name = models.CharField(max_length=128);

	revision_DateTime = models.DateTimeField(
			"Revision creation date",
			auto_now_add = True);

	revision_desc = models.TextField(
			"Transformation revision description",
			help_text="Description of this transformation revision");

	# A TransformationInput is associated with a method/pipeline
	# Using GenericForeignKey in TransformationXput
	# A transformation can access it's inputs/outputs via GenericRelation
	inputs = generic.GenericRelation("TransformationInput");
	outputs = generic.GenericRelation("TransformationOutput");

	class Meta:
		abstract = True;

	def check_input_indices(self):
		"""Check that inputs are numbered consecutively from 1"""

		# Start with an empty list
		input_nums = [];

		# For each tranformation input, append dataset index to the list
		for curr_input in self.inputs.all():
			input_nums += [curr_input.dataset_idx];

		# If list is not exactly a sequence from 1 to n, raise an error
		if sorted(input_nums) != range(1, self.inputs.count()+1):
			raise ValidationError(
					"Inputs are not consecutively numbered starting from 1");
		
	def check_output_indices(self):
		"""Check that outputs are numbered consecutively from 1"""

		# Start with an empty list
		output_nums = [];

		# For each tranformation input, append output index to the list
		for curr_output in self.outputs.all():
			output_nums += [curr_output.dataset_idx];

		# If list is not exactly a sequence from 1 to n, raise an error
		if sorted(output_nums) != range(1, self.outputs.count()+1):
			raise ValidationError(
					"Outputs are not consecutively numbered starting from 1");

	def clean(self):
		"""Validate transformation inputs and outputs."""
		self.check_input_indices();
		self.check_output_indices();

class Method(Transformation):
	"""
	Methods are atomic transformations.

	Inherits from :model:`copperfish.Transformation`
	Related to :model:`copperfish.CodeResource`
	Related to :model:`copperfish.MethodFamily`
	"""

	# Implicitly defined:
	#   descendants (self/ForeignKey)

	family = models.ForeignKey(	MethodFamily,
								related_name="members");

	revision_parent = models.ForeignKey("self",
										related_name = "descendants",
										null=True,
										blank=True);

	# CodeResourceRevision must be executable
	driver = models.ForeignKey(CodeResourceRevision);

	def __unicode__(self):
		"""Represent a method by it's revision name and method family"""
		string_rep = u"Method {} {}".format("{}", self.revision_name);

		# Family may not be set (If created from the family admin page)
		if hasattr(self, "family"):
			string_rep = string_rep.format(unicode(self.family));
		else:
			string_rep = string_rep.format("[family unset]");
		return string_rep;


	def save(self, *args, **kwargs):
		"""
		Create or update a method revision. If method has no associated
		inputs or outputs, but has a parent, copy parent input/outputs.

		There are two states when invoking save()
		1) First time creating the method revision
		2) Updating a method revision

		When creating a method revision the first time, a standard
		save() must be called first: inputs/outputs cannot be registered
		unless this method revision has been entered into the database.	
		"""

		# Execute original save()
		super(Method, self).save(*args, **kwargs);

		# If no parent revision of this method exists, there's nothing to do
		if self.revision_parent == None:
			return None;

		# If parent revision exists, and inputs/outputs haven't been registered
		if self.inputs.count() + self.outputs.count() == 0:

			# Copy all inputs from the parent revision into this revision
			for parent_input in self.revision_parent.inputs.all():
				self.inputs.create(
						compounddatatype = parent_input.compounddatatype,
						dataset_name = parent_input.dataset_name,
						dataset_idx = parent_input.dataset_idx,
						min_row = parent_input.min_row,
						max_row = parent_input.max_row);

			# Copy all outputs from the parent revision into this revision
			for parent_output in self.revision_parent.outputs.all():
				self.outputs.create(
						compounddatatype = parent_output.compounddatatype,
						dataset_name = parent_output.dataset_name,
						dataset_idx = parent_output.dataset_idx,
						min_row = parent_output.min_row,
						max_row = parent_output.max_row);
				

class Pipeline(Transformation):
	"""
	A particular pipeline revision. (MORE DETAIL NEEDED)

	Inherits from :model:`copperfish.Transformation`
	Related to :model:`copperfish.PipelineFamily`
	"""

	# Implicitly defined
	#   steps (PipelineStep/ForeignKey)
	#   descendants (self/ForeignKey)
	#   outmap (PipelineOutputMapping/ForeignKey)

	family = models.ForeignKey(	PipelineFamily,
								related_name="members");	

	revision_parent = models.ForeignKey("self",
										related_name = "descendants",
										null=True,
										blank=True);

	# When defining a pipeline, we don't define the outputs; we define
	# outmap instead and during the clean stage the outputs are created.
	
	# outmap describes where a given terminal pipeline output of a pipeline comes from...
	# with respect to its own steps' outputs

	def __unicode__(self):
		"""Represent pipeline by revision name and pipeline family"""

		string_rep = u"Pipeline {} {}".format("{}", self.revision_name);

		# If family isn't set (if created from family admin page)
		if hasattr(self, "family"):
			string_rep = string_rep.format(unicode(self.family));
		else:
			string_rep = string_rep.format("[family unset]");

		return string_rep;

	def clean(self):
		"""
		Validate pipeline revision inputs/outputs

		1) Pipeline STEPS must be consecutively starting from 1
		2) Pipeline INPUTS must be consecutively numbered from 1
		3) Inputs are available at a needed step and of the type expected
		4) Outputs of the pipeline will be mapped to outputs generated by its steps (???)
		"""

		# Check that inputs are numbered consecutively from 1
		# We don't care about the outputs, but if they are set, check them

		# Call Transformation (the superclass of Pipeline)'s clean()
		super(Pipeline, self).clean();

		# 1) Validate pipeline step numbering
		all_steps = self.steps.all();
		step_nums = [];

		for step in all_steps:
			step_nums += [step.step_num];

		# If it is not exactly a sequence from 1 to n, raise error
		if sorted(step_nums) != range(1, len(all_steps)+1):
			raise ValidationError(
					"Steps are not consecutively numbered starting from 1");

		# Check that steps are coherent with each other
		# Is the inputs of each step... are A) available (produced by a previous step, not delete OR an absolute input)
		# B) Of the correct CompoundDatatype (There are also constraints on min/max rows that get checked...)

		# a) In Pipeline, steps is implicitly defined by PipelineStep
		# b) In PipelineStep, inputs is implicitly defined by PipelineStepInput
		
		# For each Pipeline step
 		for step in all_steps:

			# For each input for that step, extract the input parameters
			for curr_in in step.inputs.all():
				input_requested = curr_in.provider_output_name;		# What name of the pipeline step's output
				requested_from = curr_in.step_providing_input;		# What step the data should come from
				feed_to_input = curr_in.transf_input_name;			# Where the data should go

				# Find the requested input; raise ValidationError on failure (???)
				req_input = None;

				# If this is an initial pipeline input...
				if requested_from == 0:

					# self is Pipeline, a transformation, which has inputs
					# So get it from an initial pipeline input (dataset_name)
					try:
						req_input = self.inputs.get(
								dataset_name=input_requested);

					except TransformationInput.DoesNotExist as e:
						raise ValidationError(
								"Pipeline does not have input \"{}\"".
								format(input_requested));	

				# Otherwise, the input comes from a previous pipeline step
				# (Which is why are are looking at OUTPUTS)
				else:
					providing_step = all_steps[requested_from-1];
					try:
						req_input = providing_step.transformation.outputs.get(
								dataset_name=input_requested);

					except TransformationOutput.DoesNotExist as e:
						raise ValidationError(
								"Transformation at step {} does not produce output \"{}\"".
								format(requested_from, input_requested));
						
					# Was this dataset deleted?
					if providing_step.outputs_to_delete.filter(
							dataset_to_delete=input_requested).count() != 0:
						raise ValidationError(
								"Input \"{}\" from step {} to step {} is deleted prior to request".
								format(input_requested, requested_from,
									   step.step_num));

				# Check that the requested input matches the expected prototype.
				# Note: we don't check for ValidationError because this was
				# already checked in the clean() step of PipelineStep.
				transf_input = step.transformation.inputs.get(dataset_name=feed_to_input);

				# FIXME: we're just going to enforce that transf_input
				# and req_input have the same CompoundDatatype, rather
				# than making sure that their CompoundDatatypes match;
				# is this too restrictive?				
				if req_input.compounddatatype != transf_input.compounddatatype:
					raise ValidationError(
							"Data fed to input \"{}\" of step {} does not have the expected CompoundDatatype".
							format(feed_to_input, step.step_num));

				provided_min_row = 0;
				required_min_row = 0;
				if req_input.min_row != None:
					providing_min_row = req_input.min_row;
				if transf_input.min_row != None:
					required_min_row = transf_input.min_row;
				if (provided_min_row < required_min_row):
					raise ValidationError(
							"Data fed to input \"{}\" of step {} may have too few rows".
							format(feed_to_input, step.step_num));
				
				provided_max_row = float("inf");
				required_max_row = float("inf");
				if req_input.max_row != None:
					providing_max_row = req_input.max_row;
				if transf_input.max_row != None:
					required_max_row = transf_input.max_row;
				if (provided_max_row > required_max_row):
					raise ValidationError(
							"Data fed to input \"{}\" of step {} may have too many rows".
							format(feed_to_input, step.step_num));

		# Check the output mappings, making sure the wiring is coherent.
		output_indices = [];
		for mapping in self.outmap.all():
			output_requested = mapping.provider_output_name;
			requested_from = mapping.step_providing_output;
			connect_to_output = mapping.output_name;
			output_indices += [mapping.output_idx];

			# Is the step number valid?
			if requested_from > len(all_steps):
				raise ValidationError(
						"Output requested from a non-existent step");	
			
			providing_step = all_steps[requested_from-1];
			req_output = None;
			try:
				req_output = providing_step.transformation.outputs.get(
						dataset_name=output_requested);
			except TransformationOutput.DoesNotExist as e:
				raise ValidationError(
						"Transformation at step {} does not produce output \"{}\"".
						format(requested_from, output_requested));

			# Was this output deleted by the step producing it?
			if providing_step.outputs_to_delete.filter(
					dataset_to_delete=output_requested).count() != 0:
				raise ValidationError(
						"Output \"{}\" from step {} is deleted prior to request".
						format(output_requested, requested_from));

		if sorted(output_indices) != range(1, self.outmap.count()+1):
			raise ValidationError(
					"Outputs are not consecutively numbered starting from 1");

			

	def save(self, *args, **kwargs):
		"""
		When saving, set up outputs as specified.

		This must be done after saving, because otherwise the manager for
		the calling instance's outputs will not have been set up.
		"""
		super(Pipeline, self).save(*args, **kwargs);

		# Delete existing outputs (if we ever customize delete()
		# of TransformationOutput we'll need to change this)

		self.outputs.all().delete();
		all_steps = self.steps.all();

		# And then recreate the outputs

		# What is outmap accomplishing????????
		# outmap (PipelineOutputMapping/ForeignKey)
 		for mapping in self.outmap.all():
			output_requested = mapping.provider_output_name;
			requested_from = mapping.step_providing_output;
			connect_to_output = mapping.output_name;
			
			providing_step = all_steps[requested_from-1];
			req_output = providing_step.transformation.outputs.get(
					dataset_name=output_requested);
				
			self.outputs.create(compounddatatype=req_output.compounddatatype,
								dataset_name=connect_to_output,
								dataset_idx=mapping.output_idx,
								min_row=req_output.min_row,
								max_row=req_output.max_row);

 			

class PipelineStep(models.Model):
	"""
	A particular step within an execution....??????

	Related to :model:`copperfish.Pipeline`
	"""

	# Implicitly defined
	#   inputs (PipelineStepInput/ForeignKey)
	#   outputs_to_delete: from PipelineStepDelete

	pipeline = models.ForeignKey(	Pipeline,
									related_name="steps");

	# Pipeline steps are associated with a method or pipeline (WHY???) [names must be lower-case]
	content_type = models.ForeignKey(	ContentType,
										limit_choices_to = {"model__in": ("method", "pipeline")});

	object_id = models.PositiveIntegerField();
	transformation = generic.GenericForeignKey("content_type", "object_id");
	step_num = models.PositiveIntegerField(validators=[MinValueValidator(1)]);
	
	def __unicode__(self):
		""" Represent with the pipeline and step number """

		pipeline_name = "[no pipeline assigned]";	
		if hasattr(self, "pipeline"):
			pipeline_name = unicode(self.pipeline);
		return "{} step {}".format(pipeline_name, self.step_num);

	def recursive_pipeline_check(self, pipeline):
		"""Check if the specified pipeline occurs within this step."""

		contains_pipeline = False;

		# Base case 1: the transformation is a method.
		if type(self.transformation) == Method:
			contains_pipeline = False;

		# Base case 2: the transformation equals the pipeline.

		# WHY ARE WE NOT CALLING TYPE() HERE???
		elif self.transformation == pipeline:
			contains_pipeline = True;

		# Recursive case: go through all of the pipeline steps.
		else:
			transf_steps = self.transformation.steps.all();
			for step in transf_steps:
				step_contains_pipeline = step.recursive_pipeline_check(pipeline);
				if step_contains_pipeline:
					contains_pipeline = True;
		return contains_pipeline;

	def clean(self):
		"""
		Check coherence of this step of the pipeline.

		1) Do inputs come from prior steps?
		2) Do inputs map correctly to the transformation at this step?
		3) Do outputs marked for deletion come from this transformation?
		4) Does the transformation at this step contain the parent pipeline?
		"""

		# Check recursively to see if this step's transformation contains
		# the specified pipeline at all.
		if self.recursive_pipeline_check(self.pipeline):
			raise ValidationError("Step {} contains the parent pipeline".
								  format(self.step_num));
 			
		for curr_in in self.inputs.all():
			input_requested = curr_in.provider_output_name;
			requested_from = curr_in.step_providing_input;
			feed_to_input = curr_in.transf_input_name;
				
			# Does this input come from a step prior to this one?
			if requested_from >= self.step_num:
				raise ValidationError(
						"Input \"{}\" to step {} does not come from a prior step".
						format(input_requested, self.step_num));

			# Does the transformation at this step have an input named
			# feed_to_input?
			try:
				self.transformation.inputs.get(dataset_name=feed_to_input);
			except TransformationInput.DoesNotExist as e:
				raise ValidationError ("Transformation at step {} has no input named \"{}\"".
						format(self.step_num, feed_to_input));
 
		for curr_del in self.outputs_to_delete.all():
			to_del = curr_del.dataset_to_delete;

			# Check that to_del is one of the outputs of the current step's
			# Transformation.
			if self.transformation.outputs.\
				filter(dataset_name=to_del).count() == 0:
				raise ValidationError(
						"Transformation at step {} has no output named \"{}\"".
						format(self.step_num, to_del));


class PipelineStepInput(models.Model):
	"""
	Represents the "wires" feeding into a pipeline step.
	"""

	pipelinestep = models.ForeignKey(	PipelineStep,
										related_name = "inputs");

	# The name of the input hole
	# This comes from the name attached to TransformationInput
	# (dataset_name)
	# This might be changed to a ForeignKey to TransformationInput
	transf_input_name = models.CharField(	"Transformation input name",
											max_length=128,
											help_text="The name of the input hole (FIXME)");

	# PRE: step_providing_input < the step number of the PipelineStep this
	# input goes into
	# The coherence of the data here will be enforced at the Python level
	# (i.e. does this actually refer to a Dataset produced by the
	# Transformation at the specified step, etc.)
	step_providing_input = models.PositiveIntegerField();
	provider_output_name = models.CharField(max_length=128);

	def __unicode__(self):
		step_str = "[no pipeline step set]";
		if self.pipelinestep != None:
			step_str = unicode(self.pipelinestep);
		return "{}:{}".format(step_str, self.transf_input_name);	


class PipelineStepDelete(models.Model):
	pipelinestep = models.ForeignKey(PipelineStep,
	                                 related_name="outputs_to_delete");

	# Again, the coherence of this data will be enforced at the Python level
	# (i.e. does this actually refer to a Dataset that will be produced
	# by the Transformation at this step)
	dataset_to_delete = models.CharField(max_length=128);


class PipelineOutputMapping(models.Model):
	"""Specifies mapping of PipelineStep outputs to Pipeline outputs."""

	pipeline = models.ForeignKey(	Pipeline,
									related_name="outmap");

	output_name = models.CharField(max_length=128);
	output_idx = models.PositiveIntegerField(validators=[MinValueValidator(1)]);

	# PRE: step_providing_output is an actual step of the pipeline
	# and provider_output_name actually refers to one of the outputs
	# at that step
	# The coherence of the data here will be enforced at the Python level
	step_providing_output = models.PositiveIntegerField(validators=[MinValueValidator(1)]);
	provider_output_name = models.CharField(max_length=128);

	def __unicode__(self):
		pipeline_name = "[no pipeline set]";
		if self.pipeline != None:
			pipeline_name = unicode(self.pipeline);

		return "{}:{} ({})".format(pipeline_name, self.output_idx,
								   self.output_name);


class TransformationXput(models.Model):
	"""
	Describes parameters common to all inputs
	and outputs of pipeline steps.

	Extends :model:`copperfish.TransformationInput`
	Extends :model:`copperfish.TransformationOutput`
	"""

	# TransformationXput is associated with methods and pipelines
	content_type = models.ForeignKey(
			ContentType,
			limit_choices_to = {"model__in": ("method", "pipeline")});
	object_id = models.PositiveIntegerField();
	transformation = generic.GenericForeignKey("content_type", "object_id");

	# This is either the input or the output
	compounddatatype = models.ForeignKey(CompoundDatatype);

	# Why does a transformation input/output need a dataset name?
	# Or did we really mean the "name" of the input/output?
	# The name of the "hole"
	dataset_name = models.CharField(max_length=128);

	# Is this the index on the transformation for this input/output?
	dataset_idx = models.PositiveIntegerField(validators=[MinValueValidator(1)]);
	
	# Nullable fields indicating that this dataset has
	# restrictions on how many rows it can have
	min_row = models.PositiveIntegerField(null=True, blank=True);
	max_row = models.PositiveIntegerField(null=True, blank=True);

	class Meta:
		abstract = True;

		unique_together = (("content_type", "object_id", "dataset_name"),
						   ("content_type", "object_id", "dataset_idx"));

	def __unicode__(self):
		return u"[{}]:{} {} {}".format(unicode(self.transformation),
									   self.dataset_idx,
									   unicode(self.compounddatatype),
									   self.dataset_name);

class TransformationInput(TransformationXput):
	"""
	Inherits from :model:`copperfish.TransformationXput`
	"""

	# Implicitly defined:
	#   transformations (MapTransformationToInput - ?????????)
	pass

class TransformationOutput(TransformationXput):
	"""
	Inherits from :model:`copperfish.TransformationXput`
	"""

	# Implicitly defined:
	#   transformations (MapTransformationToOutput - ?????????)
	pass
