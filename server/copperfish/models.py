"""
copperfish.models

Data model for the Shipyard (Copperfish) project - open source software
that performs revision control on datasets and bioinformatic pipelines.
"""

from django.db import models
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.contrib.contenttypes import generic
from django.db.models.signals import pre_save, post_save
from django.dispatch import receiver
from django.core.exceptions import ValidationError
from django.core.validators import MinValueValidator
from django.db import transaction

# Python math functions
import operator
# To calculate MD5 hash
import hashlib
# Regular expressions
import re
# Augments regular expressions
import string
# For checking file paths
import os.path
import sys
import csv

class Datatype(models.Model):
    """
    Abstract definition of a semantically atomic type of data.
    Related to :model:`copperfish.CompoundDatatype`
    """

    # Implicitly defined
    #   restricted_by (self/ManyToMany)
    #   compoundDatatypeMember_set (ForeignKey)

    name = models.CharField(
        "Datatype name",
        max_length=64,
        help_text="The name for this DataType");

    # auto_now_add: set to now on instantiation (editable=False)
    date_created = models.DateTimeField(
        'Date created',
        auto_now_add = True,
        help_text="Date Datatype was defined");

    description = models.TextField(
        "Datatype description",
        help_text="A description for this DataType");

    # Datatypes aren't always generated by python but are VALIDATED
    # with Python.
    # FIXME: do we need this or is this just unnecessary
    # repetition since we have a verification script that will do the
    # requisite checking?
    Python_type = models.CharField(
        'Python variable type',
        max_length=64,
        help_text="Python type, such as String, Int, Date");

    # FIXME: Check for circularly defined restrictions -- write a clean() method
    restricts = models.ManyToManyField(
        'self',
        symmetrical=False,
        related_name="restricted_by",
        null=True,
        blank=True,
        help_text="Captures hierarchical is-a classifications among Datatypes");

    verification_script = models.FileField(
        "Verification script",
        upload_to='VerificationScripts',
        help_text="Used to validate correctness of fields labelled as being this DataType");

    
    def is_restricted_by(self, possible_restrictor_datatype):
        """
        Determine if this datatype is ever *properly* restricted, directly or indirectly,
        by a given datatype.
        
        PRE: there is no circular restriction in the possible restrictor
        datatype (this would cause an infinite recursion).
        """
        # The default is that self is not restricted by
        # possible_restrictor_datatype; toggle to True if it turns out
        # that it is.
        is_restricted = False
        restrictions = possible_restrictor_datatype.restricts.all()

        for restrictedDataType in restrictions:

            # Case 1: If restrictions restrict self, return true
            if restrictedDataType == self:
                is_restricted = True

            # Case 2: Check if any restricted Datatypes themselves restrict self
            else:
                theValue = self.is_restricted_by(restrictedDataType)

                # If any restricted Datatypes themselves restrict self, propagate
                # this information to the parent Datatype as restricting self
                if theValue == True:
                    is_restricted = True

        # Return False if Case 1 is never encountered
        return is_restricted

    # FIXME: when we start with execute, we'll use this to test
    # whether a specified column in a CSV file conforms to this
    # Datatype.
    #def check_CSV_column(csv_file, col_num):
    #    pass
    # We could also do it with the following:
    def check_conforming(csv_entry):
        pass

    def clean(self):
        if hasattr(self, "restricts") and self.is_restricted_by(self):
            raise ValidationError("Circular Datatype restriction detected");

    def __unicode__(self):
        """Describe Datatype by name"""
        return self.name;

class CompoundDatatypeMember(models.Model):
    """
    A data type member of a particular CompoundDatatype.
    Related to :model:`copperfish.Dataset`
    Related to :model:`copperfish.CompoundDatatype`
    """

    compounddatatype = models.ForeignKey(
            "CompoundDatatype",
            related_name="members",
            help_text="Links this DataType member to a particular CompoundDataType");

    datatype = models.ForeignKey(
            Datatype,
            help_text="Specifies which DataType this member is");

    column_name = models.CharField(
            "Column name",
            max_length=128,
            help_text="Gives datatype a 'collumn name' as an alternative to collumn index");

    # MinValueValidator(1) constrains column_idx to be >= 1
    column_idx = models.PositiveIntegerField(
            validators=[MinValueValidator(1)],
            help_text="The column number of this DataType");

    # Define database indexing rules to ensure tuple uniqueness
    # A compoundDataType cannot have 2 member definitions with the same column name or column number
    class Meta:
        unique_together = (("compounddatatype", "column_name"),
                           ("compounddatatype", "column_idx"));

    def __unicode__(self):
        """Describe a CompoundDatatypeMember with it's column number, datatype name, and column name"""

        returnString = u"{}: <{}> [{}]".format( self.column_idx,
                                                unicode(self.datatype),
                                                self.column_name);

        return returnString

class CompoundDatatype(models.Model):
    """
    A definition of a structured collection of datatypes,
    the resultant data structure serving as inputs or outputs
    for a Transformation.

    Related to :model:`copperfish.CompoundDatatypeMember`
    Related to :model:`copperfish.Dataset`
    """

    # Implicitly defined:
    #   members (CompoundDatatypeMember/ForeignKey)
    #   Conforming_datasets (Dataset/ForeignKey)

    def __unicode__(self):
        """ Represent CompoundDatatype with a list of it's members """

        string_rep = u"(";

        # Get the members for this compound data type
        all_members = self.members.all();

        # A) Get the column index for each member
        member_indices = [member.column_idx for member in all_members];

        # B) Get the column index of each Datatype member, along with the Datatype member itself
        members_with_indices = [ (member_indices[i], all_members[i]) for i in range(len(all_members))];
        # Can we do this?
        # members_with_indices = [ (all_members[i].column_idx, all_members[i])
        #                          for i in range(len(all_members))];

        # Sort members using column index as a basis (operator.itemgetter(0))
        members_with_indices = sorted(  members_with_indices,
                                        key=operator.itemgetter(0));

        # Add sorted Datatype members to the string representation
        for i, colIdx_and_member in enumerate(members_with_indices):
            colIdx, member = colIdx_and_member;
            string_rep += unicode(member);

            # Add comma if not at the end of member list
            if i != len(members_with_indices) - 1:
                string_rep += ", ";

        string_rep += ")";

        if string_rep == "()":
            string_rep = "[empty CompoundDatatype]";

        return string_rep;

    # clean() is executed prior to save() to perform model validation
    def clean(self):
        """Check if Datatype members have consecutive indices from 1 to n"""
        column_indices = [];

        # += is shorthand for extend() - concatenate a list with another list
        for member in self.members.all():
            column_indices += [member.column_idx];

        # Check if the sorted list is exactly a sequence from 1 to n
        if sorted(column_indices) != range(1, self.members.count()+1):
            raise ValidationError("Column indices are not consecutive starting from 1");

 
class CodeResource(models.Model):
    """
    A CodeResource is any file tracked by ShipYard.
    Related to :model:`copperfish.CodeResourceRevision`
    """

    # Implicitly defined
    #   revisions (codeResourceRevision/ForeignKey)

    name = models.CharField(
            "Resource name",
            max_length=255,
            help_text="The name for this resource");

    filename = models.CharField(
            "Resource file name",
            max_length=255,
            help_text="The filename for this resource",
            blank=True);

    description = models.TextField("Resource description");

    def isValidFileName(self):

        # Code resources have no filenames if they are a meta-package of dependencies
        if self.filename == "":
            return True
    
        # File names cannot start with 1 or more spaces
        if re.search("^\s+", self.filename):
            return False

        # Names cannot end with 1 or more trailing spaces
        if re.search("\s+$", self.filename):
            return False

        # Names must be 1 or more of any from {alphanumeric, space, "-._()"}
        # This will prevent "../" as it contains a slash
        regex = "^[-_.() {}{}]+$".format(string.ascii_letters, string.digits)
        if re.search(regex, self.filename):
            pass
        else:
            return False

        return True

    def clean(self):
        """
        CodeResource name must be valid.

        It must not contain a leading space character or "..",
        must not end in space, and be composed of letters,
        numbers, dash, underscore, paranthesis, and space.
        """
        
        if self.isValidFileName():
            pass
        else:
            raise ValidationError("Invalid code resource filename");


    def __unicode__(self):
        return self.name;
    

class CodeResourceRevision(models.Model):
    """
    A particular revision of a code resource.

    Related to :model:`copperfish.CodeResource`
    Related to :model:`copperfish.CodeResourceDependency`
    Related to :model:`copperfish.Method`
    """

    # Implicitly defined
    #   descendents (self/ForeignKey)
    #   dependencies (CodeResourceDependency/ForeignKey)
    #   needed_by (CodeResourceDependency/ForeignKey)
    #   method_set (Method/ForeignKey)

    coderesource = models.ForeignKey(
            CodeResource,
            related_name="revisions");  
        
    revision_name = models.CharField(
            max_length=128,
            help_text="A name to differentiate revisions of a CodeResource");

    revision_DateTime = models.DateTimeField(
            auto_now_add=True,
            help_text="Date this resource revision was uploaded");

    revision_parent = models.ForeignKey(
            'self',
            related_name="descendants",
            null=True,
            blank=True);

    revision_desc = models.TextField(
            "Revision description",
            help_text="A description for this particular resource revision");

    content_file = models.FileField(
            "File contents",
            upload_to="CodeResources",
            null=True,
            blank=True,
            help_text="File contents of this code resource revision");

    MD5_checksum = models.CharField(
            max_length=64,
            blank=True,
            help_text="Used to validate file contents of this resource revision");

    def __unicode__(self):
        """Represent a resource revision by it's CodeResource name and revision name"""
        
        # Admin can create CR without save() and allow CRRev to be created in memory
        # So, in MEMORY, a revision can temporarily have no corresponding CodeResource
        if not hasattr(self, "coderesource"):
            returnCodeResource = u"[no code resource set]"
        else:
            returnCodeResource = unicode(self.coderesource)

        if self.revision_name == "":
            returnRevisionName = u"[no revision name]"
        else:
            returnRevisionName = unicode(self.revision_name)

        string_rep = unicode(returnCodeResource + ' ' + returnRevisionName)
        return string_rep

    # This CRR includes it's own filename at the root
    def list_all_filepaths(self):
        """Return all filepaths associated with this CodeResourceRevision.

        Filepaths are listed recursively following a root-first scheme,
        with the filepaths of the children listed in order.
        """
        return self.list_all_filepaths_h(self.coderesource.filename)

    # Self is be a dependency CRR, base_name is it's file name, specified either
    # by the parent dependency layer, or in the case of a top-level CR, just CRR.name
    def list_all_filepaths_h(self, base_name):

        # Filepath includes the original file which has dependencies
        # If just a library of dependencies (IE, base_name=""), don't add base_path
        all_filepaths = []
        if base_name != "":
            all_filepaths = [unicode(base_name)]

        # For each dependency in this code resource revision
        for dep in self.dependencies.all():

            # Get all file paths of the CR of the child dependency relative to itself
            dep_fn = dep.depFileName;
            # If depFileName is blank, check and see if the corresponding CodeResource
            # had a filename (i.e. if this is a non-metapackage CRR and so there is
            # an associated file).
            if dep_fn == "":
                dep_fn = dep.requirement.coderesource.filename;
            
            inner_dep_paths = dep.requirement.list_all_filepaths_h(dep_fn)

            # Convert the paths from being relative to the child CRR to being
            # relative to the current parent CRR by appending pathing
            # information from the dependency layer
            for paths in inner_dep_paths:
                correctedPath = os.path.join(dep.depPath, paths)
                all_filepaths.append(unicode(correctedPath))

        return all_filepaths

    def has_circular_dependence(self):
        """Detect any circular dependences defined in this CodeResourceRevision."""
        return self.has_circular_dependence_h([]);

    def has_circular_dependence_h(self, dependants):
        """Helper for has_circular_dependence.

        dependants is an accumulator that tracks all of the all of the
        CRRs that have this one as a dependency.
        """
        # Base case: self is dependant on itself, in which case, return true.
        if self in dependants:
            return True;
        
        # Recursive case: go to all dependencies and check them.
        check_dep = False;
        for dep in self.dependencies.all():
            if dep.requirement.has_circular_dependence_h(dependants + [self]):
                check_dep = True;

        return check_dep;

    def clean(self):
        """Check coherence of this CodeResourceRevision.

        Tests for any circular dependency; does this CRR depend on
        itself at all?  Also, checks for conflicts in the
        dependencies.  Finally, if there is a file specified, fill in
        the MD5 checksum.
        """
        # CodeResource can be a collection of dependencies and not contain
        # a file - in this case, MD5 has no meaning and shouldn't exist
        try:
            md5gen = hashlib.md5();
            md5gen.update(self.content_file.read());
            self.MD5_checksum = md5gen.hexdigest();

        except ValueError as e:
            self.MD5_checksum = "";

        # Check for a circular dependency.
        if self.has_circular_dependence():
            raise ValidationError("Self-referential dependency"); 

        # Check if dependencies conflict with each other
        listOfDependencyPaths = self.list_all_filepaths()
        if len(set(listOfDependencyPaths)) != len(listOfDependencyPaths):
            raise ValidationError("Conflicting dependencies");

        # If content file exists, it must have a file name
        if self.content_file and self.coderesource.filename == "":
            raise ValidationError("If content file exists, it must have a file name")

        # If no content file exists, it must not have a file name
        if not self.content_file and self.coderesource.filename != "":
            raise ValidationError("Cannot have a filename specified in the absence of a content file")

class CodeResourceDependency(models.Model):
    """
    Dependencies of a CodeResourceRevision - themselves also CodeResources.
    Related to :model:`copperfish.CodeResourceRevision`
    """

    coderesourcerevision = models.ForeignKey(CodeResourceRevision,
                         related_name="dependencies");

    # Dependency is a codeResourceRevision
    requirement = models.ForeignKey(CodeResourceRevision,
                                    related_name="needed_by");

    # Where to place it during runtime relative to the CodeResource that relies on this CodeResourceDependency
    # FIXME: specifies the subdirectory, and OPTIONALLY, the file name to adopt during execution
    depPath = models.CharField(
        "Dependency path",
        max_length=255,
        help_text="Where a code resource dependency must exist in the sandbox relative to it's parent");

    depFileName = models.CharField(
        "Dependency file name",
        max_length=255,
        help_text="The file name the dependency is given on the sandbox at execution",
        blank=True);

    def clean(self):
        """
        depPath cannot reference ".."
        """

        # Collapse down to a canonical path
        self.depPath = os.path.normpath(self.depPath)

        # Catch ".." on it's own
        if re.search("^\.\.$", self.depPath):
            raise ValidationError("depPath cannot reference ../");

        # Catch "../[whatever]"
        if re.search("^\.\./", self.depPath):
            raise ValidationError("depPath cannot reference ../");

        # This next case actually should never happen since we've collapsed down
        # to a canonical path.
        # Catch any occurrence of "/../" within a larger path (Ex: blah/../bar)
        if re.search("/\.\./", self.depPath):
            raise ValidationError("depPath cannot reference ../");

        # If the child CR is a meta-package (no filename), we cannot
        # have a depFileName as this makes no sense
        if self.requirement.coderesource.filename == "" and self.depFileName != "":
            raise ValidationError("Metapackage dependencies cannot have a depFileName");


    def __unicode__(self):
        """Represent as [codeResourceRevision] requires [dependency] as [dependencyLocation]."""
        return u"{} requires {} as {}".format(
                unicode(self.coderesourcerevision),
                unicode(self.requirement),
                os.path.join(self.depPath, self.depFileName));

class TransformationFamily(models.Model):
    """
    TransformationFamily is abstract and describes common
    parameters between MethodFamily and PipelineFamily.

    Extends :model:`copperfish.MethodFamily`
    Extends :model:`copperfish.PipelineFamily`
    """

    name = models.CharField(
            "Transformation family name",
            max_length=128,
            help_text="The name given to a group of methods/pipelines");

    description = models.TextField(
            "Transformation family description",
            help_text="A description for this collection of methods/pipelines");

    def __unicode__(self):
        """ Describe transformation family by it's name """
        return self.name;

    class Meta:
        abstract = True;

class MethodFamily(TransformationFamily):
    """
    MethodFamily groups revisions of Methods together.

    Inherits :model:`copperfish.TransformationFamily`
    Related to :model:`copperfish.Method`
    """

    # Implicitly defined:
    #   members (Method/ForeignKey)

    pass

class PipelineFamily(TransformationFamily):
    """
    PipelineFamily groups revisions of Pipelines together.

    Inherits :model:`copperfish.TransformationFamily`
    Related to :model:`copperfish.Pipeline`
    """

    # Implicitly defined:
    #   members (Pipeline/ForeignKey)

    pass


class Transformation(models.Model):
    """
    Abstract class that defines common parameters
    across Method revisions and Pipeline revisions.

    Extends :model:`copperfish.Method`
    Extends :model:`copperfish.Pipeline`
    Related to :model:`TransformationInput`
    Related to :model:`TransformationOutput`
    """

    revision_name = models.CharField(
            "Transformation revision name",
            max_length=128,
            help_text="The name of this transformation revision");

    revision_DateTime = models.DateTimeField(
            "Revision creation date",
            auto_now_add = True);

    revision_desc = models.TextField(
            "Transformation revision description",
            help_text="Description of this transformation revision");

    # inputs/outputs associated with transformations via GenericForeignKey
    # And can be accessed from within Transformations via GenericRelation
    inputs = generic.GenericRelation("TransformationInput");
    outputs = generic.GenericRelation("TransformationOutput");

    class Meta:
        abstract = True;

    def check_input_indices(self):
        """Check that input indices are numbered consecutively from 1."""
        # Append each input index (hole number) to a list
        input_nums = [];
        for curr_input in self.inputs.all():
            input_nums += [curr_input.dataset_idx];

        # Indices must be consecutively numbered from 1 to n
        if sorted(input_nums) != range(1, self.inputs.count()+1):
            raise ValidationError(
                "Inputs are not consecutively numbered starting from 1");
        
    def check_output_indices(self):
        """Check that output indices are numbered consecutively from 1."""
        # Append each output index (hole number) to a list
        output_nums = [];
        for curr_output in self.outputs.all():
            output_nums += [curr_output.dataset_idx];

        # Indices must be consecutively numbered from 1 to n
        if sorted(output_nums) != range(1, self.outputs.count()+1):
            raise ValidationError(
                "Outputs are not consecutively numbered starting from 1");

    def clean(self):
        """Validate transformation inputs and outputs."""
        self.check_input_indices();
        self.check_output_indices();

    # Helper to create inputs, which is now a 2-step operation if the input
    # is not raw.
    @transaction.commit_on_success
    def create_input(self, dataset_name, dataset_idx, compounddatatype=None,
                     min_row=None, max_row=None):
        """
        Create a TI for this transformation.

        Decides whether the created TI should have a structure or not based
        on the parameters given.

        If CDT is None but min_row or max_row is not None, then a ValueError
        is raised.
        """
        if compounddatatype == None and (min_row != None or max_row != None):
            raise ValueError("Row restrictions cannot be specified without a CDT")

        new_input = self.inputs.create(dataset_name=dataset_name,
                                       dataset_idx=dataset_idx)
        new_input.full_clean()

        if compounddatatype != None:
            new_input_structure = new_input.structure.create(
                compounddatatype=compounddatatype,
                min_row=min_row, max_row=max_row)
            new_input_structure.full_clean()

        return new_input

    
    # Same thing to create outputs.
    @transaction.commit_on_success
    def create_output(self, dataset_name, dataset_idx, compounddatatype=None,
                     min_row=None, max_row=None):
        """
        Create a TO for this transformation.

        Decides whether the created TO should have a structure or not based
        on the parameters given.

        If CDT is None but min_row or max_row is not None, then a ValueError
        is raised.
        """
        if compounddatatype == None and (min_row != None or max_row != None):
            raise ValueError("Row restrictions cannot be specified without a CDT")

        new_output = self.outputs.create(dataset_name=dataset_name,
                                         dataset_idx=dataset_idx)
        new_output.full_clean()

        if compounddatatype != None:
            new_output_structure = new_output.structure.create(
                compounddatatype=compounddatatype,
                min_row=min_row, max_row=max_row)
            new_output_structure.full_clean()

        return new_output

class Method(Transformation):
    """
    Methods are atomic transformations.

    Inherits from :model:`copperfish.Transformation`
    Related to :model:`copperfish.CodeResource`
    Related to :model:`copperfish.MethodFamily`
    """

    # Implicitly defined:
    #   descendants (self/ForeignKey)

    family = models.ForeignKey(
            MethodFamily,
            related_name="members");

    revision_parent = models.ForeignKey(
            "self",
            related_name = "descendants",
            null=True,
            blank=True);

    # Code resource revisions are executable if they link to Method
    driver = models.ForeignKey(CodeResourceRevision);

    def __unicode__(self):
        """Represent a method by it's revision name and method family"""
        string_rep = u"Method {} {}".format("{}", self.revision_name);

        # MethodFamily may not be temporally saved in DB if created by admin
        if hasattr(self, "family"):
            string_rep = string_rep.format(unicode(self.family));
        else:
            string_rep = string_rep.format("[family unset]");

        return string_rep;

    def save(self, *args, **kwargs):
        """
        Create or update a method revision.

        If a method revision being created is derived from a parental
        method revision, copy the parent input/outputs.
        """

        # Inputs/outputs cannot be stored in the database unless this
        # method revision has itself first been saved to the database
        super(Method, self).save(*args, **kwargs)

        # If no parent revision exists, there are no input/outputs to copy
        if self.revision_parent == None:
            return None

        # If parent revision exists, and inputs/outputs haven't been registered,
        # copy all inputs/outputs (Including raws) from parent revision to this revision
        if (self.inputs.count() + self.outputs.count() == 0):
            for parent_input in self.revision_parent.inputs.all():
                self.inputs.create(
                        compounddatatype = parent_input.compounddatatype,
                        dataset_name = parent_input.dataset_name,
                        dataset_idx = parent_input.dataset_idx,
                        min_row = parent_input.min_row,
                        max_row = parent_input.max_row)

            for parent_output in self.revision_parent.outputs.all():
                self.outputs.create(
                        compounddatatype = parent_output.compounddatatype,
                        dataset_name = parent_output.dataset_name,
                        dataset_idx = parent_output.dataset_idx,
                        min_row = parent_output.min_row,
                        max_row = parent_output.max_row)


class Pipeline(Transformation):
    """
    A particular pipeline revision.

    Inherits from :model:`copperfish.Transformation`
    Related to :model:`copperfish.PipelineFamily`
    Related to :model:`copperfish.PipelineStep`
    Related to :model:`copperfish.PipelineOutputCable`
    """
    # Implicitly defined
    #   steps (PipelineStep/ForeignKey)
    #   descendants (self/ForeignKey)
    #   outcables (PipelineOutputCable/ForeignKey)

    family = models.ForeignKey(
            PipelineFamily,
            related_name="members")

    revision_parent = models.ForeignKey(
            "self",
            related_name = "descendants",
            null=True,
            blank=True)

    def __unicode__(self):
        """Represent pipeline by revision name and pipeline family"""

        string_rep = u"Pipeline {} {}".format("{}", self.revision_name)

        # If family isn't set (if created from family admin page)
        if hasattr(self, "family"):
            string_rep = string_rep.format(unicode(self.family))
        else:
            string_rep = string_rep.format("[family unset]")

        return string_rep

    def clean(self):
        """
        Validate pipeline revision inputs/outputs

        - Pipeline INPUTS must be consecutively numbered from 1
        - Pipeline STEPS must be consecutively starting from 1
        - Steps are clean
        - PipelineOutputCables are appropriately mapped from the pipeline's steps
        """
        # Transformation.clean() - check for consecutive numbering of
        # input/outputs for this pipeline as a whole
        super(Pipeline, self).clean();

        # Internal pipeline STEP numbers must be consecutive from 1 to n
        all_steps = self.steps.all();
        step_nums = [];

        for step in all_steps:
            step_nums += [step.step_num];

        if sorted(step_nums) != range(1, len(all_steps)+1):
            raise ValidationError(
                "Steps are not consecutively numbered starting from 1");

        # Check that steps are clean; this also checks the cabling between steps.
        # Note: we don't call *complete_clean* because this may refer to a
        # "transient" state of the Pipeline whereby it is not complete yet.
        for step in all_steps:
            step.clean();

        # Check pipeline output wiring for coherence
        output_indices = [];
        output_names = [];

        # Validate each PipelineOutput(Raw)Cable
        for outcable in self.outcables.all():
            outcable.clean()
            output_indices += [outcable.output_idx];
            output_names += [outcable.output_name];

        # PipelineOutputCables must be numbered consecutively
        if sorted(output_indices) != range(1, self.outcables.count()+1):
            raise ValidationError(
                "Outputs are not consecutively numbered starting from 1");

    def complete_clean(self):
        """
        Check that the pipeline is both coherent and complete.

        Coherence is checked using clean(); the tests for completeness are:
        - there is at least 1 step
        - steps are complete, not just clean
        """
        self.clean();
        
        all_steps = self.steps.all();
        if all_steps.count == 0:
            raise ValidationError("Pipeline {} has no steps".format(unicode(self)));

        for step in all_steps:
            step.complete_clean();

    def create_outputs(self):
        """
        Delete existing pipeline outputs, and recreate them from output cables.

        PRE: this should only be called after the pipeline has been verified by
        clean and the outcables are known to be OK.
        """
        # Be careful if customizing delete() of TransformationOutput.
        self.outputs.all().delete()

        # outcables is derived from (PipelineOutputCable/ForeignKey).
        # For each outcable, extract the cabling parameters.
        for outcable in self.outcables.all():
            output_requested = outcable.provider_output

            new_pipeline_output = self.outputs.create(
                dataset_name=outcable.output_name,
                dataset_idx=outcable.output_idx)

            if not outcable.is_raw():
                # Define an XputStructure for new_pipeline_output.
            
                output_CDT = output_requested.compounddatatype
                if outcable.custom_outwires.all().exists():
                    # If there is custom wiring, then we need to define a new
                    # CDT for the output.
                    # Note: the integrity of the custom wiring is already enforced
                    # when you clean() the output cable.
                    output_CDT = CompoundDatatype()
                    output_CDT.save()
                    for outwire in outcable.custom_outwires.all():
                        output_CDT.members.create(
                            datatype=outwire.source_pin.datatype,
                            column_name=outwire.dest_name,
                            column_idx=outwire.dest_idx)

                new_pipeline_output.structure.create(
                    compounddatatype=output_CDT,
                    min_row=output_requested.min_row,
                    max_row=output_requested.max_row)

    # Helper to create raw outcables.  This is just so that our unit tests
    # can be easily amended to work in our new scheme, and wouldn't really
    # be used elsewhere.
    @transaction.commit_on_success
    def create_raw_outcable(self, raw_output_name, raw_output_idx,
                            step_providing_raw_output, provider_raw_output):
        """Creates a raw outcable."""
        new_outcable = self.outcables.create(
            output_name=raw_output_name,
            output_idx=raw_output_idx,
            step_providing_output=step_providing_raw_output,
            provider_output=provider_raw_output)
        new_outcable.full_clean()

        return new_outcable

class PipelineStep(models.Model):
    """
    A step within a Pipeline representing a single transformation
    operating on inputs that are either pre-loaded (Pipeline inputs)
    or derived from previous pipeline steps within the same pipeline.

    Related to :mode;:`copperfish.Dataset`
    Related to :model:`copperfish.Pipeline`
    Related to :model:`copperfish.Transformation`
    Related to :model:`copperfish.PipelineStepInput`
    Related to :model:`copperfish.PipelineStepDelete`
    """
    pipeline = models.ForeignKey(
            Pipeline,
            related_name="steps");

    # Pipeline steps are associated with a transformation
    content_type = models.ForeignKey(
            ContentType,
            limit_choices_to = {"model__in": ("method", "pipeline")});

    object_id = models.PositiveIntegerField();
    transformation = generic.GenericForeignKey("content_type", "object_id");
    step_num = models.PositiveIntegerField(validators=[MinValueValidator(1)]);

    # Which outputs of this step we want to delete.
    # Previously, this was done via another explicit class (PipelineStepDelete);
    # this is more compact.
    # -- August 21, 2013
    outputs_to_delete = models.ManyToManyField(
        "TransformationOutput",
        help_text="TransformationOutputs whose data should not be retained",
        related_name="pipeline_steps_deleting")

    def __unicode__(self):
        """ Represent with the pipeline and step number """

        pipeline_name = "[no pipeline assigned]";   
        if hasattr(self, "pipeline"):
            pipeline_name = unicode(self.pipeline);
        return "{} step {}".format(pipeline_name, self.step_num);


    def recursive_pipeline_check(self, pipeline):
        """Given a pipeline, check if this step contains it.

        PRECONDITION: the transformation at this step has been appropriately
        cleaned and does not contain any circularities.  If it does this
        function can be fragile!
        """
        contains_pipeline = False;

        # Base case 1: the transformation is a method and can't possibly contain the pipeline.
        if type(self.transformation) == Method:
            contains_pipeline = False;

        # Base case 2: this step's transformation exactly equals the pipeline specified
        elif self.transformation == pipeline:
            contains_pipeline = True;

        # Recursive case: go through all of the target pipeline steps and check if
        # any substeps exactly equal the transformation: if it does, we have circular pipeline references
        else:
            transf_steps = self.transformation.steps.all();
            for step in transf_steps:
                step_contains_pipeline = step.recursive_pipeline_check(pipeline);
                if step_contains_pipeline:
                    contains_pipeline = True;
        return contains_pipeline;

    def clean(self):
        """
        Check coherence of this step of the pipeline.

        - Does the transformation at this step contain the parent pipeline?
        - Are any inputs multiply-cabled?
        
        Also, validate each input cable, and each specified output deletion.

        A PipelineStep must be save()d before cables can be connected to
        it, but it should be clean before being saved. Therefore, this
        checks coherency rather than completeness, for which we call
        complete_clean() - such as cabling.
        """
        # Check recursively to see if this step's transformation contains
        # the specified pipeline at all.
        if self.recursive_pipeline_check(self.pipeline):
            raise ValidationError("Step {} contains the parent pipeline".
                                  format(self.step_num));

        # Check for multiple cabling to any of the step's inputs.
        for transformation_input in self.transformation.inputs.all():
            num_matches = self.cables_in.filter(transf_input=transformation_input).count()
            if num_matches > 1:
                raise ValidationError(
                    "Input \"{}\" to transformation at step {} is cabled more than once".
                    format(transformation_input.dataset_name, self.step_num))

        # Validate each cable (Even though we call PS.clean(), we want complete wires)
        for curr_cable in self.cables_in.all():
            curr_cable.clean_and_completely_wired()

        # Validate each PipelineStep output deletion
        for curr_del in self.outputs_to_delete.all():
            curr_del.clean()

        # Note that outputs_to_delete takes care of multiple deletions
        # (if a TO is marked for deletion several times, it will only
        # appear once anyway).  All that remains to check is that the
        # TOs all belong to the transformation at this step.
        for otd in self.outputs_to_delete:
            if not self.transformation.outputs.filter(pk=otd.pk).exists():
                raise ValidationError(
                    "Transformation at step {} does not have output \"{}\"".
                    format(self.step_num, otd));

    def complete_clean(self):
        """Executed after the step's wiring has been fully defined, and
        to see if all inputs are quenched exactly once.
        """
        self.clean()
            
        for transformation_input in self.transformation.inputs.all():
            # See if the input is specified more than 0 times (and
            # since clean() was called above, we know that therefore
            # it was specified exactly 1 time).
            num_matches = self.cables_in.filter(transf_input=transformation_input).count()
            if num_matches == 0:
                raise ValidationError(
                    "Input \"{}\" to transformation at step {} is not cabled".
                    format(transformation_input.dataset_name, self.step_num))

    # Helper to create *raw* cables.  This is really just so that all our
    # unit tests can be easily amended; going forwards, there's no real reason
    # to use this.
    @transaction.commit_on_success
    def create_raw_cable(transf_raw_input, pipeline_raw_input):
        """
        Create a raw cable feeding this PipelineStep.
        """
        new_cable = self.cables_in.create(
            transf_input=transf_raw_input,
            step_providing_input=0,
            provider_output=pipeline_raw_input)
        new_cable.full_clean()
        return new_cable

    # Same for raw deletes.
    @transaction.commit_on_success
    def create_raw_delete(raw_dataset_to_delete):
        """
        Mark a raw TO for deletion.
        """
        new_raw_deletion = self.outputs_to_delete.create(
            dataset_to_delete=raw_dataset_to_delete)
        new_raw_deletion.full_clean()
        return new_raw_deletion

class PipelineStepInputCable(models.Model):
    """
    Represents the "cables" feeding into the transformation of a
    particular pipeline step, specifically:

    A) Destination of cable (transf_input_name) - step implicitly defined
    B) Source of the cable (step_providing_input, provider_output_name)

    Related to :model:`copperfish.PipelineStep`
    """
    # The step (Which has a transformation) where we define incoming cabling
    pipelinestep = models.ForeignKey(
        PipelineStep,
        related_name = "cables_in");
    
    # Input hole (TransformationInput) of the transformation
    # at this step to which the cable leads
    transf_input = models.ForeignKey(
        "TransformationInput",
        help_text="Wiring destination input hole");
    
    
    # (step_providing_input, provider_output) unambiguously defines
    # the source of the cable.  step_providing_input can't refer to a PipelineStep
    # as it might also refer to the pipeline's inputs (i.e. step 0).
    step_providing_input = models.PositiveIntegerField("Step providing the input source",
                                                       help_text="Cabling source step");

    content_type = models.ForeignKey(
            ContentType,
            limit_choices_to = {"model__in": ("TransformationOutput",
                                              "TransformationInput")});
    object_id = models.PositiveIntegerField();
    # Wiring source output hole.
    provider_output = generic.GenericForeignKey("content_type", "object_id");

    # step_providing_input must be PRIOR to this step (Time moves forward)

    # Coherence of data is already enforced by Pipeline

    def __unicode__(self):
        """
        Represent PipelineStepInputCable with the pipeline step, and the cabling destination input name.

        If cable is raw, this will look like:
        [PS]:[input name](raw)
        If not:
        [PS]:[input name]
        """
        step_str = "[no pipeline step set]"
        is_raw_str = ""
        if self.pipelinestep != None:
            step_str = unicode(self.pipelinestep)
        if self.is_raw:
            is_raw_str = "(raw)"
        return "{}:{}{}".format(step_str, self.transf_input.dataset_name, is_raw_str);

    
    def clean(self):
        """Check coherence of the cable.

        Check in all cases:
        - Are the input and output either both raw or both non-raw?

        If the cable is raw:
        - Does the input come from the Pipeline?
        - Are there any wires defined?  (There shouldn't be!)

        If the cable is not raw:
        - Does the input come from a prior step?
        - Does the cable map to an (existent) input of this step's transformation?
        - Does the requested output exist?
        - Do the input and output 'work together' (compatible min/max)?

        Whether the input and output have compatible CDTs or have valid custom
        wiring is checked via clean_and_completely_wired.
        """
        input_requested = self.provider_output;
        feed_to_input = self.transf_input;

        if input_requested.is_raw() != feed_to_input.is_raw():
            raise ValidationError(
                "Cable \"{}\" has mismatched source (\"{}\") and destination (\"{}\")".
                format(self, input_requested, feed_to_input))

        if self.is_raw():
            self.raw_clean()
        else:
            self.non_raw_clean()

    def raw_clean(self):
        """
        Helper function called by clean() to deal with raw cables.
        
        PRE: the pipeline step's transformation is not the parent pipeline (this should
        never happen anyway).
        PRE: cable is raw (i.e. the source and destination are both raw); this is enforced
        by clean().
        """
        input_requested = self.pipeline_input
        feed_to_input = self.transf_input
        step_trans = self.pipelinestep.transformation

        # If this cable is raw, does step_providing_input == 0?
        if self.is_raw() and step_providing_input != 0:
            raise ValidationError(
                "Cable \"{}\" must have step 0 for a source".
                format(self))

        # Does this input cable come from a raw input of the parent pipeline?
        # Note: this depends on the pipeline step's transformation not equalling
        # the parent pipeline (which shouldn't ever happen).
        if not self.pipelinestep.pipeline.inputs.filter(pk=input_requested.pk).exists():
            raise ValidationError(
                "Step {} requests raw input not coming from parent pipeline".
                format(self.pipelinestep.step_num))

        # Does the specified input defined for this transformation exist?
        if not step_trans.inputs.filter(pk=feed_to_input.pk).exists():
            raise ValidationError(
                "Transformation at step {} does not have raw input \"{}\"".
                format(self.pipelinestep.step_num, unicode(feed_to_input)))

        # Are there any wires defined?
        if self.custom_wires.all().exists():
            raise ValidationError(
                "Cable \"{}\" is raw and should not have custom wiring defined".
                format(self))

    def non_raw_clean(self):
        """Helper function called by clean() to deal with non-raw cables."""
        input_requested = self.provider_output;
        requested_from = self.step_providing_input;
        feed_to_input = self.transf_input;
        step_trans = self.pipelinestep.transformation

        # Does this input cable come from a step prior to this one?
        if requested_from >= self.pipelinestep.step_num:
            raise ValidationError(
                "Step {} requests input from a later step".
                format(self.pipelinestep.step_num));

        # Does the specified input defined for this transformation exist?
        if not step_trans.inputs.filter(pk=feed_to_input.pk).exists():
            raise ValidationError ("Transformation at step {} does not have input \"{}\"".
                                   format(self.pipelinestep.step_num, unicode(feed_to_input)));

        # Do the source and destination work together?
        # This checks:
        # - the source produces the requested data
        # - the source doesn't delete the requested data
        # - they have compatible min_row and max_row

        if requested_from == 0:
            # Get pipeline inputs of the cable's parent Pipeline,
            # and look for pipeline inputs that match the desired input.
            
            pipeline_inputs = self.pipelinestep.pipeline.inputs.all();
            if input_requested not in pipeline_inputs:
                raise ValidationError(
                    "Pipeline does not have input \"{}\"".
                    format(unicode(input_requested)));

        # If not from step 0, input derives from the output of a pipeline step
        else:
            # Look at the pipeline step referenced by the wiring parameter
            providing_step = self.pipelinestep.pipeline.steps.get(step_num=requested_from)

            # Does the source pipeline step produce the output requested?
            source_step_outputs = providing_step.transformation.outputs.all()
            if input_requested not in source_step_outputs:
                raise ValidationError(
                    "Transformation at step {} does not produce output \"{}\"".
                    format(requested_from, unicode(input_requested)))

        # Check that the input and output connected by the
        # cable are compatible re: number of rows.  Don't check for
        # ValidationError because this was checked in the
        # clean() of PipelineStep.

        provided_min_row = 0
        required_min_row = 0

        # Source output row constraint
        if input_requested.min_row != None:
            provided_min_row = input_requested.min_row

        # Destination input row constraint
        if feed_to_input.min_row != None:
            required_min_row = feed_to_input.min_row

        # Check for contradictory min row constraints
        if (provided_min_row < required_min_row):
            raise ValidationError(
                "Data fed to input \"{}\" of step {} may have too few rows".
                format(feed_to_input.dataset_name, self.pipelinestep.step_num))

        provided_max_row = float("inf")
        required_max_row = float("inf")

        if input_requested.max_row != None:
            provided_max_row = input_requested.max_row

        if feed_to_input.max_row != None:
            required_max_row = feed_to_input.max_row

        # Check for contradictory max row constraints
        if (provided_max_row > required_max_row):
            raise ValidationError(
                "Data fed to input \"{}\" of step {} may have too many rows".
                format(feed_to_input.dataset_name, self.pipelinestep.step_num))

        # Validate whatever wires there already are
        if self.custom_wires.all().exists():
            for wire in self.custom_wires.all():
                wire.clean()

        
    def clean_and_completely_wired(self):
        """Check coherence of the cable, and check that it is correctly wired (if it is non-raw).

        This will call clean() as well as checking whether the input
        and output 'work together' via having the same CDT or having
        good wiring in the non-raw case.
        """
        # Check coherence of this cable otherwise.
        self.clean();

        # There are no checks to be done on wiring if this is a raw cable.
        if self.is_raw():
            return
        
        input_requested = self.provider_output;
        feed_to_input = self.transf_input;
        
        # If CDTs don't match, check presence of custom wiring
        if input_requested.compounddatatype != feed_to_input.compounddatatype:
            if not self.custom_wires.all().exists():
                raise ValidationError(
                    "Custom wiring required for cable \"{}\"".
                    format(unicode(self)));

        # Validate whatever wires there are.
        if self.custom_wires.all().exists():
            # Each destination CDT member of must be wired to exactly once.

            # Get the CDT members of transf_input
            dest_members = self.transf_input.compounddatatype.members.all()

            # For each CDT member, check that there is exactly 1
            # custom_wire leading to it (IE, number of occurences of
            # CDT member = dest_pin)
            for dest_member in dest_members:
                numwires = self.custom_wires.filter(dest_pin=dest_member).count()

                if numwires == 0:
                    raise ValidationError(
                        "Destination member \"{}\" has no wires leading to it".
                        format(unicode(dest_member)));

                if numwires > 1:
                    raise ValidationError(
                        "Destination member \"{}\" has multiple wires leading to it".
                        format(unicode(dest_member)));

    def is_raw(self):
        """True if this cable maps raw data; false otherwise."""
        return self.transf_input.is_raw()

class CustomCableWire(models.Model):
    """
    Defines a customized connection between internal steps of a pipeline.

    This allows us to filter/rearrange/repeat columns when handing
    data from a source TransformationXput (*Input if it's from the
    pipeline's own input, and *Output if it's from a previous step) to
    a destination TransformationInput between steps of a pipeline.

    The analogue here is that we have customized a cable by rearranging
    the connections between the pins.
    """

    # cable for which we are creating custom wiring
    pipelinestepinputcable = models.ForeignKey(
        PipelineStepInputCable,
        related_name = "custom_wires")

    # CDT member on the source output hole
    # We think of wires as connecting cable pins
    source_pin = models.ForeignKey(
        CompoundDatatypeMember,
        related_name="source_pins")

    # CDT member on the destination input hole
    dest_pin = models.ForeignKey(
        CompoundDatatypeMember,
        related_name="dest_pins")

    def clean(self):
        """
        Check the validity of this wire.

        The wire belongs to a cable which connects a source TransformationXput
        and a destination TransformationInput:
        - source_pin must be a member of the set of CDT members of the cable source
        (provider_output) TransformationXput;
        - dest_pin must be a member of the set of CDT members of the cable
        destination (transf_input) TransformationInput;
        - The datatype of the source_pin must match the datatype of the
        destination_pin.
        """

        source_CDT_members = self.pipelinestepinputcable.provider_output.compounddatatype.members.all()
        dest_CDT_members = self.pipelinestepinputcable.transf_input.compounddatatype.members.all()

        if not source_CDT_members.filter(pk=self.source_pin.pk).exists():
            raise ValidationError(
                "Source pin \"{}\" does not come from compounddatatype \"{}\"".
                format(unicode(self.source_pin), unicode(self.pipelinestepinputcable.provider_output.compounddatatype)))

        if not dest_CDT_members.filter(pk=self.dest_pin.pk).exists():
            raise ValidationError(
                "Destination pin \"{}\" does not come from compounddatatype \"{}\"".
                format(unicode(self.dest_pin), unicode(self.pipelinestepinputcable.provider_output.compounddatatype)))

        # Check that the datatypes on either side of this wire are
        # either the same, or that the source datatype is a
        # restriction of the destination datatype (thus you can feed
        # the source to the destination).
        if (self.source_pin.datatype != self.dest_pin.datatype and
                (not self.dest_pin.datatype.is_restricted_by(self.source_pin.datatype))):
            raise ValidationError(
                "The datatype of the source pin \"{}\" is incompatible with the datatype of the destination pin \"{}\"".
                format(unicode(self.source_pin), unicode(self.dest_pin)))
        
class PipelineOutputCable(models.Model):
    """
    Defines which outputs of internal PipelineSteps are mapped to
    end-point Pipeline outputs once internal execution is complete.

    Thus, a definition of cables leading to external pipeline outputs.

    Related to :model:`copperfish.Pipeline`
    Related to :model:`copperfish.TransformationOutput` (Refactoring needed)
    """
    pipeline = models.ForeignKey(
        Pipeline,
        related_name="outcables")

    output_name = models.CharField(
        "Output hole name",
        max_length=128,
        help_text="Pipeline output hole name")

    # We need to specify both the output name and the output index because
    # we are defining the outputs of the Pipeline indirectly through
    # this wiring information - name/index mapping is stored...?
    output_idx = models.PositiveIntegerField(
        "Output hole index",
        validators=[MinValueValidator(1)],
        help_text="Pipeline output hole index")

    # PRE: step_providing_output refers to an actual step of the pipeline
    # and provider_output_name actually refers to one of the outputs
    # at that step
    # The coherence of the data here will be enforced at the Python level
    step_providing_output = models.PositiveIntegerField(
        "Source pipeline step number",
        validators=[MinValueValidator(1)],
        help_text="Source step at which output comes from")

    provider_output = models.ForeignKey(
        "TransformationOutput",
        help_text="Source output hole")
    
    # Enforce uniqueness of output names and indices.
    # Note: in the pipeline, these will still need to be compared with the raw
    # output names and indices.
    class Meta:
        unique_together = (("pipeline", "output_name"),
                           ("pipeline", "output_idx"));

    def __unicode__(self):
        """ Represent with the pipeline name, output index, and output name (???) """
        pipeline_name = "[no pipeline set]";
        if self.pipeline != None:
            pipeline_name = unicode(self.pipeline);

        is_raw_str = ""
        if self.is_raw():
            is_raw_str = " (raw)"

        return "{}:{} ({}{})".format(pipeline_name, self.output_idx,
                                     self.output_name, is_raw_str);

    def clean(self):
        """
        Checks coherence of this output cable.
        
        PipelineOutputCable must reference an existant, undeleted
        transformation output hole.  Also, if the cable is raw, there
        should be no custom wiring.  If the cable is not raw and there
        are custom wires, they should be clean.
        """
        output_requested = self.provider_output;
        requested_from = self.step_providing_output;

        # Step number must be valid for this pipeline
        if requested_from > self.pipeline.steps.all().count():
            raise ValidationError(
                "Output requested from a non-existent step");
        
        providing_step = self.pipeline.steps.get(step_num=requested_from);

        # Try to find a matching output hole
        if not providing_step.transformation.outputs.filter(pk=output_requested.pk).exists():
            raise ValidationError(
                "Transformation at step {} does not produce output \"{}\"".
                format(requested_from, output_requested));

        outwires = self.custom_outwires.all()
        # A raw cable should not have any custom wiring defined.
        if self.is_raw() and outwires.exists():
            raise ValidationError(
                "Cable \"{}\" is raw and should not have wires defined".
                format(self))
        
        # If cable is not raw and custom wires exist, check that they
        # define columns in a CSV file that are indexed consecutively
        # from 1 -- uniqueness of column indices and column names is
        # already enforced by a constraint on CustomOutputCableWire.
        elif not self.is_raw():
            outwire_indices = [];
            for outwire in outwires:
                outwire.full_clean();
                outwire_indices.append(outwire.dest_idx);

            if sorted(outwire_indices) != range(1, len(outwire_indices)+1):
                raise ValidationError(
                    "Columns defined by custom wiring on output cable \"{}\" are not consecutively indexed from 1".
                    format(unicode(self)))

    def is_raw(self):
        """True if this output cable is raw; False otherwise."""
        return self.provider_output.is_raw()
 
class CustomOutputCableWire(models.Model):
    """
    Defines a customized connection from a pipeline's internal step to its output.

    This allows us to filter/rearrange/repeat columns when returning
    data from a particular step's TransformationOutput as the pipeline's output.

    The analogue here is similar to that of CustomCableWire.
    """
    pipelineoutputcable = models.ForeignKey(
        PipelineOutputCable,
        related_name="custom_outwires")

    source_pin = models.ForeignKey(CompoundDatatypeMember)

    dest_name = models.CharField(
        "Destination column name",
        max_length=128,
        help_text="CDT name of this column in the pipeline output")

    dest_idx =  models.PositiveIntegerField(
        "Destination column index",
        validators=[MinValueValidator(1)],
        help_text="CDT index of this column in the pipeline output");

    # This matches the constraint on compound data type members
    # (Cannot have destination columns with the same name/index)
    class Meta:
        unique_together =   (("pipelineoutputcable", "dest_name"),
                            ("pipelineoutputcable", "dest_idx"));
    
    def clean(self):
        """
        source_pin must be a member of the set of CDT members of the cable source (provider_output) TransformationOutput
        """
        # Get the CDT members of the output-CDT referenced by this PipelineOutputCable
        source_CDT_members = self.pipelineoutputcable.provider_output.compounddatatype.members.all()
    
        if not source_CDT_members.filter(pk=self.source_pin.pk).exists():
            raise ValidationError(
                "Source pin \"{}\" does not come from compounddatatype \"{}\"".
                format(unicode(self.source_pin), unicode(self.pipelineoutputcable.provider_output.compounddatatype)))

# August 20, 2013: changed the structure of our Xputs so that there is no distinction
# between raw and non-raw Xputs beyond the existence of an associated "structure"
class TransformationXput(models.Model):
    """
    Describes parameters common to all inputs and outputs
    of transformations - the "holes"

    Related to :models:`copperfish.Transformation`
    """
    # TransformationXput describes the input/outputs of transformations,
    # so this class can only be associated with method and pipeline.
    content_type = models.ForeignKey(
        ContentType,
        limit_choices_to = {"model__in": ("method", "pipeline")})
    object_id = models.PositiveIntegerField()
    transformation = generic.GenericForeignKey("content_type", "object_id")

    # The name of the "input/output" hole.
    dataset_name = models.CharField(
        "Input/output name",
        max_length=128,
        help_text="Name for input/output as an alternative to index")

    # Input/output index on the transformation.
    ####### NOTE: ONLY METHODS NEED INDICES, NOT TRANSFORMATIONS....!!
    # If we differentiate between methods/pipelines... dataset_idx would only
    # belong to methods
    dataset_idx = models.PositiveIntegerField(
            "Input/output index",
            validators=[MinValueValidator(1)],
            help_text="Index defining the relative order of this input/output")

    structure = generic.GenericRelation("XputStructure")

    class Meta:
        abstract = True;

        # A transformation cannot have multiple definitions for column name or column index
        unique_together = (("content_type", "object_id", "dataset_name"),
                           ("content_type", "object_id", "dataset_idx"));

    def __unicode__(self):
        unicode_rep = u"";
        if self.is_raw():
            unicode_rep = u"[{}]:raw{} {}".format(unicode(self.transformation),
                                                  self.dataset_idx, self.dataset_name)
        else:
            unicode_rep = u"[{}]:{} {} {}".format(unicode(self.transformation),
                                                  self.dataset_idx,
                                                  unicode(self.structure.compounddatatype),
                                                  self.dataset_name);

    def is_raw(self):
        """True if this Xput is raw, false otherwise."""
        return structure.all().exists()

class XputStructure(models.Model):
    """
    Describes the "holes" that are managed by Shipyard: i.e. the ones
    that correspond to well-understood CSV formatted data.

    Related to :model:`copperfish.TransformationXput`
    """
    content_type = models.ForeignKey(
        ContentType,
        limit_choices_to = {"model__in": ("TransformationInput", "TransformationOutput")});
    object_id = models.PositiveIntegerField();
    transf_xput = generic.GenericForeignKey("content_type", "object_id")

    # The expected compounddatatype of the input/output
    compounddatatype = models.ForeignKey(CompoundDatatype);
    
    # Nullable fields indicating that this dataset has
    # restrictions on how many rows it can have
    min_row = models.PositiveIntegerField(
        "Minimum row",
        help_text="Minimum number of rows this input/output returns",
        null=True,
        blank=True);

    max_row = models.PositiveIntegerField(
        "Maximum row",
        help_text="Maximum number of rows this input/output returns",
        null=True,
        blank=True);

class TransformationInput(TransformationXput):
    """
    Inherits from :model:`copperfish.TransformationXput`
    """
    pass

class TransformationOutput(TransformationXput):
    """
    Inherits from :model:`copperfish.TransformationXput`
    """
    pass

# FIXME we have to come back to this after making all of the changes
# relating to RunOutputCables
class Run(models.Model):
    """
    Stores data associated with an execution of a pipeline.

    Related to :model:`copperfish.Pipeline`
    Related to :model:`copperfish.RunStep`
    Related to :model:`copperfish.Dataset`
    """
    
    user = models.ForeignKey(User, help_text="User who performed this run")
    start_time = models.DateTimeField("start time",auto_now_add=True,help_text="Time at start of run")
    pipeline = models.ForeignKey(
        Pipeline,
        related_name="pipeline_instances",
        help_text="Pipeline used in this run")

    # If run was spawned within another run, parent_runstep denotes the run step that initiated it
    parent_runstep = models.OneToOneField(
        "RunStep",
        related_name="child_run",
        null=True,
        blank=True,
        help_text="Step of parent run initiating this one as a sub-run")

    # This field will be null if the run occurred as a sub-run of
    # another run (and if that is the case, then the parent_runstep
    # will have an ExecRecord).  This is because if it a sub-run, we'd
    # need to consider the cables feeding into the sub-pipeline at
    # that RunStep.
    execrecord = models.ForeignKey(
        "ExecRecord",
        null=True,
        help_text="Record of this run if it is run at the top-level (i.e. not a sub-run)");

    reused = models.BooleanField(
        help_text="Indicates whether this run uses the record of a previous execution");

    def clean(self):
        """
        Checks coherence of the run (possibly in an incomplete state).

        Checks completeness and cleanliness of all registered steps,
        and checks their numbering.
        """
        
        # Run clean on individual run steps
        for run_step in self.run_steps.all():
            run_step.clean()

        # Run clean on all of its outcables.
        for run_outcable in self.runoutputcables.all():
            run_outcable.clean()
        
        # Multiple-quenching of steps and outcables is taken care of already.

        # All steps that exist must be consecutively numbered starting from 1.
        step_nums = [rs.pipelinestep.step_num for rs in self.run_steps.all()]
        if sorted(step_nums) != range(1, self.run_steps.count()):
            raise ValidationError(
                "RunSteps of Run \"{}\" are not consecutively numbered starting from 1".
                format(self))

        # If there is an execrecord:
        if self.execrecord != None:
            self.execrecord.clean()

            # If there are EROs for this Run's ExecRecord, check that there are
            # corresponding RunOutputCables (we know it to be clean by checking above).
            for ero in self.execrecord.execrecordouts.all():
                curr_output = ero.output

                try:
                    corresp_roc = self.runoutputcables.get(
                        general_transf__pipelineoutputcable__output_name=
                        curr_output.dataset_name)

                    # The corresponding ROC should have the same SymbolicDataset as
                    # the ERO.
                    if (corresp_roc.execrecord.execrecordouts.all()[0].symbolicdataset !=
                            ero.symbolicdataset):
                        raise ValidationError(
                            "ExecRecordOut \"{}\" of Run \"{}\" does not match the corresponding RunOutputCable".
                            format(ero, self))
                    
                except DoesNotExist:
                    # No corresponding ROC exists.
                    raise ValidationError(
                        "ExecRecord of Run \"{}\" has an entry for output \"{}\" but no corresponding RunOutputCable exists".
                        format(self, curr_output))
            
    def complete_clean(self):
        """
        Checks coherence of a completed run.

        Calls clean(), then:
         - checks that all RunSteps are complete and clean
         - checks that the run is finished
        """
        self.clean()

        for run_step in self.run_steps.all():
            run_step.complete_clean()
        
        # Since we have a uniqueness constraint ensuring that each
        # PipelineStep has *at most* one associated RunStep, we
        # check quenching by just counting both.
        if not self.is_finished():
            raise ValidationError(
                "Run \"{}\" is not complete")
    
    def is_finished(self):
        """
        Checks if this run is finished running.
        """
        enough_steps = (self.run_steps.all().count() ==
                        self.pipeline.steps.all().count())
        enough_outcables = (self.runoutputcables.all().count() ==
                            self.pipeline.outcables.all().count())
        return (enough_steps and enough_outcables)

class RunOutputCable(models.Model):
    """
    Annotates the action of a PipelineOutputCable within a run.

    Related to :model:`copperfish.Run`
    Related to :model:`copperfish.ExecRecord`
    Related to :model:`copperfish.PipelineOutputCable`
    """
    run = models.ForeignKey(Run)
    execrecord = models.ForeignKey(
        "ExecRecord",
        related_name="runoutputcables")
    reused = models.BooleanField(
        help_text="Denotes whether this run reused the action of an output cable")
    pipelineoutputcable = models.ForeignKey(
        PipelineOutputCable,
        related_name="pipelineoutputcable_instances")

    class Meta:
        # Uniqueness constraint ensures that no POC is multiply-represented
        # within a run.
        unique_together = ("run", "pipelineoutputcable")

    def clean(self):
        """
        Check coherence of this RunOutputCable.

        PRE: the ExecRecord associated to this RunOutputCable must be
        complete before the ROC can be defined.

        Checks:
        a) pipelineoutputcable belongs to run.pipeline
        b) if this reused an execrecord, then there should be no
           Datasets directly associated to this ROC.
        c) if this ROC's output was not marked for deletion (i.e. it belongs
           to a run that represented a sub-pipeline within another run's pipeline)
           then the corresponding ERO should have existent data associated.
        b) Clean the associated output dataset (if it exists)
        """
        # First, clean the associated *complete* ExecRecord.
        self.execrecord.complete_clean()

        if (not self.run.pipeline.outcables.
                filter(pk=self.pipelineoutputcable.pk).exists()):
            raise ValidationError(
                "POC \"{}\" does not belong to Pipeline \"{}\"".
                format(self.pipelineoutputcable, self.pipeline))

        if self.reused and self.has_data():
            raise ValidationError(
                "RunOutputCable \"{}\" reused an ExecRecord and should not have generated Dataset \"{}\"".
                format(self, self.output))

        # If this ROC's output was not marked for deletion (either
        # this Run is a top-level run or the run was a sub-pipeline
        # and the parent RunStep did not mark this output for
        # deletion), then the ERO should have existent data.  That is,
        # the actual data is required to be retained.
        
        # We know there is only one ERO because we called execrecord.complete_clean().
        corresp_ero = self.execrecord.execrecordouts.get(execrecord=self.execrecord)
        if self.run.parent_runstep != None:
            # Was this marked for deletion?
            ROC_deleted = self.run.parent_runstep.outputs_to_delete.filter(
                dataset_name=self.pipelineoutputcable.output_name).exists()

            # If not, then the ER must contain real data for the ERO.
            if not ROC_deleted and not corresp_ero.has_data():
                raise ValidationError(
                    "ExecRecordOut \"{}\" should reference existent data".
                    format(corresp_ero))
                    
        # If there is existent data associated, clean it.
        if self.has_data():
            self.output.clean()

    def has_data():
        """True if associated output exists; False if not."""
        return hasattr(self, "output")
        

class RunStep(models.Model):
    """
    Annotates the execution of a pipeline step within a run.

    Related to :model:`copperfish.Run`
    Related to :model:`copperfish.ExecRecord`
    Related to :model:`copperfish.PipelineStep`
    """
    run = models.ForeignKey(Run)
    execrecord = models.ForeignKey(
        "ExecRecord",
        related_name="runsteps")
    reused = models.BooleanField(
        help_text="Denotes whether this run step reuses a previous execution")
    pipelinestep = models.ForeignKey(
        PipelineStep,
        related_name="pipelinestep_instances")

    class Meta:
        # Uniqueness constraint ensures you can't have multiple RunSteps for
        # a given PipelineStep within a Run.
        unique_together = ("run", "pipelinestep")

    def clean(self):
        """
        Check coherence of this RunStep.
        
        PRE: the ExecRecord associated to this RunStep must be complete before
        the RunStep can be defined.
        
        The checks we perform:
        a) Check that the PipelineStep belongs to the specified run
        b) Clean all output datasets
        c) More than one dataset cannot be an output from a particular TO
        d) If this PipelineStep is a sub-pipeline (if child_run is registered),
        check that it is complete and clean.

        Note: don't need to check inputs for multiple quenching due to uniqueness.
        We couldn't use this trick for outputs because that would require a
        uniqueness constraint on Dataset involving (runstep, intermediate_output),
        which can be null. (Also for final_output)
        """
        # Clean the associated *complete* ExecRecord.
        self.execrecord.complete_clean()

        # Does pipelinestep belong to run.pipeline?
        if not self.run.pipeline.steps.filter(pk=self.pipelinestep.pk).exists():
            raise ValidationError(
                "PipelineStep \"{}\" does not belong to Pipeline \"{}\"".
                format(self.pipelinestep, self.pipeline))
        
        # Get all output datasets generated by this runstep.
        step_outputs = self.outputs.all()

        # If this step reused an execrecord, there should have been no
        # output datasets generated by this runstep.
        if self.reused and step_outputs.exists():
            raise ValidationError(
                "RunStep \"{}\" reused an ExecRecord and should not have generated any data".
                format(self))
        
        for out_data in self.outputs.all():
            # Clean them individually.
            out_data.clean()

            # Check that this Dataset belongs to one of the EROs of the
            # associated ER.
            if not self.execrecord.execrecordouts.filter(
                    symbolicdataset=out_data.symbolicdataset).exists():
                raise ValidationError(
                    "Dataset \"{}\" is not in ExecRecord \"{}\"".
                    format(out_data, self.execrecord))
            
        # TOs that are *not* marked for deletion at this step should have
        # existent Datasets associated with the corresponding EROs.
        for to in self.pipelinestep.transformation.outputs.all():
            # If this was marked for deletion, skip it.
            if self.pipelinestep.outputs_to_delete.filter(dataset_to_delete=to).exists():
                continue

            corresp_ero = self.execrecord.execrecordouts.get(output=to)
            if not corresp_ero.has_data():
                raise ValidationError(
                    "ExecRecordOut \"{}\" should reference existent data".
                    format(corresp_ero))

        # If the Transformation of pipelinestep is a method, then child_run should not be set
        if hasattr(self,"child_run") == True:
            if type(self.pipelinestep.transformation) == Method:
                raise ValidationError(
                    "PipelineStep is not a Pipeline but a child run exists")

            # If child_run is set, it should be clean
            self.child_run.clean()
             
    def complete_clean(self):
        """
        Checks coherence and completeness of this step.
        
        If the specified PipelineStep is a sub-pipeline, then check
        that child_run is registered.
        """
        self.clean()

        if (type(self.pipelinestep.transformation) == Pipeline and
                hasattr(self,"child_run") == False):
            raise ValidationError(
                "Specified PipelineStep is a Pipeline but no child run exists")

        # If the child is set, it should be complete_clean
        if hasattr(self,"child_run") == True:
            self.child_run.complete_clean()


# August 20, 2013: modified from AbstractDataset.  Now this is a real class that holds
# all kinds of data, and *maybe* has a DatasetStructure associated with it.
class Dataset(models.Model):
    """
    Data files uploaded by users or created by transformations.

    Related to :model:`copperfish.RunStep`
    Related to :model:`copperfish.RunOutputCable`
    Related to :model:`copperfish.SymbolicDataset`

    The clean() function should be used when a pipeline is executed to
    confirm that the dataset structure is consistent with what's
    expected from the pipeline definition.
    
    The code looks like it's checking for things Pipeline.clean() checks,
    but it's for a different purpose:

    Pipeline.clean() checks that the pipeline is well-defined in theory,
    while Dataset.clean() ensures the Pipeline produces what is expected.

    This would catch deviations between the script and the Pipeline's
    definition of that script.
    """
    user = models.ForeignKey(User,help_text="User that uploaded this dataset.")

    name = models.CharField(
        "Dataset name",
        max_length=128,
        help_text="Description of this dataset.")

    description = models.TextField("Dataset description")

    date_created = models.DateTimeField(
        "Date created",
        auto_now_add=True,
        help_text="Date of dataset upload.")

    # Four cases from which datasets can originate:
    #
    # Case 1: Comes from a runstep but not a run
    # Case 2: Comes from a run but not a run step
    # Case 3: Comes from neither a run nor a runstep (Is uploaded)
    # Case 4: Comes from both a run, and also a run step (Run within a run)

    # If this is an intermediary dataset, it is produced by a runstep
    runstep = models.ForeignKey(
        "RunStep",
        related_name="outputs",
        null=True,
        blank=True,
        help_text="Run step dataset was created by (If applicable)")

    # If this is a final dataset, it is produced by a run
    runoutputcable = models.OneToOneField(
        "RunOutputCable",
        related_name="output",
        null=True,
        blank=True,
        help_text="Run output cable this dataset was created by (If applicable)")

    # All datasets are stored in the "Datasets" folder
    dataset_file = models.FileField(
        upload_to="Datasets",
        help_text="Physical file system path where datasets are stored",
        null=False)

    # Links to this Dataset's SymbolicDataset.
    symbolicdataset = models.OneToOneField("SymbolicDataset",
                                           related_name="dataset")

    def __unicode__(self):
        """
        Display Dataset name, user, and date created.
        """

        return "{} (created by {} on {})".format(
            self.name,
            unicode(self.user),
            self.date_created)

    def clean(self):
        """
        If this is a Shipyard-type CSV file, clean the CSV.
        """
        # If there is an associated DatasetStructure (i.e. if it is a
        # CSV file), then clean the CSV using
        # DatasetStructure.clean().
        if not self.is_raw():
            self.structure.clean()

        # FIXME should we be calling check_md5 here?  Or is that too resource-intensive?  Or
        # can we find a better time to run it?
            
    def compute_md5(self):
        """
        Computes the MD5 checksum of the Dataset and stores it to the associated SymbolicDataset.
        """
        try:
            md5gen = hashlib.md5()
            self.dataset_file.open()
            md5gen.update(self.dataset_file.read())
            self.symbolicdataset.MD5_checksum = md5gen.hexdigest()

        except ValueError as e:
            print(e)
            self.symbolicdataset.MD5_checksum = ""
            
    def check_md5(self):
        """
        Checks that the MD5 checksum of the Dataset equals that in the associated SymbolicDataset.

        This will be used when regenerating data that once existed, as a coherence check.
        """
        md5gen = hashlib.md5()
        self.dataset_file.open()
        md5gen.update(self.dataset_file.read())
        return self.symbolicdataset.MD5_checksum == md5gen.hexdigest()

    def is_raw(self):
        """True if this Dataset is raw, i.e. not a CSV file."""
        return not hasattr(self, "structure")
            
    def num_rows(self):
        """
        Returns number of rows in CSV file if Dataset is a CSV file; None otherwise.
        """
        if not self.is_raw():
            return self.structure.num_rows();
        return None

class DatasetStructure(models.Model):
    """
    Data with a Shipyard-compliant structure: a CSV file with a header.
    Encodes the CDT, and the transformation output generating this data.

    Related to :model:`copperfish.Dataset`
    Related to :model:`copperfish.CompoundDatatype`
    """
    # Note: previously we were tracking the exact TransformationOutput
    # this came from (both for its Run and its RunStep) but this is
    # now done more cleanly using ExecRecord.

    dataset = models.OneToOneField(Dataset, related_name="structure")

    compounddatatype = models.ForeignKey(
        CompoundDatatype,
        related_name="conforming_datasets")

    def clean(self):
        """
        Checks the CSV header conforms to CDT definition.
                
        FIXME: will have to be amended to validate each atomic data field
        in the file when doing execute.
        """
        data_csv = csv.DictReader(self.dataset.dataset_file)
        header = data_csv.fieldnames
        cdt_members = self.compounddatatype.members.all()

        # The number of CSV columns must match the number of CDT members
        if len(header) != cdt_members.count():
            raise ValidationError(
                "Dataset \"{}\" does not have the same number of columns as its CDT".
                format(self.dataset))

        # CDT definition must be coherent with the CSV header: ith cdt member must
        # have the same name as the ith CSV header
        for cdtm in self.compounddatatype.members.all():
            if cdtm.column_name != header[cdtm.column_idx-1]:
                raise ValidationError(
                    "Column {} of Dataset \"{}\" is named {}, not {} as specified by its CDT".
                    format(cdtm.column_idx, self.dataset, header[cdtm.column_idx-1], cdtm.column_name))
        
        # FIXME: validate the actual data in the file with unit test scripts


    def num_rows(self):
        """Reports the number of rows belonging to the CSV file (excluding header)."""
        # Note: we don't check for the integrity of self.dataset_file as that will
        # be checked when calling clean().

        # From http://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python
        return (sum(1 for line in self.dataset.dataset_file) - 1);

        # FIXME: do we need to close and reopen self.dataset_file at the end of this
        # script?  Find out by running twice consecutively.

class SymbolicDataset(models.Model):
    """
    Symbolic representation of a Dataset (that may or may not have been deleted/restored).
    """
    # For validation of Datasets when being reused, or when being regenerated.
    MD5_checksum = models.CharField(max_length=64,help_text="Validates file integrity")

    def __unicode__(self):
        """
        Unicode representation of a SymbolicDataset.

        If the dataset has been deleted, this will be S[primary key]*;
        if not, S[primary key].
        """
        unicode_rep = u"S{}".format(self.pk)
        if dataset == None:
            unicode_rep += u"*"
        return unicode_rep

    def has_data(self):
        """True if associated Dataset exists; False otherwise."""
        return hasattr(self, "dataset")

class ExecRecord(models.Model):
    """
    Record of a previous execution of a Method/Pipeline/PipelineOutputCable.

    This record is specific to using given inputs (and cables).
    """
    content_type = models.ForeignKey(
        ContentType,
        limit_choices_to = {"model__in": ("Method", "Pipeline", "PipelineOutputCable")});
    object_id = models.PositiveIntegerField();
    general_transf = generic.GenericForeignKey("content_type", "object_id");

    # Has this record been called into question by a subsequent execution?
    tainted = models.BooleanField(
        help_text="Denotes whether this record's veracity is questionable")

    def __unicode__(self):
        """Unicode representation of this ExecRecord."""
        inputs_list = [unicode(eri) for eri in self.execrecordins.all()]
        outputs_list = [unicode(ero) for ero in self.execrecordouts.all()]

        string_rep = u""
        if type(self.general_transf) in ("Method", "Pipeline"):
            string_rep = u"{}({}) = ({})".format(self.general_transf,
                                                 u", ".join(inputs_list),
                                                 u", ".join(outputs_list))
        else:
            # Return a representation for a cable.
            string_rep = (u"{}".format(u", ".join(inputs_list)) +
                          " ={" + u"{}".format(self.general_transf) + "}=> " +
                          u"{}".format(u", ".join(outputs_list)))
        return string_rep

    def clean(self):
        """
        Checks coherence of the ExecRecord.

        Calls clean on all of the in/outputs, and checks the following cases:
         - if general_transf is not a POC, then either all of the ERIs are cables
           (i.e. this ExecRecord is for a RunStep) or all are TIs (i.e.
           this ExecRecord is for a Run).
        (Multiple quenching is checked via a uniqueness condition and does not
        need to be coded here.)
        """
        eris = self.execrecordins.all()
        eros = self.execrecordouts.all()

        for eri in eris:
            eri.clean()
        for ero in eros:
            ero.clean()

        # There is nothing else to check if this ER represents a PipelineOutputCable.
        if type(self.general_transf) != PipelineOutputCable:
            
            # Get the type of the first ERI (if this is a cable, then
            # the ExecRecord represents a RunStep; if not, it
            # represents a Run).
            is_cable = False
            if eris.exists():
                is_cable = (type(eris[0].generic_input) == PipelineStepInputCable)
        
            for eri in self.execrecordins.all():
                # Check that they are all of the same type (cable or TI) as the first one.
                if ((is_cable and (type(eri) == TransformationInput)) or
                        ((not is_cable) and type(eri) == PipelineStepInputCable)):
                    raise ValidationError(
                        "Inputs to ExecRecord \"{}\" are not all either cables or TIs".
                        format(self))

    def complete_clean(self):
        """
        Checks completeness of the ExecRecord.

        Calls clean, and then checks that all in/outputs of the
        Method/Pipeline/POC are quenched.
        """
        self.clean()

        # Because we know that each ERI is clean (and therefore each
        # one maps to a valid input of our Method/Pipeline/POC), and
        # because there is no multiple quenching (due to a uniqueness
        # constraint), all we have to do is check the number of ERIs
        # to make sure everything is quenched.
        if type(self.general_transf) == PipelineOutputCable:
            # In this case we check that there is an input and an output.
            if not self.execrecordins.all().exists():
                raise ValidationError(
                    "Input to ExecRecord \"{}\" is not quenched".format(self))
            if not self.execrecordouts.all().exists():
                raise ValidationError(
                    "Output of ExecRecord \"{}\" is not quenched".format(self))

        else:
            if self.execrecordins.count() != self.general_transf.inputs.count():
                raise ValidationError(
                    "Input(s) to ExecRecord \"{}\" are not quenched".format(self));
        
            # Similar for EROs.
            if self.execrecordouts.count() != self.general_transf.outputs.count():
                raise ValidationError(
                    "Output(s) of ExecRecord \"{}\" are not quenched".format(self));
        
class ExecRecordIn(models.Model):
    """
    Denotes a symbolic input fed to the Method/Pipeline/POC in the parent ExecRecord.

    The symbolic input may map to deleted data, e.g. if it was a deleted output
    of a previous step in a pipeline.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord",
                                   related_name="execrecordins")
    symbolicdataset = models.ForeignKey(
        SymbolicDataset,
        help_text="Symbol for the dataset fed to this input")
    
    content_type = models.ForeignKey(
        ContentType,
        limit_choices_to = {"model__in":
                            ("PipelineStepInputCable", "TransformationInput",
                             "TransformationOutput")})
    object_id = models.PositiveIntegerField()
    # If it is a PSIC then this denotes the cable feeding into this
    # input; if it is a TI then this denotes that a "virtual" cable
    # fed into a pipeline's input; if it is a TO then execrecord
    # denotes a POC and generic_input is the PipelineStep output that
    # feeds it.
    generic_input = generic.GenericForeignKey("content_type", "object_id")

    class Meta:
        unique_together = ("execrecord", "content_type", "object_id");

    def __unicode__(self):
        """
        Unicode representation.
        
        If this ERI represents the input to a PipelineOutputCable, then it looks like
        [symbolic dataset]
        If it represents a TI, then it looks like
        [symbolic dataset]=>[transformation (raw) input name]
        If it represents a PSIC, then it looks like
        [symbolic dataset]={(raw)cable [pk]}=>[transformation (raw) input name]
        
        Examples:
        S552
        S552={rawcable1118}=>foo_bar
        S552=>foo_bar
        """
        transf_input_name = "";
        cable_str = "";

        if type(self.generic_input) == TransformationOutput:
            # The parent ER is a POC.
            return unicode(self.symbolicdataset)
        elif type(self.generic_input) == PipelineStepInputCable:
            transf_input_name = generic_input.transf_raw_input.dataset_name
            if self.generic_input.is_raw():
                cable_str = "{" + "rawcable{}".format(generic_input.pk) + "}="
            else:
                cable_str = "{" + "cable{}".format(generic_input.pk) + "}="
        else:
            # This is not a cable, it is a TI.
            transf_input_name = generic_input.dataset_name

        return "{}={}>{}".format(self.symbolicdataset, cable_str, transf_input_name)
            

    def clean(self):
        """
        Checks coherence of this ExecRecordIn.

        Checks that generic_input is appropriate for the parent
        ExecRecord's Method/Pipeline/POC.
        - If execrecord is for a POC, then generic_input should be the TO that
          feeds it (i.e. the PipelineStep TO that is cabled to a Pipeline output).
        - If execrecord is not for a POC and generic_input is a PSIC, then
          it must feed into execrecord.general_transf appropriately.
        - If execrecord is not for a POC and generic_input is a
          TransformationInput, then it must belong to execrecord.general_transf.

        Also, if symbolicdataset refers to existent data, check that it
        is compatible with the input requested.
        """
        parent_transf = self.execrecord.general_transf

        if type(parent_transf) == TransformationOutput:
            if self.generic_input != parent_transf.provider_output:
                raise ValidationError(
                    "ExecRecordIn \"{}\" does not denote the TO that feeds the parent ExecRecord's POC".
                    format(self))

        else:
            # The parent ER represents a Method or a Pipeline, so this ERI should
            # not have a POC for generic_input.
            if type(self.generic_input) == PipelineOutputCable:
                raise ValidationError(
                    "ExecRecordIn \"{}\" denotes a PipelineOutputCable but parent ExecRecord does not".
                    format(self))
        
            elif type(self.generic_input) == TransformationInput:
                # Look for this input in self.execrecord.general_transf.inputs.
                transf_inputs = self.execrecord.general_transf.inputs
                if not transf_inputs.filter(pk=self.generic_input.pk).exists():
                    raise ValidationError(
                        "Input \"{}\" does not belong to Method/Pipeline of ExecRecord \"{}\"".
                        format(self.generic_input, self.execrecord))
                    
            elif type(self.generic_input) == PipelineStepInputCable:
                # Look at the cable and see whether it feeds the general_transf
                # of execrecord.
                input_fed = self.generic_input.transf_input
                if not self.execrecord.general_transf.inputs.filter(pk=input_fed.pk).exists():
                    raise ValidationError(
                        "Cable \"{}\" does not feed Method/Pipeline of ExecRecord \"{}\"".
                        format(input_fed, self.execrecord))

        # If the actual data behind symbolicdata still exists, check its coherence.
        if self.symbolicdataset.has_data():
            if self.generic_input.is_raw() != self.symbolicdataset.is_raw():
                raise ValidationError(
                    "Dataset \"{}\" cannot feed source \"{}\"".
                    format(self.symbolicdataset.dataset, self.generic_input))

            if not self.symbolicdataset.dataset.is_raw():
                actual_data = self.symbolicdataset.dataset
                # This gives the row restrictions: for the POC case the restriction
                # comes from the source TO; for the other cases, the restriction
                # comes from the destination TI.
                transf_xput_used = None
                cdt_needed = None

                # FIXME: cdt_needed should really just check for *compatible* CDTs
                # and not the same one.
                if type(self.generic_input) == TransformationOutput:
                    transf_xput_used = self.generic_input
                    cdt_needed = self.generic_input.compounddatatype
                elif type(self.generic_input) == TransformationInput:
                    transf_xput_used = self.generic_input
                    cdt_needed = transf_xput_used.structure.compounddatatype
                else:
                    # generic_input is a PSIC.
                    transf_xput_used = self.generic_input.transf_input
                    cdt_needed = self.generic_input.provider_output.structure.compounddatatype

                # CDT of actual_data must match cdt_needed.
                if actual_data.compounddatatype != cdt_needed:
                    raise ValidationError("Dataset \"{}\" is not of the expected CDT".
                                          format(actual_data))

                # actual_data must satisfy the row constraints imposed by:
                # - the TO sourcing the cable if the parent ER is a POC, or
                # - the TI fed if the parent ER is a Method/Pipeline.
                if (transf_xput_used.structure.min_row != None and
                        actual_data.num_rows() < transf_xput_used.structure.min_row):
                    error_str = ""
                    if type(self.generic_input) == TransformationOutput:
                        error_str = "Dataset \"{}\" has too few rows to have come from TransformationOutput \"{}\""
                    else:
                        error_str = "Dataset \"{}\" has too few rows for TransformationInput \"{}\""
                    raise ValidationError(error_str.format(actual_data, transf_xput_used))
                    
                if (transf_xput_used.structure.max_row != None and
                        actual_data.num_rows() > transf_xput_used.structure.max_row):
                    error_str = ""
                    if type(self.generic_input) == TransformationOutput:
                        error_str = "Dataset \"{}\" has too many rows to have come from TransformationOutput \"{}\""
                    else:
                        error_str = "Dataset \"{}\" has too many rows for TransformationInput \"{}\""
                    raise ValidationError(error_str.format(actual_data, transf_xput_used))

class ExecRecordOut(models.Model):
    """
    Denotes a symbolic output from the Method/Pipeline/POC in the parent ExecRecord.

    The symbolic output may map to deleted data, i.e. if it was deleted after
    being generated.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord",
                                   related_name="execrecordouts")
    symbolicdataset = models.OneToOneField(
        SymbolicDataset,
        help_text="Symbol for the dataset coming from this output",
        related_name="execrecordout")

    output = models.ForeignKey(
        TransformationOutput,
        help_text="Producing TransformationOutput",
        related_name="execrecordouts_referencing")
    
    class Meta:
        unique_together = ("execrecord", "output");

    def __unicode__(self):
        """
        Unicode representation of this ExecRecordOut.

        If this ERO represented the output of a PipelineOutputCable, then this looks like
        [symbolic dataset]
        Otherwise, it represents a TransformationOutput, and this looks like
        [TO name]=>[symbolic dataset]
        e.g.
        S458
        output_one=>S458
        """
        unicode_rep = u""
        if type(execrecord.general_transf) == PipelineOutputCable:
            unicode_rep = unicode(self.symbolicdataset)
        else:
            unicode_rep = u"{}=>{}".format(self.output.dataset_name,
                                           self.symbolicdataset)
        return unicode_rep


    def clean(self):
        """
        Checks coherence of this ExecRecordOut.

        If execrecord represents a POC, then check that output is the one defined
        by the POC.
        
        If execrecord is not a POC, then check that output belongs to 
        execrecord.general_transf.

        If SymbolicDataset points to existent
        data, then this existent data is compatible with the producing
        TransformationOutput.
        """
        if type(self.execrecord.general_transf) == PipelineOutputCable:
            parent_er_outcable = self.execrecord.general_transf

            # Do self.output and parent_er_outcable even belong to the same pipeline?
            if self.output.general_transf != parent_er_outcable.pipeline:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not belong to the same pipeline as its parent ExecRecord's POC".
                    format(self))

            # Does parent_er_outcable define self.output within the pipeline they belong to?
            if parent_er_outcable.output_name != self.output.dataset_name:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not represent the same output as its parent ExecRecord's POC".
                    format(self))

        else:
            # self.execrecord.general_transf is either a Method or a Pipeline.
            query_for_outs = self.execrecord.general_transf.outputs
            
            if not query_for_outs.filter(pk=self.output.pk).exists():
                raise ValidationError(
                    "Output \"{}\" does not belong to Method/Pipeline of ExecRecord \"{}\"".
                    format(self.output, self.execrecord))

        # Check that if the SymbolicDataset's contents (i.e. the Dataset it points to)
        # hasn't been deleted, then it is coherent with the output.
        if self.symbolicdataset.has_data():
            if self.symbolicdataset.is_raw() != self.output.is_raw():
                raise ValidationError(
                    "Dataset \"{}\" cannot have come from output \"{}\"".
                    format(self.symbolicdataset.dataset, self.output))

            # If the Dataset is not raw, check its CDT and number of rows against
            # the producing TO.
            if not self.symbolicdataset.dataset.is_raw():
                actual_data = self.symbolicdataset.dataset
            
                if actual_data.compounddatatype != self.output.compounddatatype:
                    raise ValidationError(
                        "CDT of Dataset \"{}\" does not match the CDT of the generating TransformationOutput \"{}\"".
                        format(actual_data, self.output))

                if self.output.min_row != None and actual_data.num_rows() < self.output.min_row:
                    raise ValidationError(
                        "Dataset \"{}\" was produced by TransformationOutput \"{}\" but has too few rows".
                        format(actual_data, self.output))

                if self.output.max_row != None and actual_data.num_rows() > self.output.max_row:
                    raise ValidationError(
                        "Dataset \"{}\" was produced by TransformationOutput \"{}\" but has too many rows".
                        format(actual_data, self.output))
