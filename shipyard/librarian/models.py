"""
librarian.models

Shipyard data models pertaining to the lookup of the past: ExecRecord,
SymbolicDataset, etc.
"""

from django.db import models, transaction
from django.contrib.contenttypes.models import ContentType
from django.contrib.contenttypes import generic
from django.core.exceptions import ValidationError
from django.core.validators import MinValueValidator, RegexValidator
from django.core.files import File
from django.utils import timezone

import re
import logging
import tempfile
import hashlib

import archive.models
import method.models
import pipeline.models
import transformation.models
import datachecking.models
import file_access_utils
import logging_utils
from metadata.models import CompoundDatatype
import csv

LOGGER = logging.getLogger(__name__)


class SymbolicDataset(models.Model):
    """
    Symbolic representation of a (possibly temporary) data file.

    That is to say, at some point, there was a data file uploaded to/
    generated by Shipyard, which was coherent with its
    specified/generating CDT and its producing
    TransformationOutput/cable (if it was generated), and this
    represents it, whether or not it was saved to the database.

    This holds metadata about the data file.

    PRE: the actual file that the SymbolicDataset represents (whether
    it still exists or not) is/was coherent (e.g. checked using
    CDT.summarize_CSV()).
    """
    # For validation of Datasets when being reused, or when being
    # regenerated.  A blank MD5_checksum means that the file was
    # missing (not created when it was supposed to be created).
    MD5_checksum = models.CharField( max_length=64,
        validators=[RegexValidator(
            regex=re.compile("(^[0-9A-Fa-f]{32}$)|(^$)"),
            message="MD5 checksum is not either 32 hex characters or blank")],
        blank=True, default="", help_text="Validates file integrity")

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __unicode__(self):
        """
        Unicode representation of a SymbolicDataset.

        This is S[pk] or S[pk]d if it has data.
        """
        has_data_suffix = "d" if self.has_data() else ""
        return "S{}{}".format(self.pk, has_data_suffix)

    @property
    def is_missing(self):
        """Is this SymbolicDataset missing data?"""
        return 

    @property
    def compounddatatype(self):
        if self.is_raw(): return None
        return self.structure.first().compounddatatype

    def clean(self):
        """
        Checks coherence of this SymbolicDataset.

        If it has data (i.e. an associated Dataset), it cleans that
        Dataset.  Then, if there is an associated DatasetStructure,
        clean that.

        Note that the MD5 checksum is already checked via a validator.
        """
        if self.has_data():
            self.dataset.clean()

    def has_data(self):
        """True if associated Dataset exists; False otherwise."""
        if not hasattr(self, "dataset"):
            return False
        return self.dataset.pk is not None

    def is_raw(self):
        """True if this SymbolicDataset is raw, i.e. not a CSV file."""
        return not hasattr(self, "structure")
            
    def num_rows(self):
        """Returns number of rows in the associated Dataset.

        This returns None if the Dataset is raw.
        """
        return (None if self.is_raw() else self.structure.num_rows)

    def get_cdt(self):
        """
        Retrieve the CDT of this SymbolicDataset (none if it is raw).
        """
        return (None if self.is_raw() else self.structure.compounddatatype)

    def create_structure(self, compounddatatype, num_rows=-1):
        """Add a DatasetStructure to this SymbolicDataset."""
        if not self.is_raw():
            raise ValueError('CompoundDatatype "{}" already has a structure.')
        structure = DatasetStructure(symbolicdataset=self, compounddatatype=compounddatatype, num_rows=num_rows)
        structure.clean()
        structure.save()
        return structure

    def set_MD5(self, file_path):
        """Set the MD5 hash from a file."""
        with open(file_path, "rb") as f:
            self.MD5_checksum = file_access_utils.compute_md5(f)
        self.clean()
        self.save()

    def set_MD5_and_count_rows(self, file_path):
        """Set the MD5 hash and number of rows from a file.

        PRE
        This SymbolicDataset must have a DatasetStructure
        """
        assert not self.is_raw()

        num_rows = -1 # skip header
        md5gen = hashlib.md5()
        with open(file_path, "r") as f:
            for line in f:
                md5gen.update(line)
                num_rows += 1
        self.structure.num_rows = num_rows
        self.MD5_checksum = md5gen.hexdigest()
        self.clean()
        self.save()

    def register_dataset(self, file_path, user, name, description, created_by=None):
        """Create and register a new Dataset for this SymbolicDataset.

        Note that this does NOT compute an MD5 for the new Dataset, nor
        does it do an integrity check if self already has an MD5 set.
        
        INPUTS
        file_path           file to upload as the new Dataset
        user                user who uploaded the Dataset
        name                name for the new Dataset
        description         description for the new Dataset
        created_by          a RunAtomic which created this Dataset, or
                            None if the Dataset was uploaded by the 
                            user

        PRE
        self must not have a Dataset already associated
        """
        assert not self.has_data()

        dataset = archive.models.Dataset(user=user, name=name, description=description, symbolicdataset=self)
        if created_by:
            dataset.created_by = created_by
        with open(file_path, "r") as f:
            dataset.dataset_file.save(file_path, File(f))
        dataset.clean()
        dataset.save()

    def mark_missing(self, start_time, end_time, execlog):
        """Mark a SymbolicDataset as missing output.

        INPUTS
        start_time      time when we started checking for the file
        end_time        time when check for file finished
        execlog         ExecLog of execution which did not produce 
                        output
        """
        ccl = self.content_checks.create(start_time=start_time, end_time=end_time, execlog=execlog)
        ccl.add_missing_output()

    @classmethod
    def create_empty(cls, compound_datatype=None):
        """Create an empty SymbolicDataset.

        INPUTS
        compound_datatype   CompoundDatatype for the new SymbolicDataset
                            (None indicates a raw SymbolicDataset)

        OUTPUTS
        empty_SD            SymbolicDataset with a blank MD5 and an
                            appropriate DatasetStructure
        """
        empty_SD = cls(MD5_checksum="")
        empty_SD.clean()
        empty_SD.save()
        if compound_datatype:
            empty_SD.create_structure(compound_datatype)
        return empty_SD
        
    @classmethod
    # FIXME what does it do for num_rows when file_path is unset?
    def create_SD(cls, file_path, cdt=None, make_dataset=True, user=None,
                  name=None, description=None, created_by=None, check=True):
        """
        Helper function to make defining SDs and Datasets faster.
    
        user, name, and description must all be set if make_dataset=True.
        make_dataset creates a Dataset from the given file path to go
        with the SD. created_by can be a RunAtomic to register the
        Dataset with, or None if it was uploaded by the user (or if 
        make_dataset=False). If check is True, do a ContentCheck on the
        file.
    
        Returns the SymbolicDataset created.
        """
        LOGGER.debug("Creating SymbolicDataset from file {}".format(file_path))
        with transaction.atomic():
            symDS = cls.create_empty(cdt)

            if cdt is None:
                symDS.set_MD5(file_path)
            else:
                symDS.set_MD5_and_count_rows(file_path)

                if check:
                    run_dir = tempfile.mkdtemp(prefix="SD{}".format(symDS.pk))
                    content_check = symDS.check_file_contents(file_path, run_dir, None, None, None)
                    if content_check.is_fail():
                        if content_check.baddata.bad_header:
                            raise ValueError('The header of file "{}" does not match the CompoundDatatype "{}"'
                                             .format(file_path, cdt))
                        elif content_check.baddata.cell_errors.exists():
                            error = content_check.baddata.cell_errors.first()
                            cdtm = error.column
                            raise ValueError('The entry at row {}, column {} of file "{}" did not pass the constraints of '
                                             'Datatype "{}"'.format(error.row_num, cdtm.column_idx, file_path, cdtm.datatype))
                        else:
                            # Shouldn't reach here.
                            raise ValueError('The file "{}" was malformed'.format(file_path))
                    LOGGER.debug("Read {} rows from file {}".format(symDS.structure.num_rows, file_path))

            if make_dataset:
                symDS.register_dataset(file_path, user, name, description, created_by)

            symDS.clean()
            symDS.save()
        return symDS

    @classmethod
    # FIXME what does it do for num_rows when file_path is unset?
    def create_SD_bulk(cls, csv_file_path, cdt=None, make_dataset=True, user=None,
                       created_by=None, check=True):
        """
        Helper function to make defining multiple SDs and Datasets faster.
        Instead of specifying datasets one by one,
        specify multiple datasets in a CSV.

        The CSV must have these columns, not necessarily in this order:
        - Name
        - Description
        - Datatype
        - File

        make_dataset creates a Dataset from the given file path to go
        with the SD. created_by can be a RunAtomic to register the
        Dataset with, or None if it was uploaded by the user (or if
        make_dataset=False). If check is True, do a ContentCheck on the
        file.

        Returns the SymbolicDataset created.
        :rtype : object
        :param csv_file_path:
        :param cdt:
        :param make_dataset:
        :param user:
        :param created_by:
        :param check:
        """
        symDSs = []
        LOGGER.debug("Creating SymbolicDataset from file {}".format(csv_file_path))
        with open(csv_file_path, 'rU') as fh_datasets:
            # TODO:  this is a major db blocking call.  Can we break this up?
            try:
                with transaction.atomic():
                    line = 0
                    reader = csv.DictReader(fh_datasets)
                    for row in reader:
                        line += 1
                        name = row['Name'].strip() if row['Name'] else ""
                        desc = row['Description'].strip() if row['Description'] else ""
                        file = row['File'].strip() if row['File'] else ""

                        # check for empty entries:
                        if not (name or desc or file):
                            raise Exception("Line " + line + " is invalid: " +
                                            "Name, Description, File, Datatype must be defined")

                        symDS = SymbolicDataset.create_SD(file, cdt=cdt, make_dataset=True, user=user, name=name,
                                                          description=desc, created_by=None, check=True)

                    symDSs.extend([symDS])
            except Exception, e:
                LOGGER.exception("Error while parsing line " + str(line) + " -- " + str(row))
                raise e

        return symDSs


    # FIXME: use a transaction!
    # TODO: clean this up, end_time is set in too many places
    def check_file_contents(self, file_path_to_check, summary_path, min_row, max_row, execlog):
        """
        Performs content check on a file, generates a CCL, and sets this
        SD's num_rows.

        OUTPUTS
        If SD is raw, creates a clean CCL.
        If not raw, checks the file and returns CCL with/without a
        corresponding BadData.

        PRE
        Should never be called twice on the same symbolic dataset, as
        this would overwrite num_rows to a potentially new value?
        """
        self.logger.debug("Creating clean ContentCheckLog for file {} and linking to ExecLog"
                          .format(file_path_to_check))
        ccl = self.content_checks.create(execlog=execlog)
        ccl.start()

        if self.is_raw():
            ccl.stop()
            return ccl

        my_CDT = self.get_cdt()
        with open(file_path_to_check, "rb") as f:
            csv_summary = my_CDT.summarize_CSV(f, summary_path, ccl)

        if ("bad_num_cols" in csv_summary or "bad_col_indices" in csv_summary):
            self.logger.warn("malformed header")
            ccl.add_bad_header()
            ccl.stop()
            return ccl

        if csv_summary["num_rows"] == 0:
            self.logger.warn("file had no rows")

        csv_baddata = False
        self.structure.num_rows = csv_summary["num_rows"]
        self.structure.save()
        if max_row is not None and csv_summary["num_rows"] > max_row:
            self.logger.warn("too many rows")
            # FIXME: Do we only create these BD objects if they don't already exist?
            ccl.add_bad_num_rows()
            csv_baddata = True

        if min_row is not None and csv_summary["num_rows"] < min_row:
            self.logger.warn("too few rows")
            # FIXME: Do we only create these BD objects if they don't already exist?
            ccl.add_bad_num_rows()
            csv_baddata = True

        if "failing_cells" in csv_summary:
            self.logger.warn("cells failed datatype check")

            if not csv_baddata:
                datachecking.models.BadData(contentchecklog=ccl).save()
                csv_baddata = True

            for row, col in csv_summary["failing_cells"]:
                fails = csv_summary["failing_cells"][(row,col)]
                for failed_constr in fails:
                    new_cell_error = ccl.baddata.cell_errors.create(row_num=row,
                                                                    column=my_CDT.members.get(column_idx=col))

                    # If failure is a string (Ex: "Was not integer"), leave constraint_failed as null.
                    if type(failed_constr) != str:
                        new_cell_error.constraint_failed = failed_constr

                    new_cell_error.clean()
                    new_cell_error.save()

        else:
            self.logger.debug("Content check passed - file {} conforms to SymbolicDataset {}".
                    format(file_path_to_check, self))
        ccl.stop()
        return ccl

    def check_integrity(self, new_file_path, checking_user, execlog, newly_computed_MD5=None):
        """
        Checks integrity of SD against the md5 provided (newly_computed_MD5),
        or in it's absence, the MD5 computed from new_file_path.

        OUTPUT
        Returns the ICL.
        """
        # RL February 6: I'm choosing this to be the time of the "start" of the
        # check, but it does raise the question: what exactly is the start and
        # end time of an integrity check?  Is the check just the comparison
        # of the MD5s or is it the time that you finish computing the MD5 or
        # is it the time that you start computing the MD5?
        icl = self.integrity_checks.create(execlog=execlog)
        icl.start()

        if newly_computed_MD5 == None:
            with open(new_file_path, "rb") as f:
                newly_computed_MD5 = file_access_utils.compute_md5(f)
                
        if newly_computed_MD5 != self.MD5_checksum:
            self.logger.warn("md5s do not agree")

            # June 4, 2014: this evil_twin should be a raw SD -- we don't really care what it contains,
            # just that it conflicted with the existing one.
            evil_twin = SymbolicDataset.create_SD(
                    new_file_path,
                    cdt=None,
                    user=checking_user,
                    name="{}eviltwin".format(self),
                    description="MD5 conflictor of {}".format(self))

            note_of_usurping = datachecking.models.MD5Conflict(integritychecklog=icl, conflicting_SD=evil_twin)
            note_of_usurping.save()

        icl.end_time = timezone.now()
        icl.clean()
        icl.save()
        return icl
    
    def is_OK(self):
        """
        Check that this SD has passed a check for contents if not raw,
        and it has never failed any check for integrity or contents.
        """
        icls = self.integrity_checks.all()
        ccls = self.content_checks.all()

        # No content check has been performed.
        if not (self.is_raw() or ccls.exists()):
            self.logger.debug("SD '{}' may not be OK - no content check performed".format(self))
            return False

        # Look for failed integrity/content checks, and also check that at least one
        # content check has been passed.
        for icl in icls:
            if icl.is_fail():
                self.logger.debug("SD '{}' failed integrity check".format(self))
                return False

        # No failed integrity checks: in the raw case, we're done.
        if self.is_raw():
            return True

        content_check_completed = False
        for ccl in ccls:
            if ccl.is_fail():
                self.logger.debug("SD '{}' failed content check".format(self))
                return False
            elif ccl.is_complete():
                content_check_completed = True

        # At this point we know no checks have failed; return False if
        # none of the checks are complete yet.
        self.logger.debug("Has a content check completed on SD '{}'?  {}".format(self, content_check_completed))
        return content_check_completed


class DatasetStructure(models.Model):
    """
    Data with a Shipyard-compliant structure: a CSV file with a header.
    Encodes the CDT, and the transformation output generating this data.

    PRECONDITION
    Any SymbolicDataset that represents a CSV file has to have confirmed using
    summarize_CSV() that the CSV file is coherent.
    """
    # Note: previously we were tracking the exact TransformationOutput
    # this came from (both for its Run and its RunStep) but this is
    # now done more cleanly using ExecRecord.

    symbolicdataset = models.OneToOneField(
            SymbolicDataset,
            related_name="structure")

    compounddatatype = models.ForeignKey(
            "metadata.CompoundDatatype",
            related_name="conforming_datasets")

    # A value of -1 means the file is missing or num rows has never been counted
    num_rows = models.IntegerField("number of rows", validators=[MinValueValidator(-1)], default=-1)


class ExecRecord(models.Model):
    """
    Record of a previous execution of a Method/PSIC/POC
    """
    generator = models.ForeignKey("archive.ExecLog", related_name="execrecords")

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __unicode__(self):
        """Unicode representation of this ExecRecord."""
        inputs_list = [unicode(eri) for eri in self.execrecordins.all()]
        outputs_list = [unicode(ero) for ero in self.execrecordouts.all()]

        string_rep = u""
        if type(self.general_transf()) == method.models.Method:
            string_rep = u"{}({}) = ({})".format(
                    self.general_transf(),
                    u", ".join(inputs_list),
                    u", ".join(outputs_list))
        else:
            # Return a representation for a cable.
            string_rep = (u"{}".format(u", ".join(inputs_list)) +
                          " ={" + u"{}".format(self.general_transf()) + "}=> " +
                          u"{}".format(u", ".join(outputs_list)))
        return string_rep

    @classmethod
    @transaction.atomic
    def create(cls, generator, component, input_SDs, output_SDs):
        """Create a complete ExecRecord, including inputs and outputs.
        
        INPUTS
        generator       ExecLog generating this ExecRecord
        component       Pipeline component the ExecRecord is for (a 
                        PipelineStep, PipelineOutputCable, or 
                        PipelineStepInputCable)
        input_SDs       list of SymbolicDatasets input to the component
                        during execution, in order of their index
        output_SDs      list of SymbolicDatasets output by the component
                        during execution, in order of their index
        """
        execrecord = cls(generator=generator)
        execrecord.clean()
        execrecord.save()
        for i, component_input in enumerate(component.inputs):
            execrecord.execrecordins.create(generic_input=component_input, symbolicdataset=input_SDs[i])
        for i, component_output in enumerate(component.outputs):
            er = execrecord.execrecordouts.create(generic_output=component_output, symbolicdataset=output_SDs[i])
        execrecord.save()
        execrecord.complete_clean()
        return execrecord

    def get_execrecordout(self, xput):
        """Get the ExecRecordOut for a TransformationXput.

        INPUTS
        xput        TransformationXput to get ExecRecordOut for
        """
        try:
            return self.execrecordouts.get(generic_output=xput)
        except ExecRecordOut.DoesNotExist:
            return None

    def clean(self):
        """
        Checks coherence of the ExecRecord.

        Calls clean on all of the in/outputs.  (Multiple quenching is
        checked via a uniqueness condition and does not need to be
        coded here.)

        If this ER represents a trivial cable, then the single ERI and
        ERO should have the same SymbolicDataset.
        """
        eris = self.execrecordins.all()
        eros = self.execrecordouts.all()

        for eri in eris:
            eri.clean()
        for ero in eros:
            ero.clean()

        if type(self.general_transf()) != method.models.Method:
            # If the cable is quenched:
            if eris.exists() and eros.exists():
                
                # If the cable is trivial, then the ERI and ERO should
                # have the same SymbolicDataset (if they both exist).
                if self.general_transf().is_trivial():
                    if eris[0].symbolicdataset != eros[0].symbolicdataset:
                        raise ValidationError(('ExecRecord "{}" represents a trivial cable but its input and output '
                                               'do not match').format(self))

                # If the cable is not trivial and both sides have
                # data, then the column *Datatypes* on the destination
                # side are the same as the corresponding column on the
                # source side.  For example, if a CDT like (DNA col1,
                # int col2) is fed through a cable that maps col1 to
                # produce (string foo), then the actual Datatype of
                # the column in the corresponding Dataset would be
                # DNA.

                # Note that because the ERI and ERO are both clean,
                # and because we checked general_transf is not
                # trivial, we know that both have well-defined
                # DatasetStructures.
                elif not self.general_transf().is_trivial():
                    cable_wires = self.general_transf().custom_wires.all()

                    source_CDT = eris[0].symbolicdataset.structure.compounddatatype
                    dest_CDT = eros[0].symbolicdataset.structure.compounddatatype

                    for wire in cable_wires:
                        source_idx = wire.source_pin.column_idx
                        dest_idx = wire.dest_pin.column_idx
                        
                        dest_dt = dest_CDT.members.get(column_idx=dest_idx).datatype
                        source_dt = source_CDT.members.get(column_idx=source_idx).datatype

                        if source_dt != dest_dt:
                            raise ValidationError(('ExecRecord "{}" represents a cable, but the Datatype '
                                                   'of its destination column, "{}", does not match the Datatype '
                                                   'of its source column, "{}"').format(self, dest_dt, source_dt))

    def complete_clean(self):
        """
        Checks completeness of the ExecRecord.

        Calls clean, and then checks that all in/outputs of the
        Method/POC/PSIC are quenched.
        """
        self.clean()

        # Because we know that each ERI is clean (and therefore each
        # one maps to a valid input of our Method/POC/PSIC), and
        # because there is no multiple quenching (due to a uniqueness
        # constraint), all we have to do is check the number of ERIs
        # to make sure everything is quenched.
        if type(self.general_transf()) in (
                pipeline.models.PipelineOutputCable,
                pipeline.models.PipelineStepInputCable
                ):
            # In this case we check that there is an input and an output.
            if not self.execrecordins.all().exists():
                raise ValidationError(
                    "Input to ExecRecord \"{}\" is not quenched".format(self))
            if not self.execrecordouts.all().exists():
                raise ValidationError(
                    "Output of ExecRecord \"{}\" is not quenched".format(self))

        else:
            if self.execrecordins.count() != self.general_transf().inputs.count():
                raise ValidationError(
                    "Input(s) to ExecRecord \"{}\" are not quenched".format(self))
        
            # Similar for EROs.
            if self.execrecordouts.count() != self.general_transf().outputs.count():
                raise ValidationError(
                    "Output(s) of ExecRecord \"{}\" are not quenched".format(self))

    def general_transf(self):
        """Returns the Method/POC/PSIC represented by this ExecRecord."""
        if self.generator.record.is_cable:
            return self.generator.record.component
        else:
            # This is a Method.
            return self.generator.record.component.transformation.definite

    def provides_outputs(self, outputs):
        """
        Checks whether this ER has existent data for these outputs.
        outputs: an iterable of TOs we want the ER to have real data for.

        PRE
        1) outputs must be TransformationOutputs of the Transformation associated
        with the RunStep/RunSIC/RunOutputCable associated with this ExecRecord 
        (they cannot be arbitrary TransformationOutputs).
        """
        # Load each TO in outputs
        for curr_output in outputs:
            corresp_ero = self.execrecordouts.get(generic_output=curr_output)

            if not corresp_ero.has_data():
                self.logger.debug(
                    "corresponding ERO doesn't have data - ER doesn't have existent data for all TOs requested")
                return False

        self.logger.debug("all outputs needed have corresponding EROs with data")
        return True

    def outputs_OK(self):
        """Checks whether all of the EROs of this ER are OK."""
        return all([ero.is_OK() for ero in self.execrecordouts.all()])

    def has_ever_failed(self):
        """Has any execution of this ExecRecord ever failed?"""
        # Go through all RunSteps using this ExecRecord.
        runsteps_using_this = []
        for component_using_this in self.used_by_components.all():
            if component_using_this.is_step:
                runsteps_using_this.append(component_using_this.runstep)
        return any(not runstep.successful_execution() for runstep in runsteps_using_this)


class ExecRecordIn(models.Model):
    """
    Denotes a symbolic input fed to the Method/POC/PSIC in the parent ExecRecord.

    The symbolic input may map to deleted data, e.g. if it was a deleted output
    of a previous step in a pipeline.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord", related_name="execrecordins")
    symbolicdataset = models.ForeignKey(SymbolicDataset, help_text="Symbol for the dataset fed to this input")

    # For a Method/Pipeline, this denotes the input that this ERI refers to;
    # for a cable, this denotes the thing that "feeds" it.
    generic_input = models.ForeignKey(transformation.models.TransformationXput)

    class Meta:
        unique_together = ("execrecord", "generic_input");

    def __unicode__(self):
        """
        Unicode representation.
        
        If this ERI represents the source of a POC/PSIC, then it looks like
        [symbolic dataset]
        If it represents a TI, then it looks like
        [symbolic dataset]=>[transformation (raw) input name]
        
        Examples:
        S552
        S552=>foo_bar

        PRE: the parent ER must exist and be clean.
        """
        dest_name = "";

        if (type(self.execrecord.general_transf()) in
                (pipeline.models.PipelineOutputCable,
                 pipeline.models.PipelineStepInputCable)):
            return unicode(self.symbolicdataset)
        else:
            dest_name = self.generic_input.definite.dataset_name

        return "{}=>{}".format(self.symbolicdataset, dest_name)

    def clean(self):
        """
        Checks coherence of this ExecRecordIn.

        Checks that generic_input is appropriate for the parent
        ExecRecord's Method/POC/PSIC.
        - If execrecord is for a POC, then generic_input should be the TO that
          feeds it (i.e. the PipelineStep TO that is cabled to a Pipeline output).
        - If execrecord is for a PSIC, then generic_input should be the TO or TI
          that feeds it (TO if it's from a previous step; TI if it's from a Pipeline
          input).
        - If execrecord is for a Method, then generic_input is the TI
          that this ERI represents.
          
        Also, if symbolicdataset refers to existent data, check that it
        is compatible with the input represented.
        """
        parent_transf = self.execrecord.general_transf()

        # If ER links to POC, ERI must link to TO which the outcable runs from.
        if type(parent_transf) == pipeline.models.PipelineOutputCable:
            if self.generic_input.definite != parent_transf.source.definite:
                raise ValidationError(
                    "ExecRecordIn \"{}\" does not denote the TO that feeds the parent ExecRecord POC".
                    format(self))
        # Similarly for a PSIC.
        elif type(parent_transf) == pipeline.models.PipelineStepInputCable:
            if self.generic_input.definite != parent_transf.source.definite:
                raise ValidationError(
                    "ExecRecordIn \"{}\" does not denote the TO/TI that feeds the parent ExecRecord PSIC".
                    format(self))

        else:
            # The ER represents a Method (not a cable).  Therefore the
            # ERI must refer to a TI of the parent ER's Method.
            if type(self.generic_input) == transformation.models.TransformationOutput:
                raise ValidationError(
                    "ExecRecordIn \"{}\" must refer to a TI of the Method of the parent ExecRecord".
                    format(self))

            transf_inputs = parent_transf.inputs
            if not transf_inputs.filter(pk=self.generic_input.pk).exists():
                raise ValidationError(
                    "Input \"{}\" does not belong to Method of ExecRecord \"{}\"".
                    format(self.generic_input, self.execrecord))


        # The ERI's SymbolicDataset raw/unraw state must match the
        # raw/unraw state of the generic_input that it feeds it (if ER is a cable)
        # or that it is fed into (if ER is a Method).
        if self.generic_input.is_raw() != self.symbolicdataset.is_raw():
            raise ValidationError(
                "SymbolicDataset \"{}\" cannot feed source \"{}\"".
                format(self.symbolicdataset, self.generic_input))

        if not self.symbolicdataset.is_raw():
            transf_xput_used = self.generic_input
            cdt_needed = self.generic_input.get_cdt()
            input_SD = self.symbolicdataset

            # CDT of input_SD must be a restriction of cdt_needed,
            # i.e. we can feed it into cdt_needed.
            if not input_SD.structure.compounddatatype.is_restriction(
                    cdt_needed):
                raise ValidationError(
                    "CDT of SymbolicDataset \"{}\" is not a restriction of the required CDT".
                    format(input_SD))

            # Check row constraints.
            if (transf_xput_used.get_min_row() != None and
                    input_SD.num_rows() < transf_xput_used.get_min_row()):
                error_str = ""
                if type(self.generic_input) == transformation.models.TransformationOutput:
                    error_str = "SymbolicDataset \"{}\" has too few rows to have come from TransformationOutput \"{}\""
                else:
                    error_str = "SymbolicDataset \"{}\" has too few rows for TransformationInput \"{}\""
                raise ValidationError(error_str.format(input_SD, transf_xput_used))
                    
            if (transf_xput_used.get_max_row() != None and
                input_SD.num_rows() > transf_xput_used.get_max_row()):
                error_str = ""
                if type(self.generic_input) == transformation.models.TransformationOutput:
                    error_str = "SymbolicDataset \"{}\" has too many rows to have come from TransformationOutput \"{}\""
                else:
                    error_str = "SymbolicDataset \"{}\" has too many rows for TransformationInput \"{}\""
                raise ValidationError(error_str.format(input_SD, transf_xput_used))

    def is_OK(self):
        """Checks if the associated SymbolicDataset is OK."""
        return self.symbolicdataset.is_OK()


class ExecRecordOut(models.Model):
    """
    Denotes a symbolic output from the Method/PSIC/POC in the parent ExecRecord.

    The symbolic output may map to deleted data, i.e. if it was deleted after
    being generated.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord",
                                   related_name="execrecordouts")
    symbolicdataset = models.ForeignKey(
        SymbolicDataset,
        help_text="Symbol for the dataset coming from this output",
        related_name="execrecordouts")

    # For a Method/Pipeline this represents the TO that produces this output.
    # For a cable, this represents the TO (for a POC) or TI (for a PSIC) that
    # this cable feeds into.
    generic_output = models.ForeignKey(transformation.models.TransformationXput,
                                       related_name="execrecordouts_referencing")

    class Meta:
        unique_together = ("execrecord", "generic_output");

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __unicode__(self):
        """
        Unicode representation of this ExecRecordOut.

        If this ERO represented the output of a PipelineOutputCable, then this looks like
        [symbolic dataset]
        If it represents the input that a PSIC feeds into, then it looks like
        [symbolic dataset]
        Otherwise, it represents a TransformationOutput, and this looks like
        [TO name]=>[symbolic dataset]
        e.g.
        S458
        output_one=>S458
        """
        unicode_rep = u""
        if (type(self.execrecord.general_transf()) in
                (pipeline.models.PipelineOutputCable,
                 pipeline.models.PipelineStepInputCable)):
            unicode_rep = unicode(self.symbolicdataset)
        else:
            unicode_rep = u"{}=>{}".format(self.generic_output.definite.dataset_name,
                                           self.symbolicdataset)
        return unicode_rep


    def clean(self):
        """
        If ER represents a POC, check output defined by the POC.
        If ER represents a PSIC, check output is the TI the cable feeds.
        If ER is not a cable, check output belongs to ER's Method.
        The SD is compatible with generic_output. (??)
        """
        # If the parent ER is linked with POC, the corresponding ERO TO must be coherent
        if (type(self.execrecord.general_transf()) == pipeline.models.PipelineOutputCable):
            parent_er_outcable = self.execrecord.general_transf()

            # ERO TO must belong to the same pipeline as the ER POC
            if self.generic_output.definite.transformation.definite != parent_er_outcable.pipeline:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not belong to the same pipeline as its parent ExecRecord POC".
                    format(self))

            # And the POC defined output name must match the pipeline TO name
            if parent_er_outcable.output_name != self.generic_output.definite.dataset_name:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not represent the same output as its parent ExecRecord POC".
                    format(self))

        # Second case: parent ER represents a PSIC.
        elif (type (self.execrecord.general_transf()) == pipeline.models.PipelineStepInputCable):
            parent_er_psic = self.execrecord.general_transf()

            # This ERO must point to a TI.
            if not self.generic_output.is_input:
                raise ValidationError(
                    "Parent of ExecRecordOut \"{}\" represents a PSIC; ERO must be a TransformationInput".
                    format(self))

            # June 9, 2014: this requirement has been relaxed.
            # # The TI this ERO points to must be the one fed by the PSIC.
            # if parent_er_psic.dest != self.generic_output:
            #     raise ValidationError(
            #         "Input \"{}\" is not the one fed by the PSIC of ExecRecord \"{}\"".
            #         format(self.generic_output, self.execrecord))

        # Else the parent ER is linked with a method
        else:
            query_for_outs = self.execrecord.general_transf().outputs

            # The ERO output TO must be a member of the ER's method/pipeline
            if not query_for_outs.filter(pk=self.generic_output.pk).exists():
                raise ValidationError(
                    "Output \"{}\" does not belong to Method/Pipeline of ExecRecord \"{}\"".
                    format(self.generic_output, self.execrecord))

        # Check that the SD is compatible with generic_output.

        self.logger.debug("ERO SD '{}' is raw? {}".format(self.symbolicdataset, self.symbolicdataset.is_raw()))
        self.logger.debug("ERO generic_output '{}' {} is raw? {}".format(self.generic_output, type(self.generic_output), self.generic_output.is_raw()))

        # If SD is raw, the ERO output TO must also be raw
        if self.symbolicdataset.is_raw() != self.generic_output.is_raw():
            if type(self.generic_output) == pipeline.models.PipelineStepInputCable:
                raise ValidationError(
                    "SymbolicDataset \"{}\" cannot feed input \"{}\"".
                    format(self.symbolicdataset, self.generic_output))
            else:
                raise ValidationError(
                    "SymbolicDataset \"{}\" cannot have come from output \"{}\"".
                    format(self.symbolicdataset, self.generic_output))



        # SD must satisfy the CDT / row constraints of the producing TO (Methods/Pipelines/POCs)
        # or of the TI fed (PSIC case)
        if not self.symbolicdataset.is_raw():
            input_SD = self.symbolicdataset

            # If this execrecord refers to a Method, the SD CDT
            # must *exactly* be generic_output's CDT since it was
            # generated by this Method.

            if type(self.execrecord.general_transf()) == method.models.Method:
                if input_SD.structure.compounddatatype != self.generic_output.get_cdt():
                    raise ValidationError(
                        'CDT of SymbolicDataset "{}" is not the CDT of the TransformationOutput "{}" of the generating Method'.
                        format(input_SD, self.generic_output))

            # For POCs, ERO SD's CDT must be >>identical<< to generic_output's CDT, because it was generated either
            # by this POC or by a compatible one.
            # FIXME: self.generic_output.get_cdt().is_restriction(self.symbolicdataset.structure.compounddatatype)

            elif type(self.execrecord.general_transf()) == pipeline.models.PipelineOutputCable:
                if not self.symbolicdataset.structure.compounddatatype.is_identical(self.generic_output.get_cdt()):
                    raise ValidationError(
                        "CDT of SymbolicDataset \"{}\" is not identical to the CDT of the TransformationOutput \"{}\" of the generating Pipeline".
                        format(input_SD, self.generic_output))
                    
            # If it refers to a PSIC, then SD CDT must be a
            # restriction of generic_output's CDT.
            else:
                if not input_SD.structure.compounddatatype.is_restriction(self.generic_output.get_cdt()):
                    raise ValidationError(
                        "CDT of SymbolicDataset \"{}\" is not a restriction of the CDT of the fed TransformationInput \"{}\"".
                        format(input_SD, self.generic_output))

            if (self.generic_output.get_min_row() != None and
                    input_SD.num_rows() < self.generic_output.get_min_row()):
                if (type(self.execrecord.general_transf()) ==
                        pipeline.models.PipelineStepInputCable):
                    raise ValidationError(
                        "SymbolicDataset \"{}\" feeds TransformationInput \"{}\" but has too few rows".
                        format(input_SD, self.generic_output))
                else:
                    raise ValidationError(
                        "SymbolicDataset \"{}\" was produced by TransformationOutput \"{}\" but has too few rows".
                        format(input_SD, self.generic_output))

            if (self.generic_output.get_max_row() != None and 
                    input_SD.num_rows() > self.generic_output.get_max_row()):
                if (type(self.execrecord.general_transf()) ==
                        pipeline.models.PipelineStepInputCable):
                    raise ValidationError(
                        "SymbolicDataset \"{}\" feeds TransformationInput \"{}\" but has too many rows".
                        format(input_SD, self.generic_output))
                else:
                    raise ValidationError(
                        "SymbolicDataset \"{}\" was produced by TransformationOutput \"{}\" but has too many rows".
                        format(input_SD, self.generic_output))

        self.logger.debug("ERO is clean")

    def has_data(self):
        """True if associated Dataset exists; False otherwise."""
        return self.symbolicdataset.has_data()

    def is_OK(self):
        """Checks if the associated SymbolicDataset is OK."""
        return self.symbolicdataset.is_OK()
