# CfE Cluster Setup

This directory contains code and instructions for setting up a multi-host compute cluster.

## Deployment to Octomore

This procedure, as of November 22, 2023, looks like the following.

### Before you wipe the old machine

Make sure your backups are in order.  System backups are typically kept using `rsnapshot`,
and a backup of the Kive PostgreSQL database is kept using `barman`.  For example,
on our production server, these are kept on a NAS mounted at `/media/dragonite`.

Preserve copies of your system's `/etc/passwd`, `/etc/group`, and `/etc/shadow`.  This 
information will be used to populate the new system with the same users and groups
from the old system.

Create a dump of the Kive PostgreSQL database using `pg_dumpall`.  As the upgrade may
involve moving to a newer version of PostgreSQL, we likely can't use the Barman
backups to migrate from; thus we must do it the "old-fashioned" way.

Preserve a copy of `/etc/kive/kive_apache.conf`.  This file contains the database
password used by Kive (via `apache2`) to access PostgreSQL.  (You can also just preserve
this password and discard the file; the file should be present in the old system's
`rsnapshot` backups anyway if needed later.)

### Install Ubuntu and do basic network setup on the head node

First, manually install Ubuntu Jammy on the head node using an Ubuntu live USB drive.
- Create a user with username `ubuntu` when prompted during installation.  This will be
  our "bootstrap" user. 
- Choose the 120GB SSD as the root drive.
- Manually set up the LAN-facing interface (probably `eno0`) with IP address and subnet 192.168.69.86/23,
  with gateway 192.168.68.1 and DHCP server 192.168.168.101.
Once this is done, you can interact with the head node via SSH.

Next, upload the contents of [cloud-init/head] to the server and run `head_configuration.bash`.
This sets up the root user's SSH key and `/etc/hosts`, and installs Ansible on the head node.
Now that Ansible is available on the root node, most of the rest of the procedure will be done
using Ansible playbooks defined in the [deployment] directory.

#### Prepare Ansible configuration

Go to the `deployment/group_vars` directory and create an `all.yaml` file from the
`octomore_template.yaml` file by copying and filling in some details.

> For the passwords, it probably makes sense to use a password generator of some form.
> However, for `kive_db_password` it makes sense to plug in the password you preserved
> from the old system, as this is the password that will be used when we restore
> the database from old backups.

Then go to `deployment/` and create an `ansible.cfg` from one of the provided templates, 
probably `ansible_octomore.cfg`.  These files will be necessary for Ansible to work.

> Note: all playbooks should be run using `sudo`!

#### General preliminary setup

The first thing to do with Ansible is to run the `octomore_preliminary_setup.yaml`
playbook.  Find the `/dev/disk/by-id/` entry that corresponds to the 10GB volume on the system 
and put the *basename* (i.e. the name of the soft link in the directory without the 
`/dev/disk/by-id/` part of the path) into `group_vars/all.yml` as the lone entry in the 
`data_physical_volumes` list.  (Or, if you wish to use several volumes combined into 
one logical volume, put all their names in this list.)  This sets up the `/data` partition,
prepares some other system stuff on the head node, and configures the internal-facing networking.
With this in place, the playbook should set up an `ext4` volume at `/data` on the drive 
you specified.

#### Set up your backup drive

Next, set up a backup drive for your system.  A sample of how this was done for Octomore
using all the leftover drives from the old system is detailed in `create_backup_filesystem.yaml`.
On another server you might use a NAS-based backup solution instead.  The goal in the end 
is to have a backup drive mounted at `/media/backup`.

### Install Ubuntu on the compute nodes

At this point, go back into the server room and install Ubuntu Jammy on the compute nodes.
These machines only have one hard drive, and their ethernet should automatically be set up
by default (the head node provides NAT and DHCP), so this should be a very straighforward
installation.  Again, create a user with username `ubuntu` to be the bootstrap user.

Now, upload the contents of [cloud-init/worker] to each compute node, along with the SSH
public key generated by the root user on the head node during the running of 
`head_configuration.bash`.  Then, run `worker_configuration.bash`, which will install
the necessary packages and set up the necessary SSH access for the node to be used with Ansible.

### Annoying detour: reassign the bootstrap user's UID and GID

At this point, you can run `reassign_bootstrap_user_uid.yaml`, which is necessary because
the `ubuntu` bootstrap user on both machines has a UID and GID that overlaps with 
a user account that will later be imported into this machine.  You may need to create a *second* 
bootstrap user to do this, as running the playbook as `ubuntu` may fail because the user
is currently being used (even if you use `sudo`).

### Import users and groups from the old system

The next playbook to run imports users from the old system.  First, a YAML file must be prepared
using `export_users_and_groups.py` from the old system's `/etc/shadow`, `/etc/passwd`, and 
`/etc/group`.  Next, run

    sudo ansible-playbook --extra-vars "@[name of the produced YAML file]" import_users.yaml

This will import user accounts into the head node.  (These will later be synchronized to the
compute node as part of a subsequent playbook.)

### Install Kive

With all of that table-setting in place, the main playbook to run is `kive_setup.yml`.  This is
the "main" playbook, and will take longer to run.

### Restore the Kive database

At this point, you should have a fresh, "empty" server.  You can now restore the Kive database
from the database dump you made earlier on the old system.

First, restore the Kive data folders from the old backups.  On our prod and dev 
clusters this folder was `/data/kive`; use `rsync -avz` to copy this information 
into place on your new server.

With these in place, you can now restore the PostgreSQL database.  First,
shut down `apache2` and `postgresql`:

```
sudo systemctl stop apache2
sudo systemctl stop postgresql@14-main
```

Next you can restore the data using `psql` as the `postgres` user:

```
sudo su -l postgres
psql -f [dumped file from the old system] postgres
```

(In the `psql` command, we specified the database `postgres`.  This will actually 
be ignored but should still be specified or else `psql` will complain.)

At this point, the database will have been restored to the old settings.  If you didn't
use it before in your Ansible configuration (i.e. in `group_vars/all.yaml`), you should
now specify the PostgreSQL password preserved from the old system in 
`/etc/kive/kive_apache.conf`.

[cloud-init/head]: ./cloud-init/head
[cloud-init/worker]: ./cloud-init/worker
[deployment]: ./deployment

## Test Environment

We can use Multipass to bring up a test environment for development purposes, or 
Vagrant.

### Multipass

The [cloud-init] directory contains templates and scripts for generating cloud-init
files to use when setting up a "head" VM and a "worker" VM.  FIXME more instructions to come

### Vagrant

This directory contains a Vagrantfile that describes two VMs (a head node and a
worker node) that can be used to test Ansible playbooks or practice performing
cluster management tasks. Ansible is installed on the `head` node, and this directory
is mounted at `/vagrant`. Playbooks can be edited from the host machine, but should
be run from the `head` node.


# Quickstart

This will guide you through setting up your test environment and running your
first Ansible commands. You'll need to have [Vagrant] and [VirtualBox] installed.

To begin, bring up the Vagrant VMs. This will create two VMs (`head` and
`worker`) and install Ansible on `head`.

    vagrant up

Next, log in to `head` and move into the test environment directory. This is where
we'll do most of our testing and practice.

    vagrant ssh head
    cd /vagrant/testenv

`ansible.cfg` contains the configuration for the test environment. Most
importantly, it directs ansible to load its inventory from
`testenv/inventory.ini` instead of from the default location under `/etc`.

From `./testenv`, you can run Ansible commands against the inventoried
hosts (including the head node).

This command runs the Ansible's `ping` module against all hosts, which checks that
they can be accessed.

    ansible -m ping all


[Vagrant]: https://www.vagrantup.com/downloads.html
[VirtualBox]: https://www.virtualbox.org/wiki/Downloads


# Architecture (for lack of a better name)

Ansible executes *tasks* against one or more managed machines. Tasks may also
depend on *variables*, *files*, or *templates*. These can be grouped into *roles*.

This project uses roles to configure servers (e.g. Slurm worker, Kive server).


# Ansible Docs

Essential:

- [Concepts](https://docs.ansible.com/ansible/latest/user_guide/basic_concepts.html)
- [Quickstart](https://docs.ansible.com/ansible/latest/user_guide/quickstart.html)

Thorough:

- [Playbooks](https://docs.ansible.com/ansible/2.3/playbooks.html)
- [How to build your inventory](https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#intro-inventory)
- [Creating Reusable Playbooks](https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse.html)
- [Module Index](https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html)
- [Best Practices](https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html#playbooks-best-practices)
- [Interpreter Discovery](https://docs.ansible.com/ansible/latest/reference_appendices/interpreter_discovery.html#interpreter-discovery)

Extended:

- [Installation](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installation-guide)
- [Become (privesc)](https://docs.ansible.com/ansible/2.3/become.html)
- ["Dry Run" mode](https://docs.ansible.com/ansible/2.3/playbooks_checkmode.html)
- [Asynchronous Actions and Polling](https://docs.ansible.com/ansible/2.3/playbooks_async.html)
- [Vault](https://docs.ansible.com/ansible/2.3/playbooks_vault.html)


# Useful modules

- [copy](https://docs.ansible.com/ansible/latest/modules/copy_module.html#copy-module)
- [user](https://docs.ansible.com/ansible/latest/modules/user_module.html#user-module)
- [file](https://docs.ansible.com/ansible/latest/modules/file_module.html#file-module), for creating directories
- [systemd](https://docs.ansible.com/ansible/latest/modules/systemd_module.html#systemd-module)
- [debug](https://docs.ansible.com/ansible/latest/modules/debug_module.html#debug-module)
- [dnf](https://docs.ansible.com/ansible/latest/modules/dnf_module.html#dnf-module) (use instead of `yum`, which is Python2 only)
- [mysql_db](https://docs.ansible.com/ansible/latest/modules/mysql_db_module.html#mysql-db-module)
- [get_url](https://docs.ansible.com/ansible/latest/modules/get_url_module.html)
- [replace](https://docs.ansible.com/ansible/latest/modules/replace_module.html)
- [firewalld](https://docs.ansible.com/ansible/latest/modules/firewalld_module.html)
- [command](https://docs.ansible.com/ansible/latest/modules/command_module.html)
- [Postgresql Modules](https://docs.ansible.com/ansible/latest/modules/list_of_database_modules.html#postgresql)
- [lineinfile](https://docs.ansible.com/ansible/latest/modules/lineinfile_module.html)
- [blockinfile](https://docs.ansible.com/ansible/latest/modules/blockinfile_module.html#blockinfile-module)
- [git](https://docs.ansible.com/ansible/latest/modules/git_module.html#git-module)
- [unarchive](https://docs.ansible.com/ansible/latest/modules/unarchive_module.html)

# Applying a single role

Per [this](https://stackoverflow.com/questions/38350674/ansible-can-i-execute-role-from-command-line)
stack overflow answer, a single role can be run with the following command:

    ansible <hostname> -m include_role -a name=<role name>

This has more verbose output and can be run in isolation, making it suitable
for development and debugging.


<!-- TODO(nknight): Move ansible reference into its own document -->
<!-- TODO(nknight): Overview of roles and environments -->
