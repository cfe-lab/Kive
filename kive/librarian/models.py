"""
librarian.models

Shipyard data models pertaining to the lookup of the past: ExecRecord,
Dataset, etc.
"""
from __future__ import unicode_literals

from collections import defaultdict
import csv
from datetime import datetime, date, timedelta
import hashlib
import heapq
import itertools
import logging
import os
import os.path
import re
import shutil
import sys
import tempfile
import time

from django.db import models, transaction
from django.core.exceptions import ValidationError, ObjectDoesNotExist
from django.core.validators import MinValueValidator, RegexValidator
from django.core.files import File
from django.utils.encoding import python_2_unicode_compatible
from django.utils import timezone
from django.conf import settings
from django.template.defaultfilters import filesizeformat
from django.db.models import Min
from django.db.models.signals import post_delete
from django.core.urlresolvers import reverse

import method.models
import pipeline.models
import transformation.models
import datachecking.models
import metadata.models
import archive.exceptions
import librarian.signals
import file_access_utils
from constants import maxlengths, runcomponentstates
from datachecking.models import BadData
import librarian.filewalker as filewalker
import six

LOGGER = logging.getLogger(__name__)


def get_upload_path(instance, filename):
    """
    Helper method for uploading dataset_files for Dataset.
    This is outside of the Dataset class, since @staticmethod and other method decorators were used instead of the
    method pointer when this method was inside Dataset class.

    :param instance:  Dataset instance
    :param filename: Dataset.dataset_file.name
    :return:  The upload directory for Dataset files.
    """
    return instance.UPLOAD_DIR + os.sep + time.strftime('%Y_%m') + os.sep + filename


@python_2_unicode_compatible
class ExternalFileDirectory(models.Model):
    """
    A database table storing directories whose contents we can make Datasets out of.
    """
    name = models.CharField(
        help_text="Human-readable name for this external file directory",
        unique=True,
        max_length=maxlengths.MAX_EXTERNAL_PATH_LENGTH
    )
    path = models.CharField(
        help_text="Absolute path",
        max_length=maxlengths.MAX_EXTERNAL_PATH_LENGTH
    )

    def __str__(self):
        return self.name

    def list_files(self):
        """
        Return a list of tuples representing files under this directory.

        The tuple looks like:
        ([absolute file path], [file path with external file directory name substituted])
        """
        path_with_slash = self.path if self.path.endswith("/") else "{}/".format(self.path)
        all_files = []
        for root, _dirs, files in sorted(os.walk(self.path)):
            for f in files:
                f = os.path.join(root, f)
                all_files.append((f, f.replace(path_with_slash, "[{}]/".format(self.name), 1)))
        return all_files

    def save(self, *args, **kwargs):
        """
        Normalize the path before saving.
        """
        self.path = os.path.normpath(self.path)
        super(ExternalFileDirectory, self).save(*args, **kwargs)


class SafeContext:
    def __enter__(self):
        pass

    def __exit__(self, *args):
        pass


@python_2_unicode_compatible
class Dataset(metadata.models.AccessControl):
    """
    A (possibly temporary) data file.

    That is to say, at some point, there was a data file uploaded to/
    generated by Shipyard, which was coherent with its
    specified/generating CDT and its producing
    TransformationOutput/cable (if it was generated), and this
    represents it, whether or not it was saved to the database.

    PRE: the actual file that the Dataset represents (whether
    it still exists or not) is/was coherent (e.g. checked using
    CDT.summarize_csv()).
    """
    UPLOAD_DIR = "Datasets"  # This is relative to kive.settings.MEDIA_ROOT

    name = models.CharField(max_length=maxlengths.MAX_FILENAME_LENGTH,
                            help_text="Name of this Dataset.",
                            blank=True)
    description = models.TextField(help_text="Description of this Dataset.",
                                   max_length=maxlengths.MAX_DESCRIPTION_LENGTH,
                                   blank=True)
    date_created = models.DateTimeField(default=timezone.now,
                                        help_text="Date of Dataset creation.",
                                        db_index=True)

    # Four cases from which Datasets can originate:
    #
    # Case 1: uploaded
    # Case 2: from the transformation of a RunStep
    # Case 3: from the execution of a POC (i.e. from a ROC)
    # Case 4: from the execution of a PSIC (i.e. from a RunSIC)
    file_source = models.ForeignKey("archive.RunComponent", related_name="outputs", null=True, blank=True)

    # Datasets are stored in the "Datasets" folder
    dataset_file = models.FileField(upload_to=get_upload_path,
                                    help_text="Physical path where datasets are stored",
                                    blank=True,
                                    default='',
                                    db_index=True,
                                    max_length=maxlengths.MAX_FILENAME_LENGTH)

    externalfiledirectory = models.ForeignKey(
        ExternalFileDirectory,
        verbose_name="External file directory",
        help_text="External file directory containing the data file",
        null=True,
        blank=True
    )
    external_path = models.CharField(
        help_text="Relative path of the file within the specified external file directory",
        blank=True,
        max_length=maxlengths.MAX_EXTERNAL_PATH_LENGTH
    )

    logger = logging.getLogger('librarian.Dataset')

    # This class has a FilePurger instance. See purge() below.
    filepurger = filewalker.FilePurger(os.path.join(settings.MEDIA_ROOT, UPLOAD_DIR),
                                       grace_period_hrs=settings.DATASET_GRACE_PERIOD_HRS,
                                       walk_period_hrs=settings.DATASET_PURGE_SCAN_PERIOD_HRS,
                                       logger=logger)
    # For validation of Datasets when being reused, or when being
    # regenerated.  A blank MD5_checksum means that the file was
    # missing (not created when it was supposed to be created).
    MD5_checksum = models.CharField(
        max_length=64,
        validators=[RegexValidator(
            regex=re.compile("(^[0-9A-Fa-f]{32}$)|(^$)"),
            message="MD5 checksum is not either 32 hex characters or blank")],
        blank=True,
        default="",
        help_text="Validates file integrity")

    _redacted = models.BooleanField(default=False)

    # The last time a check was performed on this external file, to see whether
    # the external file referenced was still there.
    # See self.idle_externalcheck() for details.
    last_time_checked = models.DateTimeField(default=timezone.now,
                                             help_text="Date-time of last (external) dataset existence check.",
                                             null=True)

    class Meta:
        ordering = ["-date_created", "name"]

    def __init__(self, *args, **kwargs):
        super(Dataset, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """
        Unicode representation of a Dataset.

        This is simply S[pk] if it has no data.
        """
        if not self.has_data():
            return "S{}".format(self.pk)

        display_name = self.name if self.name != "" else "[no name specified]"

        return "{} (created by {} on {})".format(display_name, self.user, self.date_created)

    def external_absolute_path(self):
        if not self.external_path:
            return None
        return os.path.normpath(os.path.join(self.externalfiledirectory.path, self.external_path))

    def get_open_file_handle(self, mode="rb"):
        """
        Retrieves an open Django file with which to access the data.

        This is self.dataset_file if possible, falls back to the external file if possible,
        and otherwise returns None.

        NOTE: for python3 there is a significant difference in opening a file in binary or
        text mode.
        Use binary when calculating hashes.
        Use text when read the CSV contents.
        """
        if self.dataset_file:
            try:
                self.dataset_file.open(mode)
            except IOError as e:
                self.logger.warn('error accessing dataset file: %s', e)
                return None
            return self.dataset_file
        elif self.external_path:
            abs_path = self.external_absolute_path()
            if os.path.exists(abs_path) and os.access(abs_path, os.R_OK):
                try:
                    fhandle = open(abs_path, mode)
                except IOError as e:
                    self.logger.warn('error accessing external file: %s', e)
                    return None
                return File(fhandle, name=abs_path)
        return None

    def all_rows(self, data_check=False, insert_at=None, limit=None, extra_errors=None):
        """ Returns an iterator over all rows of this Dataset.

        :param bool data_check: each field becomes a tuple: (value, [error])
        :param list insert_at: [column_index] add blank columns at each
        zero-based index
        :param int limit: maximum row number returned (header is not counted)
        :param list extra_errors: this will have extra rows added to it that
        contain the first error in each column, if they appear after the row
        limit. [(row_num, [(field_value, [error])])] row_num is 1-based.
        :return: an iterator over the rows, each row is either [field_value] or
        [(field_value, [error])], depending on data_check.
        """
        data_handle = self.get_open_file_handle("r")
        if data_handle is None:
            raise RuntimeError('Dataset file has been removed.')

        with data_handle:
            reader = csv.reader(data_handle)
            if data_check:
                try:
                    content_check = self.content_checks.first()
                    if content_check is None:
                        raise BadData.DoesNotExist('Dataset has no content checks')
                    baddata = content_check.baddata
                    cell_errors = baddata.cell_errors.order_by('row_num', 'column')
                    if limit is not None:
                        cell_errors = cell_errors.filter(row_num__lte=limit)
                    row_errors = defaultdict(dict)  # {row_num: {col_num: column}}
                    for error in cell_errors:
                        row_error = row_errors[error.row_num]
                        row_error[error.column.column_idx] = error.column
                    if extra_errors is not None:
                        first_errors = baddata.cell_errors.values(
                            'column_id').annotate(Min('row_num')).order_by(
                                'row_num__min')
                        columns = None
                        for error in first_errors:
                            if columns is None:
                                members = self.structure.compounddatatype.members.all()
                                columns = {col.id: col for col in members}
                            failed_column = columns[error['column_id']]
                            row_error = row_errors[error['row_num__min']]
                            row_error[failed_column.column_idx] = failed_column
                except BadData.DoesNotExist:
                    row_errors = {}
            for row_num, row in enumerate(reader):
                if not data_check:
                    if limit is not None and row_num > limit:
                        break
                else:
                    row_error = row_errors.pop(row_num, {})
                    if limit is not None and row_num > limit and not row_error:
                        if row_errors:
                            # Still have errors on later rows
                            continue
                        # No more errors
                        break
                    new_row = []
                    for column_num, value in enumerate(row, 1):
                        failed_column = row_error.get(column_num)
                        if failed_column is None:
                            new_errors = []
                        else:
                            new_errors = failed_column.check_basic_constraints(value)
                        new_row.append((value, new_errors))

                    row = new_row
                if insert_at is not None:
                    dummy = ('', []) if data_check else ''
                    [row.insert(pos, dummy) for pos in insert_at]
                if limit is None or row_num <= limit:
                    yield row
                else:
                    extra_errors.append((row_num, row))

    def header(self):
        rows = self.all_rows()
        return next(rows)

    def rows(self, data_check=False, insert_at=None, limit=None, extra_errors=None):
        rows = self.all_rows(data_check,
                             insert_at,
                             limit=limit,
                             extra_errors=extra_errors)
        for i, row in enumerate(rows):
            if i == 0:
                pass  # skip header
            else:
                yield row

    def expected_header(self):
        header = []
        if not self.is_raw():
            header = [c.column_name for c in self.compounddatatype.members.order_by("column_idx")]
        return header

    @property
    def content_matches_header(self):
        observed = self.header()

        # Cache this so we only hit the db once here
        if hasattr(self, "_expected_header_cache"):
            expected = self._expected_header_cache
        else:
            expected = self._expected_header_cache = self.expected_header()

        if len(observed) != len(expected):
            return False
        return not any([o != x for (o, x) in zip(observed, expected)])

    def column_alignment(self):
        """
        This function looks at the expected and observed headers for
        a Dataset, and tries to align them if they don't match.

        :return: a tuple whose first element is a list of tuples
        i.e (expected header name, observed header name), and
        whose second element is a list of gaps indicating where
        to insert blank fields in a row
        """
        expt = self.expected_header()
        obs = self.header()
        i, insert = 0, []

        if self.is_raw() and not self.content_matches_header:
            return None, None

        # Do a greedy 'hard matching' over the columns
        while i < max(len(expt), len(obs)) - 1:
            ex, ob = zip(*(map(None, expt, obs)[i:]))
            u_score = float('inf')
            l_score = float('inf')

            for j, val in enumerate(ob):
                if val == ex[0]:
                    u_score = j
            for j, val in enumerate(ex):
                if val == ob[0]:
                    l_score = j
            if l_score == u_score == float('inf'):
                pass
            elif u_score < l_score and u_score != float('inf'):
                [expt.insert(i, "") for _ in range(u_score)]
            elif l_score <= u_score and l_score != float('inf'):
                [obs.insert(i, "") for _ in range(l_score)]
                insert += [i] * l_score  # keep track of where to insert columns in the resulting view
            i += 1

        # it would be nice to do a similar soft matching to try to
        # match columns that are close to being the same string

        # Pad out the arrays
        diff = abs(len(expt)-len(obs))
        if len(expt) > len(obs):
            obs += [""] * diff
        else:
            expt += [""] * diff

        return zip(expt, obs), insert

    @property
    def compounddatatype(self):
        if self.is_raw():
            return None
        return self.structure.compounddatatype

    def clean(self):
        """
        Checks coherence of this Dataset.

        If it has data (i.e. an associated Dataset), it cleans that
        Dataset.  Then, if there is an associated DatasetStructure,
        clean that.

        Note that the MD5 checksum is already checked via a validator.
        """
        if self.has_structure():
            self.structure.clean()

        if self.file_source is not None:
            # Whatever run created this Dataset must have had access to the parent Dataset.
            self.file_source.definite.top_level_run.validate_restrict_access([self])

        if not (self.externalfiledirectory and self.external_path or
                not self.externalfiledirectory and not self.external_path):
            raise ValidationError(
                {
                    "external_path": "Both externalfiledirectory and external_path should be set or "
                                     "neither should be set"
                }
            )

        if self.has_data() and not self.check_md5():
            error_str = ('File integrity of "{}" lost. Current checksum "{}" does not equal expected checksum ' +
                         '"{}"').format(self, self.compute_md5(), self.MD5_checksum)
            raise ValidationError(
                {
                    "dataset_file": error_str
                }
            )

    def validate_uniqueness_on_upload(self, *args, **kwargs):
        """
        Validates that the name and MD5 of the Dataset are unique.

        This isn't at the model level because we do want to allow
        these duplicates (e.g. empty files generated by the same
        Pipeline), but we want to check files on upload.
        """
        query = Dataset.objects.filter(MD5_checksum=self.MD5_checksum,
                                       name=self.name)

        if query.exclude(pk=self.pk).exists():
            error_str = "A Dataset with that name and MD5 already exists."
            raise ValidationError(
                {
                    "dataset_file": error_str
                }
            )

    @property
    def absolute_url(self):
        """
        :return str: URL to access the dataset_file
        """
        return reverse('dataset_download', kwargs={"dataset_id": self.id})

    def get_filesize(self):
        """
        :return int: size of dataset_file in bytes or None if the file handle
        cannot be accessed.
        """
        data_handle = None
        try:
            data_handle = self.get_open_file_handle("rb")
            if data_handle is None:
                return None
            return data_handle.size
        finally:
            if data_handle is not None:
                data_handle.close()

    def get_basename_and_formatted_size(self):
        """
        :return str: basename of the dataset file.
        :return str: size of dataset_file in bytes.
        If the file handle cannot be accessed, this routine returns (None, 'missing').
        """
        data_handle = None
        try:
            data_handle = self.get_open_file_handle("rb")
            if data_handle is None:
                return None, 'missing'
            return os.path.basename(data_handle.name), filesizeformat(data_handle.size)
        finally:
            if data_handle is not None:
                data_handle.close()

    def get_formatted_filesize(self):
        unformatted_size = self.get_filesize()
        if unformatted_size is None:
            return 'missing'
        return filesizeformat(unformatted_size)

    def compute_md5(self):
        """Computes the MD5 checksum of the Dataset.
        Return None if the file could not be accessed.
        """
        data_handle = self.get_open_file_handle("rb")
        if data_handle is None:
            self.logger.warn('cannot access file handle')
            return None
        with data_handle:
            return file_access_utils.compute_md5(data_handle.file)

    def check_md5(self):
        """
        Checks the MD5 checksum of the Dataset against its stored value.

        The stored value is used when regenerating data
        that once existed, as a coherence check.

        Return True if the check passed, otherwise False.
        """
        # Recompute the MD5, see if it equals what is already stored
        new_md5 = self.compute_md5()
        if self.MD5_checksum != new_md5:
            if self.dataset_file:
                filename = self.dataset_file.name
            else:
                filename = self.external_absolute_path()
            self.logger.warn('MD5 mismatch for %s: expected %s, but was %s.',
                             filename,
                             self.MD5_checksum,
                             new_md5)
            return False
        return True

    def has_data(self):
        data_handle = self.get_open_file_handle("rb")
        if data_handle is not None:
            data_handle.close()
            return True
        return False

    def has_structure(self):
        """True if associated DatasetStructure exists; False otherwise."""
        try:
            self.structure
        except ObjectDoesNotExist:
            return False
        return self.structure.pk is not None

    def is_raw(self):
        """True if this Dataset is raw, i.e. not a CSV file."""
        # return not hasattr(self, "structure")
        try:
            self.structure
        except ObjectDoesNotExist:
            return True
        return self.structure is None

    def num_rows(self):
        """Returns number of rows in the associated Dataset.

        This returns None if the Dataset is raw.
        """
        return None if self.is_raw() else self.structure.num_rows

    def get_cdt(self):
        """
        Retrieve the CDT of this Dataset (none if it is raw).
        """
        return None if self.is_raw() else self.structure.compounddatatype

    def create_structure(self, compounddatatype, num_rows=-1):
        """Add a DatasetStructure to this Dataset."""
        if not self.is_raw():
            raise ValueError('CompoundDatatype "{}" already has a structure.')
        structure = DatasetStructure(dataset=self, compounddatatype=compounddatatype, num_rows=num_rows)
        structure.clean()
        structure.save()
        return structure

    def set_MD5(self, file_path, file_handle=None):
        """Set the MD5 hash from a file.
        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.
        :param str file_path:  Path to file to calculate MD5 for. file_path not used if file_handle supplied.
        :param file file_handle: file handle of file to calculate MD5.
                Moves file handle to beginning of file before calculating MD5.
                If file_handle empty, then uses file_path.
        """
        with file_access_utils.FileReadHandler(file_path=file_path, file_handle=file_handle, access_mode="rb") as f:
            self.MD5_checksum = file_access_utils.compute_md5(f)

    def set_MD5_and_count_rows(self, file_path, file_handle=None):
        """Set the MD5 hash and number of rows from a file.
        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.
        PRE
        This Dataset must have a DatasetStructure
        :param str file_path:  Path to file to calculate MD5 for. file_path not used if file_handle supplied.
        :param file file_handle: file handle of file to calculate MD5.
                Moves file handle to beginning of file before calculating MD5.
                If file_handle empty, then uses file_path.
        """
        assert not self.is_raw()

        num_rows = -1  # skip header
        md5gen = hashlib.md5()
        with file_access_utils.FileReadHandler(file_path=file_path,
                                               file_handle=file_handle,
                                               access_mode="r") as f:
            for line in f:
                md5gen.update(line.encode())
                num_rows += 1

        self.structure.num_rows = num_rows
        self.MD5_checksum = md5gen.hexdigest()

    @transaction.atomic
    def register_file(self, file_path, file_handle=None):
        """
        Save and register a new file for this Dataset.

        Compute and set the MD5.

        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.

        INPUTS
        file_path           file to upload as the new contents
        file_handle         file handle of the file to upload as the new contents.
                            If supplied, then does not reopen the file in file_path.
                            Moves handle to beginning of file before calculating MD5.
                            If None, then opens the file in file_path.

        PRE
        self must not have a file already associated
        """
        assert not bool(self.dataset_file)

        with file_access_utils.FileReadHandler(file_path=file_path,
                                               file_handle=file_handle,
                                               access_mode="rb") as f:
            full_name = file_path
            assert isinstance(full_name, six.string_types), "fname '{}' is not a string {}".format(f.name,
                                                                                                   type(f.name))
            fname = os.path.basename(full_name)
            self.dataset_file.save(fname, File(f))

        self.clean()
        self.save()

    def mark_missing(self, start_time, end_time, execlog, checking_user):
        """Mark a Dataset as missing output.

        INPUTS
        start_time      time when we started checking for the file
        end_time        time when check for file finished
        execlog         ExecLog of execution which did not produce
                        output
        checking_user   user that discovered the missing output
        """
        ccl = self.content_checks.create(start_time=start_time, end_time=end_time, execlog=execlog, user=checking_user)
        ccl.add_missing_output()
        return ccl

    def mark_file_not_stable(self, start_time, end_time, execlog, checking_user):
        """Mark a Dataset as having had an unstable file size.

        INPUTS
        start_time      time when we started checking for the file
        end_time        time when check for file finished
        execlog         ExecLog of execution which did not produce
                        output
        checking_user   user that discovered the missing output
        """
        ccl = self.content_checks.create(start_time=start_time, end_time=end_time, execlog=execlog, user=checking_user)
        ccl.add_file_not_stable()
        return ccl

    @classmethod
    def create_empty(cls, user=None, cdt=None, users_allowed=None, groups_allowed=None,
                     file_source=None, instance=None):
        """Create an empty Dataset.

        INPUTS
        cdt   CompoundDatatype for the new Dataset
                            (None indicates a raw Dataset)
        instance            None or a Dataset to fill in (e.g. if we get a dummy one from DatasetForm)

        OUTPUTS
        empty_SD            Dataset with a blank MD5 and an
                            appropriate DatasetStructure
        """
        users_allowed = users_allowed or []
        groups_allowed = groups_allowed or []

        if user is None:
            assert file_source is not None
            user = file_source.top_level_run.user
            users_allowed = file_source.top_level_run.users_allowed.all()
            groups_allowed = file_source.top_level_run.groups_allowed.all()
        elif file_source is not None:
            assert user == file_source.top_level_run.user
            assert set(users_allowed) == set(file_source.top_level_run.users_allowed.all())
            assert set(groups_allowed) == set(file_source.top_level_run.groups_allowed.all())

        with transaction.atomic():
            empty_SD = instance or cls()
            empty_SD.user = user
            empty_SD.MD5_checksum = ""
            empty_SD.dataset_file = None
            empty_SD.file_source = file_source
            empty_SD.last_time_checked = None
            # Save so we can add structure and permissions.
            empty_SD.save()

            if cdt:
                empty_SD.create_structure(cdt)

            for user in users_allowed:
                empty_SD.users_allowed.add(user)
            for group in groups_allowed:
                empty_SD.groups_allowed.add(group)
            empty_SD.clean()

        return empty_SD

    @classmethod
    # FIXME what does it do for num_rows when file_path is unset?
    def create_dataset(cls, file_path, user=None, users_allowed=None, groups_allowed=None,
                       cdt=None, keep_file=True,
                       name=None, description=None, file_source=None, check=True, file_handle=None,
                       instance=None, externalfiledirectory=None, precomputed_md5=None):
        """
        Helper function to make defining SDs and Datasets faster.

        user and name must both be set if make_dataset=True.
        make_dataset creates a Dataset from the given file path to go
        with the SD. file_source can be a RunAtomic to register the
        Dataset with, or None if it was uploaded by the user (or if
        make_dataset=False). If check is True, do a ContentCheck on the
        file.  file_path is an absolute path; if externalfiledirectory
        is specified, file_path will be checked to ensure that it's
        inside the specified directory.

        Returns the Dataset created.
        """
        users_allowed = users_allowed or []
        groups_allowed = groups_allowed or []

        if user is None:
            assert file_source is not None
            user = file_source.top_level_run.user
            users_allowed = file_source.top_level_run.users_allowed.all()
            groups_allowed = file_source.top_level_run.groups_allowed.all()
        elif file_source is not None:
            assert user == file_source.top_level_run.user
            assert set(users_allowed) == set(file_source.top_level_run.users_allowed.all())
            assert set(groups_allowed) == set(file_source.top_level_run.groups_allowed.all())

        if file_path:
            LOGGER.debug("Creating Dataset from file {}".format(file_path))
            file_name = file_path
        elif file_handle:
            LOGGER.debug("Creating Dataset from file {}".format(file_handle.name))
            file_name = file_handle.name
        else:
            raise ValueError("Must supply either the file path or file handle")

        if not isinstance(file_name, six.string_types):
            raise ValueError("file_name '{}' is not a string '{}'".format(file_name, type(file_name)))
        with transaction.atomic():
            external_path = ""
            # We do this in the transaction because we're accessing ExternalFileDirectory.
            if externalfiledirectory:
                # Check that file_path is in the specified ExternalFileDirectory.
                normalized_path = os.path.normpath(file_name)
                normalized_efd_with_slash = "{}/".format(os.path.normpath(externalfiledirectory.path))
                assert normalized_path.startswith(normalized_efd_with_slash)
                external_path = normalized_path.replace(normalized_efd_with_slash, "", 1)

            new_dataset = cls.create_empty(user, cdt=cdt,
                                           users_allowed=users_allowed, groups_allowed=groups_allowed,
                                           instance=instance, file_source=file_source)

            new_dataset.name = name or ""
            new_dataset.description = description or ""
            new_dataset.externalfiledirectory = externalfiledirectory
            new_dataset.external_path = external_path
            new_dataset.last_time_checked = timezone.now()

            if precomputed_md5 is not None:
                new_dataset.MD5_checksum = precomputed_md5
            elif new_dataset.is_raw():
                new_dataset.set_MD5(file_name, file_handle)
            else:
                new_dataset.set_MD5_and_count_rows(file_name, file_handle)

            if cdt is not None and check:
                run_dir = tempfile.mkdtemp(
                    prefix="SD{}_".format(new_dataset.pk),
                    dir=file_access_utils.create_sandbox_base_path()
                )
                file_access_utils.configure_sandbox_permissions(run_dir)

                # Note that this may raise a VerificationMethodError if a CustomConstraint
                # verification method fails.  We allow the error to propagate
                # up.
                content_check = new_dataset.check_file_contents(
                    file_path_to_check=file_name,
                    file_handle=file_handle,
                    summary_path=run_dir,
                    min_row=None,
                    max_row=None,
                    execlog=None,
                    checking_user=user
                )

                shutil.rmtree(run_dir)
                if content_check.is_fail():
                    if content_check.baddata.bad_header:
                        raise ValueError('The header of file "{}" does not match the CompoundDatatype "{}"'
                                         .format(file_name, cdt))
                    elif content_check.baddata.cell_errors.exists():
                        error = content_check.baddata.cell_errors.first()
                        cdtm = error.column
                        if error.has_blank_error():
                            raise ValueError(
                                'Entry ({},{}) of file "{}" is blank.'.format(
                                    error.row_num, cdtm.column_idx, file_name)
                            )
                        else:
                            raise ValueError(
                                'The entry at row {}, column {} of file "{}" did not pass the constraints of '
                                'Datatype "{}"'.format(error.row_num, cdtm.column_idx, file_name, cdtm.datatype)
                            )
                    else:
                        # Shouldn't reach here.
                        raise ValueError('The file "{}" was malformed'.format(file_name))
                LOGGER.debug("Read {} rows from file {}".format(new_dataset.structure.num_rows, file_name))

            if keep_file:
                new_dataset.register_file(file_path=file_name, file_handle=file_handle)

            new_dataset.clean()
            if not new_dataset.is_raw():
                new_dataset.structure.save()
            new_dataset.save()
        return new_dataset

    @classmethod
    # FIXME what does it do for num_rows when file_path is unset?
    def create_dataset_bulk(cls, csv_file_path, user, users_allowed=None, groups_allowed=None, csv_file_handle=None,
                            cdt=None, keep_files=True, file_source=None, check=True):
        """
        Helper function to make defining multiple SDs and Datasets faster.
        Instead of specifying datasets one by one,
        specify multiple datasets in a CSV.

        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.

        The CSV must have these columns, not necessarily in this order:
        - Name
        - Description
        - File

        make_dataset creates a Dataset from the given file path to go
        with the SD. file_source can be a RunAtomic to register the
        Dataset with, or None if it was uploaded by the user (or if
        make_dataset=False). If check is True, do a ContentCheck on the
        file.  If this fails, then a ValueError is raised and no changes
        are made to the database.

        Returns the Dataset created.
        :rtype : object
        :param csv_file_path:  path to csv file.  Not used if csv_file_handle supplied.
        :param csv_file_handle:  file handle of csv.  If supplied, then does not
            reopen file and moves handle to beginning of file. If None, then
            uses csv_file_path.
        :param cdt:
        :param keep_files:
        :param user:
        :param file_source:
        :param check:
        """
        new_datasets = []
        if csv_file_path:
            LOGGER.debug("Creating Datasets from csv {}".format(csv_file_path))
        elif csv_file_handle:
            LOGGER.debug("Creating Datasets from csv {}".format(csv_file_handle.name))
        else:
            raise Exception("Must supply either the csv file path or csv file handle")

        with file_access_utils.FileReadHandler(
                file_path=csv_file_path,
                file_handle=csv_file_handle,
                access_mode='rU') as fh_datasets:
            # TODO:  this is a major db blocking call.  Can we break this up?
            try:
                with transaction.atomic():
                    line = 0
                    reader = csv.DictReader(fh_datasets)
                    for row in reader:
                        line += 1
                        name = row['Name'].strip() if row['Name'] else ""
                        desc = row['Description'].strip() if row['Description'] else ""
                        file_name = row['File'].strip() if row['File'] else ""

                        # check for empty entries:
                        if not (name and desc and file_name):
                            raise ValueError(
                                "Line " + str(line) +
                                " is invalid: Name, Description, File must be defined")

                        new_dataset = Dataset.create_dataset(
                            file_path=file_name, user=user, users_allowed=users_allowed,
                            groups_allowed=groups_allowed, cdt=cdt,
                            keep_file=keep_files, name=name, description=desc,
                            file_source=file_source, check=check
                        )

                        new_datasets.extend([new_dataset])
            except Exception as e:
                message = "Error while parsing line " + str(line) + ":\n" + str(row)
                LOGGER.exception(message)
                six.reraise(ValueError(message + ":\n" + str(e)), None, sys.exc_info()[2])

        return new_datasets

    # FIXME: use a transaction!
    # TODO: clean this up, end_time is set in too many places
    def check_file_contents(self, file_path_to_check, summary_path, min_row, max_row, execlog,
                            checking_user, file_handle=None, notify_all=True):
        """
        Performs content check on a file, generates a CCL, and sets this
        SD's num_rows.

        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.

        OUTPUTS
        If SD is raw, creates a clean CCL.
        If not raw, checks the file and returns CCL with/without a
        corresponding BadData.

        PRE
        Should never be called twice on the same dataset, as
        this would overwrite num_rows to a potentially new value?

        :param str file_path_to_check:  Path to file to check. file_path_to_check not used if file_handle supplied.
        :param file file_handle: file handle of file to check.
                Moves file handle to beginning of file before checking.
                If file_handle empty, then uses file_path_to_check.
        :param summary_path:
        :param min_row:
        :param max_row:
        :param execlog:
        :rtype ContentCheckLog :
        """
        self.logger.debug("Creating clean ContentCheckLog for file {} and linking to ExecLog"
                          .format(file_path_to_check))
        ccl = self.content_checks.create(execlog=execlog, user=checking_user)
        ccl.start(save=True)

        if self.is_raw():
            ccl.stop(save=True, clean=False)
            ccl.clean()
            return ccl

        my_CDT = self.get_cdt()
        file_path_to_report = (file_handle.name
                               if file_handle
                               else file_path_to_check)

        # This may raise a VerificationMethodError; if so, then throw away the ContentCheckLog.
        try:
            with file_access_utils.FileReadHandler(file_path=file_path_to_check,
                                                   file_handle=file_handle,
                                                   access_mode="r") as f:
                csv_summary = my_CDT.summarize_csv(f)
        except metadata.models.VerificationMethodError:
            self.logger.error("ContentCheckLog for file %s failed because the verification method failed",
                              file_path_to_check)
            ccl.delete()
            raise

        if "bad_num_cols" in csv_summary or "bad_col_indices" in csv_summary:
            self.logger.warn("malformed header in %r.", file_path_to_report)
            ccl.add_bad_header()
            ccl.stop(save=True, clean=True)
            return ccl

        if csv_summary["num_rows"] == 0:
            self.logger.debug("file had no rows in %r.", file_path_to_report)

        csv_baddata = False
        self.structure.num_rows = csv_summary["num_rows"]
        self.structure.save()
        if max_row is not None and csv_summary["num_rows"] > max_row:
            self.logger.warn("too many rows in %r.", file_path_to_report)
            # FIXME: Do we only create these BD objects if they don't already exist?
            ccl.add_bad_num_rows()
            csv_baddata = True

        if min_row is not None and csv_summary["num_rows"] < min_row:
            self.logger.warn("too few rows in %r.", file_path_to_report)
            # FIXME: Do we only create these BD objects if they don't already exist?
            ccl.add_bad_num_rows()
            csv_baddata = True

        if "failing_cells" in csv_summary:
            self.logger.warn("cells failed datatype check in %r.", file_path_to_report)

            if not csv_baddata:
                bad_data = BadData.objects.create(contentchecklog=ccl)
                csv_baddata = True

            for row, col in csv_summary["failing_cells"]:
                fails = csv_summary["failing_cells"][(row, col)]
                for failed_constr in fails:
                    new_cell_error = bad_data.cell_errors.create(
                        row_num=row,
                        column=my_CDT.members.get(column_idx=col))

                    if failed_constr == metadata.models.CompoundDatatypeMember.BLANK_ENTRY:
                        blank_cell = datachecking.models.BlankCell(cellerror=new_cell_error)
                        blank_cell.save()
                    # If failure is a string (Ex: "Was not integer"), leave constraint_failed as null.
                    elif not isinstance(failed_constr, six.string_types):
                        new_cell_error.constraint_failed = failed_constr

                    new_cell_error.clean()
                    new_cell_error.save()

        if csv_baddata:
            self.logger.debug(
                "Content check failed - file {} does not conform to Dataset {}".
                format(file_path_to_check, self))
            if notify_all:
                self.quarantine_runcomponents_using_as_output()
        else:
            self.logger.debug(
                "Content check passed - file {} conforms to Dataset {}".
                format(file_path_to_check, self))
        ccl.stop(save=True, clean=True)
        return ccl

    def check_integrity(self, new_file_path, checking_user, execlog=None, runcomponent=None,
                        newly_computed_MD5=None, notify_all=True):
        """
        Checks integrity of SD against the md5 provided (newly_computed_MD5),
        or in it's absence, the MD5 computed from new_file_path.

        If this is the output of a RunComponent, it will quarantine or
        attempt to decontaminate all RunComponents using the same ExecRecord
        depending on the result of the check by default.

        OUTPUT
        Returns the ICL.
        """
        # RL February 6: I'm choosing this to be the time of the "start" of the
        # check, but it does raise the question: what exactly is the start and
        # end time of an integrity check?  Is the check just the comparison
        # of the MD5s or is it the time that you finish computing the MD5 or
        # is it the time that you start computing the MD5?
        icl = self.integrity_checks.create(execlog=execlog, runcomponent=runcomponent, user=checking_user)
        icl.start(save=True)

        if newly_computed_MD5 is None:
            with open(new_file_path, "rb") as f:
                newly_computed_MD5 = file_access_utils.compute_md5(f)

        if newly_computed_MD5 != self.MD5_checksum:
            self.logger.warn(
                "md5s do not agree for dataset id %d (old: %s; new: %s)",
                self.id,
                self.MD5_checksum,
                newly_computed_MD5)

            # June 4, 2014: this evil_twin should be a raw SD -- we don't really care what it contains,
            # just that it conflicted with the existing one.
            evil_twin = Dataset.create_dataset(file_path=new_file_path, user=checking_user, cdt=None,
                                               description="MD5 conflictor of {}".format(self),
                                               name="{}eviltwin".format(self))

            note_of_usurping = datachecking.models.MD5Conflict(integritychecklog=icl, conflicting_dataset=evil_twin)
            note_of_usurping.save()

        icl.stop(save=True, clean=True)

        # for rc in self.used_by_components.filter(_runcomponentstate_id=runcomponentstates.QUARANTINED_PK):

        if notify_all:
            if newly_computed_MD5 != self.MD5_checksum:
                any_successful_ers = ExecRecord.objects.filter(
                    execrecordouts__dataset=self,
                    used_by_components___runcomponentstate_id=runcomponentstates.SUCCESSFUL_PK
                ).exists()
                if any_successful_ers:
                    self.quarantine_runcomponents_using_as_output()
            else:
                any_quarantined_ers = ExecRecord.objects.filter(
                    execrecordouts__dataset=self,
                    used_by_components___runcomponentstate_id=runcomponentstates.QUARANTINED_PK
                ).exists()
                if any_quarantined_ers:
                    self.attempt_to_decontaminate_runcomponents_using_as_output()

        return icl

    @transaction.atomic
    def quarantine_runcomponents_using_as_output(self):
        """
        Quarantine all RunComponents that use an ExecRecord that outputs this Dataset.
        """
        for er in ExecRecord.objects.filter(execrecordouts__dataset=self):
            er.quarantine_runcomponents()

    @transaction.atomic
    def attempt_to_decontaminate_runcomponents_using_as_output(self):
        """
        Decontaminate all RunComponents that use an ExecRecord that outputs this Dataset.
        """
        for er in ExecRecord.objects.filter(execrecordouts__dataset=self):
            er.attempt_decontamination(self)

    def initially_OK(self):
        """
        Check that the Dataset was, at some point, okay.

        Such Datasets may be used as Run inputs, even if they've had some
        failed checks in the interim.
        """
        # If this is not raw, check that there is at least one content check completed and
        # successful.
        if not self.is_raw() and not self.content_checks.filter(
                baddata__isnull=True, end_time__isnull=False).exists():
            return False

        return True

    def usable_in_run(self):
        """
        Check that the Dataset is eligible to be used in a Run.

        Such a Dataset must not have failed its first content check, if there is one.
        It may have failed subsequent checks, but either
        a) it was initially good so warrants reexamination in case corruption has been fixed
        b) it never got checked initially so it should be tried again and checked now
        """
        if self.is_redacted():
            # Don't use redacted data.
            return False
        if self.is_raw():
            # If it's raw, go ahead.
            return True
        elif self.initially_OK():
            # If it was initially OK, go ahead.
            return True
        elif not self.content_checks.filter(baddata__isnull=False, end_time__isnull=False).exists():
            # At least there is no failed content check yet (maybe the execution
            # crashed during a content check), so go ahead and try again.
            return True
        return False

    def is_OK(self):
        """
        Check that this Dataset is fit for consumption.

        We check this by making sure that the Dataset has passed a content
        check (unless it's raw), and that if any more recent content or
        integrity checks have failed, the Dataset has subsequently been
        re-validated with a successful integrity check, i.e. there is an
        integrity check more recent than any failed data check.

        Redacted Datasets are not considered OK.
        """
        if self.is_redacted():
            return False

        # Was this Dataset initially OK?
        if not self.initially_OK():
            return False

        # If there are any failures, check that the most recent integrity check is good.
        if self.any_failed_checks():
            last_icl = self.integrity_checks.order_by("-end_time").first()
            if last_icl is None or last_icl.is_fail():
                return False

            last_bad_ccl = self.content_checks.filter(baddata__isnull=False).order_by("-end_time").first()
            if last_bad_ccl is not None and last_icl.start_time <= last_bad_ccl.end_time:
                return False

        return True

    def any_failed_checks(self):
        """ Checks if any integrity or content checks failed. """
        if self.integrity_checks.filter(usurper__isnull=False).exists():
            self.logger.debug("Dataset '{}' failed integrity check".format(self))
            return True

        if self.content_checks.filter(baddata__isnull=False).exists():
            self.logger.debug("Dataset '{}' failed content check".format(self))
            return True

        return False

    @transaction.atomic
    def build_redaction_plan(self, redaction_accumulator=None):
        """
        Create a list of what will be affected when redacting this Dataset.
        """
        redaction_plan = redaction_accumulator or archive.models.empty_redaction_plan()
        assert self not in redaction_plan["Datasets"]
        if self.is_redacted():
            return redaction_plan
        redaction_plan["Datasets"].add(self)

        # Make a special note if this Dataset is associated with an external file.
        if self.external_path:
            redaction_plan["ExternalFiles"].add(self)

        # Mark anything that was produced from this Dataset for redaction.
        for used_as_input in self.execrecordins.all().select_related("execrecord"):
            if used_as_input.execrecord not in redaction_plan["ExecRecords"]:
                metadata.models.update_removal_plan(
                    redaction_plan, used_as_input.execrecord.build_redaction_plan(redaction_plan)
                )

        return redaction_plan

    @transaction.atomic
    def redact_this(self):
        """
        Helper function that only redacts this Dataset and does not handle any recursion.
        """
        if self.is_redacted():
            return

        self._redacted = True
        self.MD5_checksum = ""
        self.externalfiledirectory = None
        if self.external_path:
            self.external_path = ""
        self.save(update_fields=["_redacted", "MD5_checksum", "externalfiledirectory", "external_path"])

        if bool(self.dataset_file):
            self.dataset_file.delete(save=True)
        if self.has_structure():
            self.structure.delete()

    @transaction.atomic
    def redact(self):
        redaction_plan = self.build_redaction_plan()
        archive.models.redact_helper(redaction_plan)

    def is_redacted(self):
        return self._redacted

    @property
    def uploaded(self):
        return self.file_source is None and not hasattr(self, 'usurps')

    @transaction.atomic
    def build_removal_plan(self, removal_accumulator=None):
        """
        Make a manifest of objects to remove when removing this Dataset.
        """
        removal_plan = removal_accumulator or metadata.models.empty_removal_plan()
        assert self not in removal_plan["Datasets"]
        removal_plan["Datasets"].add(self)

        # Make a special note if this Dataset is associated with an external file.
        if self.external_path:
            removal_plan["ExternalFiles"].add(self)

        for er_xput in itertools.chain(self.execrecordins.all(), self.execrecordouts.all()):
            curr_ER = er_xput.execrecord
            if curr_ER not in removal_plan["ExecRecords"]:
                metadata.models.update_removal_plan(removal_plan, curr_ER.build_removal_plan(removal_plan))

        return removal_plan

    @transaction.atomic
    def remove(self):
        removal_plan = self.build_removal_plan()
        metadata.models.remove_helper(removal_plan)

    @classmethod
    def idle_create_next_month_upload_dir(cls):
        """Create next month's dataset directory if it doesn't exist.
        NOTE: We do not need to take into account different month lengths, because
        we run this routine frequently.
        """
        numcheck = CHECK_ITERS = 10000
        while True:
            (yield None)
            numcheck += 1
            if numcheck > CHECK_ITERS:
                numcheck = 0
                date_str = (date.today() + timedelta(days=30)).strftime('%Y_%m')
                next_dirname = os.path.join(settings.MEDIA_ROOT, cls.UPLOAD_DIR, date_str)
                cls.logger.debug("idle_next_month_upload_dir: checking for '%s'", next_dirname)
                try:
                    with SafeContext():
                        if os.path.exists(next_dirname):
                            cls.logger.debug("idle_next_month_upload_dir: directory exists.")
                        else:
                            os.makedirs(next_dirname)
                except OSError:
                    cls.logger.warn("Could not make directory '%s'", next_dirname)

    @staticmethod
    def _active_datasets():
        """A generator to produce the datasets of all running and pending runs."""
        pending_runs = archive.models.Run.objects.filter(start_time=None)
        running_runs = archive.models.Run.objects.filter(end_time=None)
        active_runs = pending_runs | running_runs
        for run in active_runs:
            for component in run.get_all_atomic_runcomponents():
                execrecord = component.execrecord
                if execrecord is not None:
                    for eri in execrecord.execrecordins.all():
                        yield eri.dataset
                    for ero in execrecord.execrecordouts.all():
                        yield ero.dataset

    @classmethod
    def idle_dataset_purge(cls,
                           max_storage=settings.DATASET_MAX_STORAGE,
                           target_size=settings.DATASET_TARGET_STORAGE):
        """Purge files if the total filesize is > max_storage. Once we start purging,
        do this until the filesize <= target_size.

        This is written as a generator that will interrupt itself when
        time.time() > time_to_stop, and then continue when it next gets a time
        slice by send()
        """
        # get batches of max 5 files at a time when we have to purge
        BATCH_SIZE = 5
        while True:
            time_to_stop = (yield None)
            cls.logger.debug('hello from idle_dataset_purge')
            active_files = set(os.path.join(settings.MEDIA_ROOT, ds.dataset_file.name)
                               for ds in Dataset._active_datasets())
            up_loaded = set(os.path.join(settings.MEDIA_ROOT, ds.dataset_file.name)
                            for ds in Dataset.objects.filter(file_source=None).all())
            exclude_set = active_files | up_loaded
            cls.logger.debug('Found %d active and uploaded datasets.', len(exclude_set))
            for ff in exclude_set:
                cls.logger.debug('--%s', ff)
            # recalculate the total file size, while allowing for interruptions
            # the resulting total file size will be in cls.filepurger.total_size once
            # we have finished the file system scanning
            cls.logger.debug('rescan of datasets files')
            checker = cls.filepurger.regenerator(exclude_set)
            try:
                while True:
                    time_to_stop = (yield None)
                    checker.send(time_to_stop)
            except StopIteration:
                pass
            cls.logger.debug('finished regenerating dataset file cache')
            cls.logger.debug("\n".join(["%s: %s" % itm for itm in cls.filepurger.get_scaninfo().items()]))
            if time.time() < time_to_stop and cls.filepurger.total_size > max_storage:
                cls.logger.info("Purge cycle started, total size = %d > max_storage=%d",
                                cls.filepurger.total_size, max_storage)
                tot_size_deleted = 0
                for ftup in cls.filepurger.next_to_purge(BATCH_SIZE, exclude_set,
                                                         target_size,
                                                         dodelete=True):
                    if ftup is None:
                        # we have run out of time: abruptly exit from for loop
                        break
                    else:
                        # remove the file
                        abspath, fsize = ftup
                        relpath = os.path.relpath(abspath, settings.MEDIA_ROOT)
                        tot_size_deleted += fsize
                        dataset = Dataset.objects.filter(dataset_file=relpath).first()
                        if dataset is None:
                            # NOTE: must be defensive here: the file might have
                            # moved in the mean-time (#481)
                            try:
                                with SafeContext():
                                    filedate = os.path.getmtime(abspath)
                                    file_age = datetime.now() - datetime.fromtimestamp(filedate)
                                    if file_age > timedelta(hours=1):
                                        cls.logger.warn('No dataset matches file %r, deleting it.',
                                                        abspath)
                                        os.remove(abspath)
                            except OSError:
                                cls.logger.warn("Failed to remove file %r", abspath)
                        else:
                            dataset.dataset_file.delete(save=True)
                # -- report purging progress
                cls.logger.info("Purge cycle completed, deleted size %d", tot_size_deleted)

    @classmethod
    def purge(cls,
              max_storage=settings.DATASET_MAX_STORAGE,
              target=settings.DATASET_TARGET_STORAGE):

        files = []  # [(date, path, filesize)]
        start_path = os.path.join(settings.MEDIA_ROOT, cls.UPLOAD_DIR)
        total_size = 0
        skipped_count = 0
        for dirpath, _dirnames, filenames in os.walk(start_path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                filedate = os.path.getmtime(filepath)
                filesize = os.path.getsize(filepath)
                relpath = os.path.relpath(filepath, settings.MEDIA_ROOT)
                total_size += filesize
                heapq.heappush(files, (filedate, relpath, filesize))
        if total_size < max_storage:
            cls.logger.debug('Dataset purge not needed at %s over %d files.',
                             filesizeformat(total_size),
                             len(files))
        else:
            cls.logger.info('Dataset purge triggered at %s over %d files.',
                            filesizeformat(total_size),
                            len(files))
            active_dataset_ids = set(d.id for d in Dataset._active_datasets())
            cls.logger.debug('Found %d active datasets.',
                             len(active_dataset_ids))
            while total_size > target and files:
                filedate, relpath, filesize = heapq.heappop(files)
                dataset = Dataset.objects.filter(dataset_file=relpath).first()
                if dataset is None:
                    filepath = os.path.join(settings.MEDIA_ROOT, relpath)
                    filedate = os.path.getmtime(filepath)
                    file_age = datetime.now() - datetime.fromtimestamp(filedate)
                    if file_age < timedelta(hours=1):
                        skipped_count += 1
                    else:
                        cls.logger.warn('No dataset matches file %r, deleting it.',
                                        relpath)
                        os.remove(filepath)
                        total_size -= filesize
                else:
                    if dataset.file_source is None:
                        is_skipped = True  # it was uploaded, not created
                    else:
                        # Check to see if it's being used by an active run.
                        is_skipped = dataset.id in active_dataset_ids

                    if is_skipped:
                        skipped_count += 1
                    else:
                        dataset.dataset_file.delete(save=True)
                        total_size -= filesize

            remaining = 'Leaving {} over {} files.'.format(
                filesizeformat(total_size),
                len(files) + skipped_count)
            if total_size > max_storage:
                target = max_storage
                log_method = cls.logger.error
            elif total_size > target:
                target = target
                log_method = cls.logger.warn
            else:
                target = None
                log_method = cls.logger.info
            if target:
                message = 'Cannot purge datasets below {}. {}'.format(
                    filesizeformat(target),
                    remaining)
            else:
                message = 'Dataset purge finished. ' + remaining
            message = message.replace('\xa0', ' ')
            log_method(message)
            if log_method == cls.logger.error:
                raise RuntimeError(message)

    @classmethod
    def idle_external_file_check(cls):
        """ Perform a consistency check of external files as an idle task.

        We search for datasets that fullfil the following criteria:
        a) external files with last_time_checked < now - CHECK_INTERVAL
        b) sorted in ascending order by last_time_checked
        c) in batches of N at a time.
        """
        batch_size = 10
        missing_count = 0
        report_start = None
        while True:
            time_to_stop = (yield None)
            cut_off_time = timezone.now() - timedelta(days=settings.EXTERNAL_FILE_CHECK_DAYS,
                                                      hours=settings.EXTERNAL_FILE_CHECK_HOURS,
                                                      minutes=settings.EXTERNAL_FILE_CHECK_MINUTES)
            if report_start is not None and cut_off_time > report_start:
                cut_off_time = report_start

            # aset: only external files
            a_set = Dataset.objects.filter(externalfiledirectory__isnull=False)
            # bset: only those external files that haven't been checked for some time
            b_set = a_set.filter(last_time_checked__lt=cut_off_time)
            # prioritise least recently checked files
            c_set = b_set.order_by('last_time_checked')
            # limit number of results returned to 10. This must be the last filter to apply
            d_set = c_set[:batch_size]
            did_something = True
            while time.time() < time_to_stop and did_something:
                did_something = False
                for dataset in d_set.all():
                    did_something = True
                    path_name = dataset.external_absolute_path()
                    if path_name is None:
                        raise RuntimeError("Unexpected None for external dataset path!")
                    # --update the last_time_checked regardless of
                    # whether we issue a warning or not
                    if report_start and dataset.last_time_checked > report_start:
                        did_something = False
                        break
                    dataset.last_time_checked = timezone.now()
                    dataset.save()
                    kive_name = dataset.name
                    if not os.path.exists(path_name):
                        if missing_count == 0:
                            cls.logger.warn("Missing external file '%s' at '%s'",
                                            kive_name,
                                            path_name)
                            report_start = dataset.last_time_checked
                        missing_count += 1
            if not did_something and report_start is not None:
                # See if all external datasets have now been checked.
                unchecked_set = a_set.filter(last_time_checked__lt=report_start)
                if not unchecked_set.exists():
                    # We've checked all external files since reporting the first missing file
                    if missing_count > 1:
                        cls.logger.warn("Missing %d more external files.", missing_count-1)
                    missing_count = 0
                    report_start = None

    def increase_permissions_from_json(self, permissions_json):
        """
        Grant permission to all users and groups specified in the parameter.

        The permissions_json parameter should be a JSON string formatted as it would
        be by the permissions widget used in the UI.
        """
        self.grant_from_json(permissions_json)

        for ic in self.integrity_checks.all():
            if ic.is_fail():
                ic.usurper.conflicting_dataset.increase_permissions_from_json(permissions_json)


class DatasetStructure(models.Model):
    """
    Data with a Shipyard-compliant structure: a CSV file with a header.
    Encodes the CDT, and the transformation output generating this data.

    PRECONDITION
    Any Dataset that represents a CSV file has to have confirmed using
    summarize_csv() that the CSV file is coherent.
    """
    # Note: previously we were tracking the exact TransformationOutput
    # this came from (both for its Run and its RunStep) but this is
    # now done more cleanly using ExecRecord.
    dataset = models.OneToOneField(Dataset, related_name="structure")
    compounddatatype = models.ForeignKey("metadata.CompoundDatatype", related_name="conforming_datasets")

    # A value of -1 means the file is missing or num rows has never been counted
    num_rows = models.IntegerField("number of rows", validators=[MinValueValidator(-1)], default=-1)

    def clean(self):
        self.dataset.validate_restrict_access([self.compounddatatype])


@python_2_unicode_compatible
class ExecRecord(models.Model):
    """
    Record of a previous execution of a Pipeline component.
    """
    generator = models.OneToOneField("archive.ExecLog", related_name="execrecord")

    # FIXME exactly one of these must be non-null

    def __init__(self, *args, **kwargs):
        super(ExecRecord, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """Unicode representation of this ExecRecord."""
        inputs_list = [str(eri) for eri in self.execrecordins.all()]
        outputs_list = [str(ero) for ero in self.execrecordouts.all()]

        if type(self.general_transf()) == method.models.Method:
            return "{}({}) = ({})".format(
                    self.general_transf(),
                    ", ".join(inputs_list),
                    ", ".join(outputs_list))
        else:
            # Return a representation for a cable.
            return ("{}".format(", ".join(inputs_list)) +
                    " ={" + "{}".format(str(self.general_transf())) + "}=> " +
                    "{}".format(", ".join(outputs_list)))

    @property
    def execrecordins_in_order(self):
        return sorted(self.execrecordins.all(), key=lambda e: e.generic_input.definite.dataset_idx)

    @property
    def execrecordouts_in_order(self):
        return sorted(self.execrecordouts.all(), key=lambda e: e.generic_output.definite.dataset_idx)

    @classmethod
    @transaction.atomic
    def create(cls, generator, component, input_SDs, output_SDs):
        """Create a complete ExecRecord, including inputs and outputs.

        INPUTS
        generator       ExecLog generating this ExecRecord
        component       Pipeline component the ExecRecord is for (a
                        PipelineStep, PipelineOutputCable, or
                        PipelineStepInputCable)
        input_SDs       list of Datasets input to the component
                        during execution, in order of their index
        output_SDs      list of Datasets output by the component
                        during execution, in order of their index
        """
        execrecord = cls(generator=generator)
        # execrecord.clean()
        execrecord.save()
        for i, component_input in enumerate(component.inputs):
            execrecord.execrecordins.create(generic_input=component_input, dataset=input_SDs[i])
        for i, component_output in enumerate(component.outputs):
            execrecord.execrecordouts.create(generic_output=component_output, dataset=output_SDs[i])
        execrecord.complete_clean()
        return execrecord

    def get_execrecordout(self, xput):
        """Get the ExecRecordOut for a TransformationXput.

        INPUTS
        xput        TransformationXput to get ExecRecordOut for
        """
        try:
            return self.execrecordouts.get(generic_output=xput)
        except ExecRecordOut.DoesNotExist:
            return None

    def clean(self):
        """
        Checks coherence of the ExecRecord.

        Calls clean on all of the in/outputs.  (Multiple quenching is
        checked via a uniqueness condition and does not need to be
        coded here.)

        If this ER represents a trivial cable, then the single ERI and
        ERO should have the same Dataset.
        """
        eris = self.execrecordins.all()
        eros = self.execrecordouts.all()

        for eri in eris:
            eri.clean()
        for ero in eros:
            ero.clean()

        # Check that the permissions on the generating Run do not exceed those of the inputs.
        # (That the output permissions are the same as the generating Run will be checked
        # by the output Datasets themselves.)
        input_SDs = [x.dataset for x in self.execrecordouts.all()]
        self.generating_run.validate_restrict_access(input_SDs)

        if not isinstance(self.general_transf(), method.models.Method):
            # If the cable is quenched:
            if eris.exists() and eros.exists():

                # If the cable is trivial, then the ERI and ERO should
                # have the same Dataset (if they both exist).
                if self.general_transf().is_trivial():
                    if eris[0].dataset != eros[0].dataset:
                        raise ValidationError(('ExecRecord "{}" represents a trivial cable but its input and output '
                                               'do not match').format(self))

                # From this point on we can't proceed if the ExecRecord is redacted.
                if self.is_redacted():
                    return

                # If the cable is not trivial and both sides have
                # data, then the column *Datatypes* on the destination
                # side are the same as the corresponding column on the
                # source side.  For example, if a CDT like (DNA col1,
                # int col2) is fed through a cable that maps col1 to
                # produce (string foo), then the actual Datatype of
                # the column in the corresponding Dataset would be
                # DNA.

                # Note that because the ERI and ERO are both clean,
                # and because we checked general_transf is not
                # trivial, we know that both have well-defined
                # DatasetStructures.
                elif not self.general_transf().is_trivial():
                    cable_wires = self.general_transf().custom_wires.all()

                    source_CDT = eris[0].dataset.structure.compounddatatype
                    dest_CDT = eros[0].dataset.structure.compounddatatype

                    for wire in cable_wires:
                        source_idx = wire.source_pin.column_idx
                        dest_idx = wire.dest_pin.column_idx

                        dest_dt = dest_CDT.members.get(column_idx=dest_idx).datatype
                        source_dt = source_CDT.members.get(column_idx=source_idx).datatype

                        if source_dt != dest_dt:
                            raise ValidationError(
                                ('ExecRecord "{}" represents a cable, but the Datatype '
                                 'of its destination column, "{}", does not match the Datatype '
                                 'of its source column, "{}"').format(self, dest_dt, source_dt)
                            )

    def complete_clean(self):
        """
        Checks completeness of the ExecRecord.

        Calls clean, and then checks that all in/outputs of the
        Method/POC/PSIC are quenched.
        """
        self.clean()

        # Because we know that each ERI is clean (and therefore each
        # one maps to a valid input of our Method/POC/PSIC), and
        # because there is no multiple quenching (due to a uniqueness
        # constraint), all we have to do is check the number of ERIs
        # to make sure everything is quenched.
        if type(self.general_transf()) in (
                pipeline.models.PipelineOutputCable,
                pipeline.models.PipelineStepInputCable
                ):
            # In this case we check that there is an input and an output.
            if not self.execrecordins.all().exists():
                raise ValidationError(
                    "Input to ExecRecord \"{}\" is not quenched".format(self))
            if not self.execrecordouts.all().exists():
                raise ValidationError(
                    "Output of ExecRecord \"{}\" is not quenched".format(self))

        else:
            if self.execrecordins.count() != self.general_transf().inputs.count():
                raise ValidationError(
                    "Input(s) to ExecRecord \"{}\" are not quenched".format(self))

            # Similar for EROs.
            if self.execrecordouts.count() != self.general_transf().outputs.count():
                raise ValidationError(
                    "Output(s) of ExecRecord \"{}\" are not quenched".format(self))

    def general_transf(self):
        """Returns the Method/POC/PSIC represented by this ExecRecord."""
        if self.generator.record.is_cable():
            return self.generator.record.component
        else:
            # This is a Method.
            return self.generator.record.component.transformation.definite

    @property
    def generating_run(self):
        return self.generator.record.top_level_run

    def provides_outputs(self, outputs):
        """
        Checks whether this ER has existent data for these outputs.
        outputs: an iterable of TOs we want the ER to have real data for.

        PRE
        1) outputs must be TransformationOutputs of the Transformation associated
        with the RunStep/RunSIC/RunOutputCable associated with this ExecRecord
        (they cannot be arbitrary TransformationOutputs).
        """
        # Load each TO in outputs
        for curr_output in outputs:
            corresp_ero = self.execrecordouts.get(generic_output=curr_output)

            if not corresp_ero.has_data():
                self.logger.debug(
                    "corresponding ERO doesn't have data - ER doesn't have existent data for all TOs requested")
                return False

        self.logger.debug("all outputs needed have corresponding EROs with data")
        return True

    def outputs_OK(self):
        """Checks whether all of the EROs of this ER are OK."""
        return all([ero.is_OK() for ero in self.execrecordouts.all()])

    def outputs_not_usable_in_run(self):
        """
        Checks whether any of the EROs of this ER have ever failed any checks.
        """
        return any([not ero.dataset.usable_in_run() for ero in self.execrecordouts.all()])

    def has_ever_failed(self):
        """Has any execution of this ExecRecord ever failed?"""
        # Go through all RunSteps using this ExecRecord.
        run_components = self.used_by_components.exclude(
            reused=True).filter(runstep__isnull=False)
        for component_using_this in run_components:
            if not component_using_this.runstep.is_successful():
                return True
        return False

    def is_redacted(self):
        ins = self.execrecordins.all()
        outs = self.execrecordouts.all()
        if not hasattr(self, '_prefetched_objects_cache'):
            # Not already prefetched, use select_related.
            ins = ins.select_related('dataset')
            outs = outs.select_related('dataset')

        for eri in ins:
            if eri.dataset.is_redacted():
                return True
        for ero in outs:
            if ero.dataset.is_redacted():
                return True

        return self.generator.is_redacted()

    @transaction.atomic
    def build_redaction_plan(self, redaction_accumulator=None):
        redaction_plan = redaction_accumulator or archive.models.empty_redaction_plan()
        assert self not in redaction_plan["ExecRecords"]
        if self.is_redacted():
            return redaction_plan
        redaction_plan["ExecRecords"].add(self)

        metadata.models.update_removal_plan(redaction_plan, self.generator.build_redaction_plan())

        for ero in self.execrecordouts.exclude(dataset___redacted=True).select_related("dataset"):
            # If any of these are already redacted, this call will simply do nothing.
            if ero.dataset not in redaction_plan["Datasets"]:
                metadata.models.update_removal_plan(
                    redaction_plan, ero.dataset.build_redaction_plan(redaction_plan)
                )

        return redaction_plan

    @transaction.atomic
    def redact_this(self):
        # Redact components that used this ExecRecord, and purge any sandboxes that still exist.
        runs_to_purge = set()
        for rc in self.used_by_components.all():
            rc.redact()
            if rc.top_level_run not in runs_to_purge:
                runs_to_purge.add(rc.top_level_run)

        for run in runs_to_purge:
            try:
                if not run.purged:
                    run.collect_garbage()
            except ObjectDoesNotExist:
                # No RunToProcess exists.
                pass
            except archive.exceptions.SandboxActiveException as e:
                # The Run never started or hasn't finished.
                self.logger.warning(e)
            except OSError as e:
                # The sandbox could not be removed.
                self.logger.warning(e)

    @transaction.atomic
    def redact(self):
        """
        "Hollow out" this ExecRecord.

        This may be triggered by an input Dataset or by the ExecLog.
        """
        redaction_plan = self.build_redaction_plan()
        archive.models.redact_helper(redaction_plan)

    @transaction.atomic
    def build_removal_plan(self, removal_accumulator=None):
        """
        Creates a manifest of objects that will be removed if this ExecRecord is removed.
        """
        removal_plan = removal_accumulator or metadata.models.empty_removal_plan()
        assert self not in removal_plan["ExecRecords"]
        removal_plan["ExecRecords"].add(self)

        if not (self.generator.record.is_cable() and self.general_transf().is_trivial()):
            for ero in self.execrecordouts.exclude(
                    dataset__in=removal_plan["Datasets"]).select_related("dataset"):
                if ero.dataset not in removal_plan["Datasets"]:
                    metadata.models.update_removal_plan(
                        removal_plan, ero.dataset.build_removal_plan(removal_plan)
                    )

        for rc in self.used_by_components.all():
            if rc.top_level_run not in removal_plan["Runs"]:
                metadata.models.update_removal_plan(removal_plan, rc.top_level_run.build_removal_plan(removal_plan))

        return removal_plan

    @transaction.atomic
    def remove(self):
        removal_plan = self.build_removal_plan()
        metadata.models.remove_helper(removal_plan)

    @transaction.atomic
    def quarantine_runcomponents(self):
        """
        Quarantine RunComponents that used this ExecRecord.
        """
        for rc in self.used_by_components.filter(_runcomponentstate_id=runcomponentstates.SUCCESSFUL_PK):
            rc.quarantine(save=True, recurse_upward=True)

    @transaction.atomic
    def decontaminate_runcomponents(self):
        """
        Decontaminate RunComponents that used this ExecRecord.
        """
        for rc in self.used_by_components.filter(_runcomponentstate_id=runcomponentstates.QUARANTINED_PK):
            rc.decontaminate(save=True, recurse_upward=True)

    @transaction.atomic
    def attempt_decontamination(self, decontaminated_dataset):
        """
        Attempt to decontaminate all RunComponents that used this ExecRecord.

        It goes ahead and does so if the specified Dataset was the last
        contaminated one, and if the last complete RunComponent to use it did
        not have a failing ExecLog.
        """
        for ero in self.execrecordouts.exclude(dataset=decontaminated_dataset):
            if not ero.is_OK():
                return

        last_rc = self.used_by_components.filter(
            log__isnull=False,
            _runcomponentstate_id__in=runcomponentstates.COMPLETE_STATE_PKS
        ).order_by("-end_time").first()
        if not last_rc.log.is_successful():
            return

        # Having reached here, we now know that the ExecRecord can be decontaminated.
        self.decontaminate_runcomponents()


@python_2_unicode_compatible
class ExecRecordIn(models.Model):
    """
    Denotes an input fed to the Method/POC/PSIC in the parent ExecRecord.

    The input may map to deleted data, e.g. if it was a deleted output
    of a previous step in a pipeline.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord", related_name="execrecordins")
    dataset = models.ForeignKey(Dataset, help_text="Dataset fed to this input",
                                related_name="execrecordins")

    # For a Method/Pipeline, this denotes the input that this ERI refers to;
    # for a cable, this denotes the thing that "feeds" it.
    generic_input = models.ForeignKey(transformation.models.TransformationXput)

    class Meta:
        unique_together = ("execrecord", "generic_input")

    def __init__(self, *args, **kwargs):
        super(ExecRecordIn, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """
        Unicode representation.

        If this ERI represents the source of a POC/PSIC, then it looks like
        [dataset]
        If it represents a TI, then it looks like
        [dataset]=>[transformation (raw) input name]

        Examples:
        S552
        S552=>foo_bar

        PRE: the parent ER must exist and be clean.
        """
        if (type(self.execrecord.general_transf()) in
                (pipeline.models.PipelineOutputCable,
                 pipeline.models.PipelineStepInputCable)):
            return str(self.dataset)
        else:
            dest_name = self.generic_input.definite.dataset_name
            return "{}=>{}".format(self.dataset, dest_name)

    def clean(self):
        """
        Checks coherence of this ExecRecordIn.

        Checks that generic_input is appropriate for the parent
        ExecRecord's Method/POC/PSIC.
        - If execrecord is for a POC, then generic_input should be the TO that
          feeds it (i.e. the PipelineStep TO that is cabled to a Pipeline output).
        - If execrecord is for a PSIC, then generic_input should be the TO or TI
          that feeds it (TO if it's from a previous step; TI if it's from a Pipeline
          input).
        - If execrecord is for a Method, then generic_input is the TI
          that this ERI represents.

        Also, if dataset refers to existent data, check that it
        is compatible with the input represented.
        """
        # Check that the input is accessible by the generating run.
        self.execrecord.generating_run.validate_restrict_access([self.dataset])

        parent_transf = self.execrecord.general_transf()

        # If ER links to POC, ERI must link to TO which the outcable runs from.
        if type(parent_transf) == pipeline.models.PipelineOutputCable:
            if self.generic_input.definite != parent_transf.source.definite:
                raise ValidationError(
                    'ExecRecordIn "{}" does not denote the TO that feeds the parent ExecRecord POC'.
                    format(self))
        # Similarly for a PSIC.
        elif type(parent_transf) == pipeline.models.PipelineStepInputCable:
            if self.generic_input.definite != parent_transf.source.definite:
                raise ValidationError(
                    'ExecRecordIn "{}" does not denote the TO/TI that feeds the parent ExecRecord PSIC'.
                    format(self))

        else:
            # The ER represents a Method (not a cable).  Therefore the
            # ERI must refer to a TI of the parent ER's Method.
            if type(self.generic_input) == transformation.models.TransformationOutput:
                raise ValidationError(
                    'ExecRecordIn "{}" must refer to a TI of the Method of the parent ExecRecord'.
                    format(self))

            transf_inputs = parent_transf.inputs
            if not transf_inputs.filter(pk=self.generic_input.pk).exists():
                raise ValidationError(
                    'Input "{}" does not belong to Method of ExecRecord "{}"'.
                    format(self.generic_input, self.execrecord))

        # If the SD is redacted, we return -- the rest is not applicable.
        if self.dataset.is_redacted():
            return

        # The ERI's Dataset raw/unraw state must match the
        # raw/unraw state of the generic_input that feeds it (if ER is a cable)
        # or that it is fed into (if ER is a Method).
        # self.dataset = librarian.models.Dataset.objects.get(pk=self.dataset.pk)
        # self.generic_input = transformation.models.TransformationXput.objects.get(pk=self.generic_input.pk)
        if self.generic_input.is_raw() != self.dataset.is_raw():
            sd_raw_str = "raw" if self.dataset.is_raw() else "non-raw"
            gi_raw_str = "raw" if self.generic_input.is_raw() else "non-raw"
            raise ValidationError(
                'Dataset "{}" ({}) cannot feed source "{}" ({})'.
                format(self.dataset, sd_raw_str, self.generic_input, gi_raw_str))

        if not self.dataset.is_raw():
            transf_xput_used = self.generic_input
            cdt_needed = self.generic_input.get_cdt()
            input_SD = self.dataset

            # CDT of input_SD must be a restriction of cdt_needed,
            # i.e. we can feed it into cdt_needed.
            if not input_SD.structure.compounddatatype.is_restriction(
                    cdt_needed):
                raise ValidationError(
                    'CDT of Dataset "{}" is not a restriction of the required CDT'.
                    format(input_SD))

            # Check row constraints.
            if (transf_xput_used.get_min_row() is not None and
                    input_SD.num_rows() < transf_xput_used.get_min_row()):
                error_str = ""
                if type(self.generic_input) == transformation.models.TransformationOutput:
                    error_str = 'Dataset "{}" has too few rows to have come from TransformationOutput "{}"'
                else:
                    error_str = 'Dataset "{}" has too few rows for TransformationInput "{}"'
                raise ValidationError(error_str.format(input_SD, transf_xput_used))

            if (transf_xput_used.get_max_row() is not None and
                    input_SD.num_rows() > transf_xput_used.get_max_row()):
                error_str = ""
                if type(self.generic_input) == transformation.models.TransformationOutput:
                    error_str = 'Dataset "{}" has too many rows to have come from TransformationOutput "{}"'
                else:
                    error_str = 'Dataset "{}" has too many rows for TransformationInput "{}"'
                raise ValidationError(error_str.format(input_SD, transf_xput_used))

    def is_OK(self):
        """Checks if the associated Dataset is OK."""
        return self.dataset.is_OK()


@python_2_unicode_compatible
class ExecRecordOut(models.Model):
    """
    Denotes an output from the Method/PSIC/POC in the parent ExecRecord.

    The output may map to deleted data, i.e. if it was deleted after
    being generated.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord",
                                   related_name="execrecordouts")

    dataset = models.ForeignKey(
        Dataset,
        help_text="Dataset coming from this output",
        related_name="execrecordouts"
    )

    # For a Method/Pipeline this represents the TO that produces this output.
    # For a cable, this represents the TO (for a POC) or TI (for a PSIC) that
    # this cable feeds into.
    generic_output = models.ForeignKey(transformation.models.TransformationXput,
                                       related_name="execrecordouts_referencing")

    class Meta:
        unique_together = ("execrecord", "generic_output")

    def __init__(self, *args, **kwargs):
        super(ExecRecordOut, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """
        Unicode representation of this ExecRecordOut.

        If this ERO represented the output of a PipelineOutputCable, then this looks like
        [dataset]
        If it represents the input that a PSIC feeds into, then it looks like
        [dataset]
        Otherwise, it represents a TransformationOutput, and this looks like
        [TO name]=>[dataset]
        e.g.
        S458
        output_one=>S458
        """
        if (type(self.execrecord.general_transf()) in
                (pipeline.models.PipelineOutputCable,
                 pipeline.models.PipelineStepInputCable)):
            return str(self.dataset)
        else:
            return "{}=>{}".format(self.generic_output.definite.dataset_name, self.dataset)

    def clean(self):
        """
        - If the ExecRecord represents a PipelineOutputCable, check
          that the output is the one defined by the PipelineOutputCable.
        - If the ExecRecord represents a PipelineStepInputCable, check
          that the output is the TransformationInput that the cable feeds.
        - If the ExecRecord is not for a cable, check that the output
          belongs to the ExecRecord's Method.
        - The Dataset is compatible with generic_output. (??)
        """
        # Runs must not increase permissions on an input.
        self.execrecord.generating_run.validate_restrict_access(
            [self.dataset])

        # If the parent ER is linked with POC, the corresponding ERO TO must be coherent
        if isinstance(self.execrecord.general_transf(), pipeline.models.PipelineOutputCable):
            parent_er_outcable = self.execrecord.general_transf()

            # ERO TO must belong to the same pipeline as the ER POC
            if self.generic_output.definite.transformation.definite != parent_er_outcable.pipeline:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not belong to the same pipeline as its parent ExecRecord POC".
                    format(self))

            # And the POC defined output name must match the pipeline TO name
            if parent_er_outcable.output_name != self.generic_output.definite.dataset_name:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not represent the same output as its parent ExecRecord POC".
                    format(self))

        # Second case: parent ER represents a PSIC.
        elif isinstance(self.execrecord.general_transf(), pipeline.models.PipelineStepInputCable):
            # This ERO must point to a TI.
            if not self.generic_output.is_input:
                raise ValidationError(
                    "Parent of ExecRecordOut \"{}\" represents a PSIC; ERO must be a TransformationInput".
                    format(self))

        # Else the parent ER is linked with a method
        else:
            query_for_outs = self.execrecord.general_transf().outputs

            # The ERO output TO must be a member of the ER's method/pipeline
            if not query_for_outs.filter(pk=self.generic_output.pk).exists():
                raise ValidationError(
                    "Output \"{}\" does not belong to Method/Pipeline of ExecRecord \"{}\"".
                    format(self.generic_output, self.execrecord))

        if self.dataset.is_redacted():
            # A redacted SD is fine -- the rest of the checks are inapplicable.
            return

        # Check that the SD is compatible with generic_output.

        self.logger.debug("ERO SD '{}' is raw? {}".format(
            self.dataset,
            self.dataset.is_raw()))
        self.logger.debug("ERO generic_output '{}' {} is raw? {}".format(
            self.generic_output,
            type(self.generic_output),
            self.generic_output.is_raw()))

        # If SD is raw, the ERO output TO must also be raw
        # Refresh dataset and generic_output to make sure we get the right information.
        self.dataset = librarian.models.Dataset.objects.get(pk=self.dataset.pk)
        self.generic_output = transformation.models.TransformationXput.objects.get(pk=self.generic_output.pk)
        if self.dataset.is_raw() != self.generic_output.is_raw():
            sd_raw_str = "raw" if self.dataset.is_raw() else "non-raw"
            go_raw_str = "raw" if self.generic_output.is_raw() else "non-raw"
            if type(self.generic_output) == pipeline.models.PipelineStepInputCable:
                raise ValidationError(
                    'Dataset "{}" ({}) cannot feed input "{}" ({})'.
                    format(self.dataset, sd_raw_str, self.generic_output, go_raw_str))
            else:
                raise ValidationError(
                    'Dataset "{}" ({}) cannot have come from output "{}" ({})'.
                    format(self.dataset, sd_raw_str, self.generic_output, go_raw_str))

        # SD must satisfy the CDT / row constraints of the producing TO (Methods/Pipelines/POCs)
        # or of the TI fed (PSIC case)
        if not self.dataset.is_raw():
            input_SD = self.dataset

            # If this execrecord refers to a Method, the SD CDT
            # must *exactly* be generic_output's CDT since it was
            # generated by this Method.

            if isinstance(self.execrecord.general_transf(), method.models.Method):
                if input_SD.structure.compounddatatype != self.generic_output.get_cdt():
                    raise ValidationError(
                        ('CDT of Dataset "{}" is not the CDT of the '
                         'TransformationOutput "{}" of the generating Method').
                        format(input_SD, self.generic_output))

            # For POCs, ERO SD's CDT must be >>identical<< to generic_output's CDT, because it was generated either
            # by this POC or by a compatible one.
            # FIXME: self.generic_output.get_cdt().is_restriction(self.dataset.structure.compounddatatype)

            elif isinstance(self.execrecord.general_transf(), pipeline.models.PipelineOutputCable):
                if not self.dataset.structure.compounddatatype.is_identical(self.generic_output.get_cdt()):
                    raise ValidationError(
                        "CDT of Dataset \"{}\" is not identical to the "
                        "CDT of the TransformationOutput \"{}\" of the "
                        "generating Pipeline".format(input_SD,
                                                     self.generic_output))

            # If it refers to a PSIC, then SD CDT must be a
            # restriction of generic_output's CDT.
            else:
                if not input_SD.structure.compounddatatype.is_restriction(self.generic_output.get_cdt()):
                    raise ValidationError(
                        'CDT of Dataset "{}" is not a restriction of '
                        'the CDT of the fed TransformationInput "{}"'.format(
                            input_SD,
                            self.generic_output))

            # If the input SD has a number of rows, then check that it is coherent.  (If it is -1,
            # then we can't check this.)
            if input_SD.num_rows() != -1:
                if (self.generic_output.get_min_row() is not None and
                        input_SD.num_rows() < self.generic_output.get_min_row()):
                    if isinstance(self.execrecord.general_transf(), pipeline.models.PipelineStepInputCable):
                        raise ValidationError(
                            "Dataset \"{}\" feeds TransformationInput \"{}\" but has too few rows".
                            format(input_SD, self.generic_output))
                    else:
                        raise ValidationError(
                            "Dataset \"{}\" was produced by TransformationOutput \"{}\" but has too few rows".
                            format(input_SD, self.generic_output))

                if (self.generic_output.get_max_row() is not None and
                        input_SD.num_rows() > self.generic_output.get_max_row()):
                    if isinstance(self.execrecord.general_transf(), pipeline.models.PipelineStepInputCable):
                        raise ValidationError(
                            "Dataset \"{}\" feeds TransformationInput \"{}\" but has too many rows".
                            format(input_SD, self.generic_output))
                    else:
                        raise ValidationError(
                            "Dataset \"{}\" was produced by TransformationOutput \"{}\" but has too many rows".
                            format(input_SD, self.generic_output))

        self.logger.debug("ERO is clean")

    def has_data(self):
        """True if associated Dataset has data; False otherwise."""
        return self.dataset.has_data()

    def is_OK(self):
        """Checks if the associated Dataset is OK."""
        return self.dataset.is_OK()


# Register signals.
post_delete.connect(librarian.signals.dataset_post_delete, sender=Dataset)
