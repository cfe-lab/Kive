"""
librarian.models

Shipyard data models pertaining to the lookup of the past: ExecRecord,
Dataset, etc.
"""
from __future__ import unicode_literals

from django.db import models, transaction
from django.core.exceptions import ValidationError, ObjectDoesNotExist
from django.core.validators import MinValueValidator, RegexValidator
from django.core.files import File
from django.utils.encoding import python_2_unicode_compatible
from django.utils import timezone
from django.conf import settings
from django.template.defaultfilters import filesizeformat
from django.db.models.signals import post_delete

from django.core.urlresolvers import reverse

import sys
import csv
import re
import logging
import tempfile
import hashlib
import shutil
import os
import itertools
import time
import heapq
from datetime import datetime, timedelta

import method.models
import pipeline.models
import transformation.models
import datachecking.models
import metadata.models
import archive.exceptions
import librarian.signals
import file_access_utils
from constants import maxlengths, runcomponentstates
from datachecking.models import BadData

LOGGER = logging.getLogger(__name__)


def get_upload_path(instance, filename):
    """
    Helper method for uploading dataset_files for Dataset.
    This is outside of the Dataset class, since @staticmethod and other method decorators were used instead of the
    method pointer when this method was inside Dataset class.

    :param instance:  Dataset instance
    :param filename: Dataset.dataset_file.name
    :return:  The upload directory for Dataset files.
    """
    return instance.UPLOAD_DIR + os.sep + time.strftime('%Y_%m') + os.sep + filename


@python_2_unicode_compatible
class ExternalFileDirectory(models.Model):
    """
    A database table storing directories whose contents we can make Datasets out of.
    """
    name = models.CharField(
        help_text="Human-readable name for this external file directory",
        unique=True,
        max_length=maxlengths.MAX_EXTERNAL_PATH_LENGTH
    )
    path = models.CharField(
        help_text="Absolute path",
        max_length=maxlengths.MAX_EXTERNAL_PATH_LENGTH
    )

    def __str__(self):
        return self.name

    def list_files(self):
        """
        Return a list of tuples representing files under this directory.

        The tuple looks like:
        ([absolute file path], [file path with external file directory name substituted])
        """
        path_with_slash = self.path if self.path.endswith("/") else "{}/".format(self.path)
        all_files = []
        for root, _dirs, files in sorted(os.walk(self.path)):
            for f in files:
                f = os.path.join(root, f)
                all_files.append((f, f.replace(path_with_slash, "[{}]/".format(self.name), 1)))
        return all_files

    def save(self, *args, **kwargs):
        """
        Normalize the path before saving.
        """
        self.path = os.path.normpath(self.path)
        super(ExternalFileDirectory, self).save(*args, **kwargs)


@python_2_unicode_compatible
class Dataset(metadata.models.AccessControl):
    """
    A (possibly temporary) data file.

    That is to say, at some point, there was a data file uploaded to/
    generated by Shipyard, which was coherent with its
    specified/generating CDT and its producing
    TransformationOutput/cable (if it was generated), and this
    represents it, whether or not it was saved to the database.

    PRE: the actual file that the Dataset represents (whether
    it still exists or not) is/was coherent (e.g. checked using
    CDT.summarize_CSV()).
    """
    UPLOAD_DIR = "Datasets"  # This is relative to kive.settings.MEDIA_ROOT

    name = models.CharField(max_length=maxlengths.MAX_FILENAME_LENGTH,
                            help_text="Name of this Dataset.",
                            blank=True)
    description = models.TextField(help_text="Description of this Dataset.",
                                   max_length=maxlengths.MAX_DESCRIPTION_LENGTH,
                                   blank=True)
    date_created = models.DateTimeField(default=timezone.now, help_text="Date of Dataset creation.")

    # Four cases from which Datasets can originate:
    #
    # Case 1: uploaded
    # Case 2: from the transformation of a RunStep
    # Case 3: from the execution of a POC (i.e. from a ROC)
    # Case 4: from the execution of a PSIC (i.e. from a RunSIC)
    file_source = models.ForeignKey("archive.RunComponent", related_name="outputs", null=True, blank=True)

    # Datasets are stored in the "Datasets" folder
    dataset_file = models.FileField(upload_to=get_upload_path,
                                    help_text="Physical path where datasets are stored",
                                    blank=True,
                                    default='',
                                    db_index=True,
                                    max_length=maxlengths.MAX_FILENAME_LENGTH)

    externalfiledirectory = models.ForeignKey(
        ExternalFileDirectory,
        verbose_name="External file directory",
        help_text="External file directory containing the data file",
        null=True,
        blank=True
    )
    external_path = models.CharField(
        help_text="Relative path of the file within the specified external file directory",
        blank=True,
        max_length=maxlengths.MAX_EXTERNAL_PATH_LENGTH
    )

    logger = logging.getLogger('librarian.Dataset')

    # For validation of Datasets when being reused, or when being
    # regenerated.  A blank MD5_checksum means that the file was
    # missing (not created when it was supposed to be created).
    MD5_checksum = models.CharField(
        max_length=64,
        validators=[RegexValidator(
            regex=re.compile("(^[0-9A-Fa-f]{32}$)|(^$)"),
            message="MD5 checksum is not either 32 hex characters or blank")],
        blank=True,
        default="",
        help_text="Validates file integrity")

    _redacted = models.BooleanField(default=False)

    class Meta:
        ordering = ["-date_created", "name"]

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """
        Unicode representation of a Dataset.

        This is simply S[pk] if it has no data.
        """
        if not self.has_data():
            return "S{}".format(self.pk)

        display_name = self.name if self.name != "" else "[no name specified]"

        return "{} (created by {} on {})".format(display_name, self.user, self.date_created)

    def external_absolute_path(self):
        if not self.external_path:
            return None
        return os.path.normpath(os.path.join(self.externalfiledirectory.path, self.external_path))

    def get_open_file_handle(self, mode="rb"):
        """
        Retrieves an open Django file with which to access the data.

        This is self.dataset_file if possible, falls back to the external file if possible,
        and otherwise returns None.
        """
        if self.dataset_file:
            self.dataset_file.open(mode)
            return self.dataset_file
        elif self.external_path:
            abs_path = self.external_absolute_path()
            if os.path.exists(abs_path) and os.access(abs_path, os.R_OK):
                return File(open(abs_path, mode))
        return None

    def all_rows(self, data_check=False, insert_at=None, limit=None):
        """
        Returns an iterator over all rows of this Dataset.

        If insert_at is specified, a blank field is inserted
        at each element of insert_at.
        """
        data_handle = self.get_open_file_handle()
        if data_handle is None:
            raise RuntimeError('Dataset file has been removed.')

        with data_handle:
            reader = csv.reader(data_handle)
            if data_check:
                try:
                    baddata = self.content_checks.first().baddata
                    cell_errors = baddata.cell_errors.order_by('row_num', 'column')
                    if limit is not None:
                        cell_errors.filter(row_num__lte=limit)
                    failed_columns = {
                        (error.row_num, error.column.column_idx): error.column
                        for error in cell_errors
                    }
                except BadData.DoesNotExist:
                    failed_columns = {}
            for row_num, row in enumerate(reader):
                if insert_at is not None:
                    [row.insert(pos, "") for pos in insert_at]
                if data_check:
                    new_row = []
                    for column_num, value in enumerate(row, 1):
                        failed_column = failed_columns.get((row_num, column_num),
                                                           None)
                        if failed_column is None:
                            new_errors = []
                        else:
                            new_errors = failed_column.check_basic_constraints(value)
                        new_row.append((value, new_errors))

                    row = new_row
                yield row

    def header(self):
        rows = self.all_rows()
        return next(rows)

    def rows(self, data_check=False, insert_at=None, limit=None):
        rows = self.all_rows(data_check, insert_at, limit=limit)
        for i, row in enumerate(rows):
            if i == 0:
                pass  # skip header
            else:
                yield row
            if limit is not None and i >= limit:
                break

    def expected_header(self):
        header = []
        if not self.is_raw():
            header = [c.column_name for c in self.compounddatatype.members.order_by("column_idx")]
        return header

    @property
    def content_matches_header(self):
        observed = self.header()

        # Cache this so we only hit the db once here
        if hasattr(self, "_expected_header_cache"):
            expected = self._expected_header_cache
        else:
            expected = self._expected_header_cache = self.expected_header()

        if len(observed) != len(expected):
            return False
        return not any([o != x for (o, x) in zip(observed, expected)])

    def column_alignment(self):
        """
        This function looks at the expected and observed headers for
        a Dataset, and tries to align them if they don't match.

        :return: a tuple whose first element is a list of tuples
        i.e (expected header name, observed header name), and
        whose second element is a list of gaps indicating where
        to insert blank fields in a row
        """
        expt = self.expected_header()
        obs = self.header()
        i, insert = 0, []

        if self.is_raw() and not self.content_matches_header:
            return None, None

        # Do a greedy 'hard matching' over the columns
        while i < max(len(expt), len(obs)) - 1:
            ex, ob = zip(*(map(None, expt, obs)[i:]))
            u_score = float('inf')
            l_score = float('inf')

            for j, val in enumerate(ob):
                if val == ex[0]:
                    u_score = j
            for j, val in enumerate(ex):
                if val == ob[0]:
                    l_score = j
            if l_score == u_score == float('inf'):
                pass
            elif u_score < l_score and u_score != float('inf'):
                [expt.insert(i, "") for _ in xrange(u_score)]
            elif l_score <= u_score and l_score != float('inf'):
                [obs.insert(i, "") for _ in xrange(l_score)]
                insert += [i] * l_score  # keep track of where to insert columns in the resulting view
            i += 1

        # it would be nice to do a similar soft matching to try to
        # match columns that are close to being the same string

        # Pad out the arrays
        diff = abs(len(expt)-len(obs))
        if len(expt) > len(obs):
            obs += [""] * diff
        else:
            expt += [""] * diff

        return zip(expt, obs), insert

    @property
    def compounddatatype(self):
        if self.is_raw():
            return None
        return self.structure.compounddatatype

    def clean(self):
        """
        Checks coherence of this Dataset.

        If it has data (i.e. an associated Dataset), it cleans that
        Dataset.  Then, if there is an associated DatasetStructure,
        clean that.

        Note that the MD5 checksum is already checked via a validator.
        """
        if self.has_structure():
            self.structure.clean()

        if self.file_source is not None:
            # Whatever run created this Dataset must have had access to the parent Dataset.
            self.file_source.definite.top_level_run.validate_restrict_access([self])

        if not (self.externalfiledirectory and self.external_path or
                not self.externalfiledirectory and not self.external_path):
            raise ValidationError(
                {
                    "external_path": "Both externalfiledirectory and external_path should be set or "
                                     "neither should be set"
                }
            )

        if self.has_data() and not self.check_md5():
            error_str = ('File integrity of "{}" lost. Current checksum "{}" does not equal expected checksum ' +
                         '"{}"').format(self, self.compute_md5(), self.MD5_checksum)
            raise ValidationError(
                {
                    "dataset_file": error_str
                }
            )

    def validate_uniqueness_on_upload(self, *args, **kwargs):
        """
        Validates that the name and MD5 of the Dataset are unique.

        This isn't at the model level because we do want to allow
        these duplicates (e.g. empty files generated by the same
        Pipeline), but we want to check files on upload.
        """
        query = Dataset.objects.filter(MD5_checksum=self.MD5_checksum,
                                       name=self.name)

        if query.exclude(pk=self.pk).exists():
            error_str = "A Dataset with that name and MD5 already exists."
            raise ValidationError(
                {
                    "dataset_file": error_str
                }
            )

    @property
    def absolute_url(self):
        """
        :return str: URL to access the dataset_file
        """
        return reverse('dataset_download', kwargs={"dataset_id": self.id})

    def get_filesize(self):
        """
        :return int: size of dataset_file in bytes
        """
        try:
            data_handle = self.get_open_file_handle()
            if data_handle is None:
                return None
            return data_handle.size
        finally:
            if data_handle is not None:
                data_handle.close()

    def get_formatted_filesize(self):
        unformatted_size = self.get_filesize()
        if unformatted_size is None:
            return None
        return filesizeformat(unformatted_size)

    def compute_md5(self):
        """Computes the MD5 checksum of the Dataset."""
        with self.get_open_file_handle() as data_handle:
            return file_access_utils.compute_md5(data_handle.file)

    def check_md5(self):
        """
        Checks the MD5 checksum of the Dataset against its stored value.

        The stored value is used when regenerating data
        that once existed, as a coherence check.
        """
        # Recompute the MD5, see if it equals what is already stored
        return self.MD5_checksum == self.compute_md5()

    def has_data(self):
        """
        True if an actual dataset file exists; False otherwise.

        This returns True if there is either a file registered in the database or if
        there is a working pointer to an external file.
        """
        # Note: "self.dataset_file is not None" won't work here because self.dataset_file
        # is a FieldFile with no file, not None.
        if bool(self.dataset_file):
            return True

        abs_path = self.external_absolute_path()
        if abs_path is not None:
            if os.path.exists(abs_path) and os.access(abs_path, os.R_OK):
                return True

        return False

    def has_structure(self):
        """True if associated DatasetStructure exists; False otherwise."""
        try:
            self.structure
        except ObjectDoesNotExist:
            return False
        return self.structure.pk is not None

    def is_raw(self):
        """True if this Dataset is raw, i.e. not a CSV file."""
        # return not hasattr(self, "structure")
        try:
            self.structure
        except ObjectDoesNotExist:
            return True
        return self.structure is None

    def num_rows(self):
        """Returns number of rows in the associated Dataset.

        This returns None if the Dataset is raw.
        """
        return None if self.is_raw() else self.structure.num_rows

    def get_cdt(self):
        """
        Retrieve the CDT of this Dataset (none if it is raw).
        """
        return None if self.is_raw() else self.structure.compounddatatype

    def create_structure(self, compounddatatype, num_rows=-1):
        """Add a DatasetStructure to this Dataset."""
        if not self.is_raw():
            raise ValueError('CompoundDatatype "{}" already has a structure.')
        structure = DatasetStructure(dataset=self, compounddatatype=compounddatatype, num_rows=num_rows)
        structure.clean()
        structure.save()
        return structure

    def set_MD5(self, file_path, file_handle=None):
        """Set the MD5 hash from a file.
        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.
        :param str file_path:  Path to file to calculate MD5 for. file_path not used if file_handle supplied.
        :param file file_handle: file handle of file to calculate MD5.
                Moves file handle to beginning of file before calculating MD5.
                If file_handle empty, then uses file_path.
        """
        with file_access_utils.FileReadHandler(file_path=file_path, file_handle=file_handle, access_mode="rb") as f:
            self.MD5_checksum = file_access_utils.compute_md5(f)

    def set_MD5_and_count_rows(self, file_path, file_handle=None):
        """Set the MD5 hash and number of rows from a file.
        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.
        PRE
        This Dataset must have a DatasetStructure
        :param str file_path:  Path to file to calculate MD5 for. file_path not used if file_handle supplied.
        :param file file_handle: file handle of file to calculate MD5.
                Moves file handle to beginning of file before calculating MD5.
                If file_handle empty, then uses file_path.
        """
        assert not self.is_raw()

        num_rows = -1  # skip header
        md5gen = hashlib.md5()
        with file_access_utils.FileReadHandler(file_path=file_path, file_handle=file_handle, access_mode="r") as f:
            for line in f:
                md5gen.update(line)
                num_rows += 1

        self.structure.num_rows = num_rows
        self.MD5_checksum = md5gen.hexdigest()

    @transaction.atomic
    def register_file(self, file_path, file_handle=None):
        """
        Save and register a new file for this Dataset.

        Compute and set the MD5.

        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.

        INPUTS
        file_path           file to upload as the new contents
        file_handle         file handle of the file to upload as the new contents.
                            If supplied, then does not reopen the file in file_path.
                            Moves handle to beginning of file before calculating MD5.
                            If None, then opens the file in file_path.

        PRE
        self must not have a file already associated
        """
        assert not bool(self.dataset_file)

        with file_access_utils.FileReadHandler(file_path=file_path, file_handle=file_handle, access_mode="r") as f:
            self.dataset_file.save(os.path.basename(f.name), File(f))

        self.clean()
        self.save()

    def mark_missing(self, start_time, end_time, execlog, checking_user):
        """Mark a Dataset as missing output.

        INPUTS
        start_time      time when we started checking for the file
        end_time        time when check for file finished
        execlog         ExecLog of execution which did not produce
                        output
        checking_user   user that discovered the missing output
        """
        ccl = self.content_checks.create(start_time=start_time, end_time=end_time, execlog=execlog, user=checking_user)
        ccl.add_missing_output()
        return ccl

    @classmethod
    def create_empty(cls, user=None, cdt=None, users_allowed=None, groups_allowed=None,
                     file_source=None, instance=None):
        """Create an empty Dataset.

        INPUTS
        cdt   CompoundDatatype for the new Dataset
                            (None indicates a raw Dataset)
        instance            None or a Dataset to fill in (e.g. if we get a dummy one from DatasetForm)

        OUTPUTS
        empty_SD            Dataset with a blank MD5 and an
                            appropriate DatasetStructure
        """
        users_allowed = users_allowed or []
        groups_allowed = groups_allowed or []

        if user is None:
            assert file_source is not None
            user = file_source.top_level_run.user
            users_allowed = file_source.top_level_run.users_allowed.all()
            groups_allowed = file_source.top_level_run.groups_allowed.all()
        elif file_source is not None:
            assert user == file_source.top_level_run.user
            assert set(users_allowed) == set(file_source.top_level_run.users_allowed.all())
            assert set(groups_allowed) == set(file_source.top_level_run.groups_allowed.all())

        with transaction.atomic():
            empty_SD = instance or cls()
            empty_SD.user = user
            empty_SD.MD5_checksum = ""
            empty_SD.dataset_file = None
            empty_SD.file_source = file_source
            # Save so we can add structure and permissions.
            empty_SD.save()

            if cdt:
                empty_SD.create_structure(cdt)

            for user in users_allowed:
                empty_SD.users_allowed.add(user)
            for group in groups_allowed:
                empty_SD.groups_allowed.add(group)
            empty_SD.clean()

        return empty_SD

    @classmethod
    # FIXME what does it do for num_rows when file_path is unset?
    def create_dataset(cls, file_path, user=None, users_allowed=None, groups_allowed=None, cdt=None, keep_file=True,
                       name=None, description=None, file_source=None, check=True, file_handle=None,
                       instance=None, externalfiledirectory=None):
        """
        Helper function to make defining SDs and Datasets faster.

        user and name must both be set if make_dataset=True.
        make_dataset creates a Dataset from the given file path to go
        with the SD. file_source can be a RunAtomic to register the
        Dataset with, or None if it was uploaded by the user (or if
        make_dataset=False). If check is True, do a ContentCheck on the
        file.  file_path is an absolute path; if externalfiledirectory
        is specified, file_path will be checked to ensure that it's
        inside the specified directory.

        Returns the Dataset created.
        """
        users_allowed = users_allowed or []
        groups_allowed = groups_allowed or []

        if user is None:
            assert file_source is not None
            user = file_source.top_level_run.user
            users_allowed = file_source.top_level_run.users_allowed.all()
            groups_allowed = file_source.top_level_run.groups_allowed.all()
        elif file_source is not None:
            assert user == file_source.top_level_run.user
            assert set(users_allowed) == set(file_source.top_level_run.users_allowed.all())
            assert set(groups_allowed) == set(file_source.top_level_run.groups_allowed.all())

        if file_path:
            LOGGER.debug("Creating Dataset from file {}".format(file_path))
            file_name = file_path
        elif file_handle:
            LOGGER.debug("Creating Dataset from file {}".format(file_handle.name))
            file_name = file_handle.name
        else:
            raise ValueError("Must supply either the file path or file handle")

        with transaction.atomic():
            external_path = ""
            # We do this in the transaction because we're accessing ExternalFileDirectory.
            if externalfiledirectory:
                # Check that file_path is in the specified ExternalFileDirectory.
                normalized_path = os.path.normpath(file_path)
                normalized_efd_with_slash = "{}/".format(os.path.normpath(externalfiledirectory.path))
                assert normalized_path.startswith(normalized_efd_with_slash)
                external_path = normalized_path.replace(normalized_efd_with_slash, "", 1)

            new_dataset = cls.create_empty(user, cdt=cdt,
                                           users_allowed=users_allowed, groups_allowed=groups_allowed,
                                           instance=instance, file_source=file_source)

            new_dataset.name = name or ""
            new_dataset.description = description or ""
            new_dataset.externalfiledirectory = externalfiledirectory
            new_dataset.external_path = external_path

            if new_dataset.is_raw():
                new_dataset.set_MD5(file_path, file_handle)
            else:
                new_dataset.set_MD5_and_count_rows(file_path, file_handle)

            if cdt is not None and check:
                run_dir = tempfile.mkdtemp(
                    prefix="SD{}_".format(new_dataset.pk),
                    dir=file_access_utils.sandbox_base_path()
                )
                file_access_utils.configure_sandbox_permissions(run_dir)

                content_check = new_dataset.check_file_contents(
                    file_path_to_check=file_path,
                    file_handle=file_handle,
                    summary_path=run_dir,
                    min_row=None,
                    max_row=None,
                    execlog=None,
                    checking_user=user
                )
                shutil.rmtree(run_dir)
                if content_check.is_fail():
                    if content_check.baddata.bad_header:
                        raise ValueError('The header of file "{}" does not match the CompoundDatatype "{}"'
                                         .format(file_name, cdt))
                    elif content_check.baddata.cell_errors.exists():
                        error = content_check.baddata.cell_errors.first()
                        cdtm = error.column
                        if error.has_blank_error():
                            raise ValueError(
                                'Entry ({},{}) of file "{}" is blank.'.format(
                                    error.row_num, cdtm.column_idx, file_name)
                            )
                        else:
                            raise ValueError(
                                'The entry at row {}, column {} of file "{}" did not pass the constraints of '
                                'Datatype "{}"'.format(error.row_num, cdtm.column_idx, file_name, cdtm.datatype)
                            )
                    else:
                        # Shouldn't reach here.
                        raise ValueError('The file "{}" was malformed'.format(file_name))
                LOGGER.debug("Read {} rows from file {}".format(new_dataset.structure.num_rows, file_name))

            if keep_file:
                new_dataset.register_file(file_path=file_path, file_handle=file_handle)

            new_dataset.clean()
            if not new_dataset.is_raw():
                new_dataset.structure.save()
            new_dataset.save()
        return new_dataset

    @classmethod
    # FIXME what does it do for num_rows when file_path is unset?
    def create_dataset_bulk(cls, csv_file_path, user, users_allowed=None, groups_allowed=None, csv_file_handle=None,
                            cdt=None, keep_files=True, file_source=None, check=True):
        """
        Helper function to make defining multiple SDs and Datasets faster.
        Instead of specifying datasets one by one,
        specify multiple datasets in a CSV.

        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.

        The CSV must have these columns, not necessarily in this order:
        - Name
        - Description
        - File

        make_dataset creates a Dataset from the given file path to go
        with the SD. file_source can be a RunAtomic to register the
        Dataset with, or None if it was uploaded by the user (or if
        make_dataset=False). If check is True, do a ContentCheck on the
        file.  If this fails, then a ValueError is raised and no changes
        are made to the database.

        Returns the Dataset created.
        :rtype : object
        :param csv_file_path:  path to csv file.  Not used if csv_file_handle supplied.
        :param csv_file_handle:  file handle of csv.  If supplied, then does not
            reopen file and moves handle to beginning of file. If None, then
            uses csv_file_path.
        :param cdt:
        :param keep_files:
        :param user:
        :param file_source:
        :param check:
        """
        new_datasets = []
        if csv_file_path:
            LOGGER.debug("Creating Datasets from csv {}".format(csv_file_path))
        elif csv_file_handle:
            LOGGER.debug("Creating Datasets from csv {}".format(csv_file_handle.name))
        else:
            raise Exception("Must supply either the csv file path or csv file handle")

        with file_access_utils.FileReadHandler(
                file_path=csv_file_path,
                file_handle=csv_file_handle,
                access_mode='rU') as fh_datasets:
            # TODO:  this is a major db blocking call.  Can we break this up?
            try:
                with transaction.atomic():
                    line = 0
                    reader = csv.DictReader(fh_datasets)
                    for row in reader:
                        line += 1
                        name = row['Name'].strip() if row['Name'] else ""
                        desc = row['Description'].strip() if row['Description'] else ""
                        file_name = row['File'].strip() if row['File'] else ""

                        # check for empty entries:
                        if not (name and desc and file_name):
                            raise ValueError(
                                "Line " + str(line) +
                                " is invalid: Name, Description, File must be defined")

                        new_dataset = Dataset.create_dataset(
                            file_path=file_name, user=user, users_allowed=users_allowed,
                            groups_allowed=groups_allowed, cdt=cdt,
                            keep_file=keep_files, name=name, description=desc,
                            file_source=file_source, check=check
                        )

                        new_datasets.extend([new_dataset])
            except Exception, e:
                message = "Error while parsing line " + str(line) + ":\n" + str(row)
                LOGGER.exception(message)
                raise ValueError(message + ":\n" + str(e)), None, sys.exc_info()[2]

        return new_datasets

    # FIXME: use a transaction!
    # TODO: clean this up, end_time is set in too many places
    def check_file_contents(self, file_path_to_check, summary_path, min_row, max_row, execlog,
                            checking_user, file_handle=None):
        """
        Performs content check on a file, generates a CCL, and sets this
        SD's num_rows.

        Closes the file afterwards if the file source is a string file path.
        Does not close the file afterwards if file source is a file handle.

        OUTPUTS
        If SD is raw, creates a clean CCL.
        If not raw, checks the file and returns CCL with/without a
        corresponding BadData.

        PRE
        Should never be called twice on the same dataset, as
        this would overwrite num_rows to a potentially new value?

        :param str file_path_to_check:  Path to file to check. file_path_to_check not used if file_handle supplied.
        :param file file_handle: file handle of file to check.
                Moves file handle to beginning of file before checking.
                If file_handle empty, then uses file_path_to_check.
        :param summary_path:
        :param min_row:
        :param max_row:
        :param execlog:
        :rtype ContentCheckLog :
        """
        self.logger.debug("Creating clean ContentCheckLog for file {} and linking to ExecLog"
                          .format(file_path_to_check))
        ccl = self.content_checks.create(execlog=execlog, user=checking_user)
        ccl.start(save=False)

        if self.is_raw():
            ccl.stop(save=True, clean=False)
            ccl.clean()
            return ccl

        my_CDT = self.get_cdt()

        with file_access_utils.FileReadHandler(file_path=file_path_to_check,
                                               file_handle=file_handle, access_mode="rb") as f:
            csv_summary = my_CDT.summarize_CSV(f, summary_path, ccl)

        if ("bad_num_cols" in csv_summary or "bad_col_indices" in csv_summary):
            self.logger.warn("malformed header")
            ccl.add_bad_header()
            ccl.stop(save=True, clean=True)
            return ccl

        if csv_summary["num_rows"] == 0:
            self.logger.warn("file had no rows")

        csv_baddata = False
        self.structure.num_rows = csv_summary["num_rows"]
        self.structure.save()
        if max_row is not None and csv_summary["num_rows"] > max_row:
            self.logger.warn("too many rows")
            # FIXME: Do we only create these BD objects if they don't already exist?
            ccl.add_bad_num_rows()
            csv_baddata = True

        if min_row is not None and csv_summary["num_rows"] < min_row:
            self.logger.warn("too few rows")
            # FIXME: Do we only create these BD objects if they don't already exist?
            ccl.add_bad_num_rows()
            csv_baddata = True

        if "failing_cells" in csv_summary:
            self.logger.warn("cells failed datatype check")

            if not csv_baddata:
                bad_data = BadData.objects.create(contentchecklog=ccl)
                csv_baddata = True

            for row, col in csv_summary["failing_cells"]:
                fails = csv_summary["failing_cells"][(row, col)]
                for failed_constr in fails:
                    new_cell_error = bad_data.cell_errors.create(
                        row_num=row,
                        column=my_CDT.members.get(column_idx=col))

                    if failed_constr == metadata.models.CompoundDatatypeMember.BLANK_ENTRY:
                        blank_cell = datachecking.models.BlankCell(cellerror=new_cell_error)
                        blank_cell.save()
                    # If failure is a string (Ex: "Was not integer"), leave constraint_failed as null.
                    elif not isinstance(failed_constr, basestring):
                        new_cell_error.constraint_failed = failed_constr

                    new_cell_error.clean()
                    new_cell_error.save()

        if csv_baddata:
            self.logger.debug(
                "Content check failed - file {} does not conform to Dataset {}".
                format(file_path_to_check, self))
            self._quarantine_runcomponents()
        else:
            self.logger.debug(
                "Content check passed - file {} conforms to Dataset {}".
                format(file_path_to_check, self))
        ccl.stop(save=True, clean=True)
        return ccl

    def check_integrity(self, new_file_path, checking_user, execlog=None, runsic=None,
                        newly_computed_MD5=None, notify_all=True):
        """
        Checks integrity of SD against the md5 provided (newly_computed_MD5),
        or in it's absence, the MD5 computed from new_file_path.

        OUTPUT
        Returns the ICL.
        """
        # RL February 6: I'm choosing this to be the time of the "start" of the
        # check, but it does raise the question: what exactly is the start and
        # end time of an integrity check?  Is the check just the comparison
        # of the MD5s or is it the time that you finish computing the MD5 or
        # is it the time that you start computing the MD5?
        icl = self.integrity_checks.create(execlog=execlog, runsic=runsic, user=checking_user)
        icl.start(save=False)

        if newly_computed_MD5 is None:
            with open(new_file_path, "rb") as f:
                newly_computed_MD5 = file_access_utils.compute_md5(f)

        if newly_computed_MD5 != self.MD5_checksum:
            self.logger.warn("md5s do not agree (old: {}; new: {})".format(self.MD5_checksum, newly_computed_MD5))

            # June 4, 2014: this evil_twin should be a raw SD -- we don't really care what it contains,
            # just that it conflicted with the existing one.
            evil_twin = Dataset.create_dataset(file_path=new_file_path, user=checking_user, cdt=None,
                                               description="MD5 conflictor of {}".format(self),
                                               name="{}eviltwin".format(self))

            note_of_usurping = datachecking.models.MD5Conflict(integritychecklog=icl, conflicting_dataset=evil_twin)
            note_of_usurping.save()

            if notify_all:
                self._quarantine_runcomponents()

        icl.stop(save=True, clean=True)
        return icl

    def _quarantine_runcomponents(self):
        """
        Mark RunComponents that use this as an output as failed.
        """
        # if self.has_data() and self.file_source is not None:
        if self.file_source is not None:
            self.file_source.execrecord.quarantine_runcomponents()

    def is_OK(self):
        """
        Check that this Dataset has passed a check for contents if not raw,
        and it has never failed any check for integrity or contents.

        Redacted Datasets are not considered OK.
        """
        if self.is_redacted():
            return False

        # Check for any failures.
        if self.any_failed_checks():
            return False

        # If this is not raw, check that there is at least one content check completed and
        # successful.
        if self.is_raw():
            return True

        for ccl in self.content_checks.all():
            if ccl.is_complete():
                return True

        self.logger.debug("Dataset '{}' may not be OK - no content check performed".format(self))
        return False

    def any_failed_checks(self):
        """ Checks if any integrity or content checks failed. """
        if self.integrity_checks.filter(usurper__isnull=False).exists():
            self.logger.debug("Dataset '{}' failed integrity check".format(self))
            return True

        if self.content_checks.filter(baddata__isnull=False).exists():
            self.logger.debug("Dataset '{}' failed content check".format(self))
            return True

        return False

    @transaction.atomic
    def build_redaction_plan(self, redaction_accumulator=None):
        """
        Create a list of what will be affected when redacting this Dataset.
        """
        redaction_plan = redaction_accumulator or archive.models.empty_redaction_plan()
        assert self not in redaction_plan["Datasets"]
        if self.is_redacted():
            return redaction_plan
        redaction_plan["Datasets"].add(self)

        # Make a special note if this Dataset is associated with an external file.
        if self.external_path:
            redaction_plan["ExternalFiles"].add(self)

        # Mark anything that was produced from this Dataset for redaction.
        for used_as_input in self.execrecordins.all().select_related("execrecord"):
            if used_as_input.execrecord not in redaction_plan["ExecRecords"]:
                metadata.models.update_removal_plan(
                    redaction_plan, used_as_input.execrecord.build_redaction_plan(redaction_plan)
                )

        return redaction_plan

    @transaction.atomic
    def redact_this(self):
        """
        Helper function that only redacts this Dataset and does not handle any recursion.
        """
        if self.is_redacted():
            return

        self._redacted = True
        self.MD5_checksum = ""
        self.externalfiledirectory = None
        if self.external_path:
            self.external_path = ""
        self.save(update_fields=["_redacted", "MD5_checksum", "externalfiledirectory", "external_path"])

        if bool(self.dataset_file):
            self.dataset_file.delete(save=True)
        if self.has_structure():
            self.structure.delete()

    @transaction.atomic
    def redact(self):
        redaction_plan = self.build_redaction_plan()
        archive.models.redact_helper(redaction_plan)

    def is_redacted(self):
        return self._redacted

    @transaction.atomic
    def build_removal_plan(self, removal_accumulator=None):
        """
        Make a manifest of objects to remove when removing this Dataset.
        """
        removal_plan = removal_accumulator or metadata.models.empty_removal_plan()
        assert self not in removal_plan["Datasets"]
        removal_plan["Datasets"].add(self)

        # Make a special note if this Dataset is associated with an external file.
        if self.external_path:
            removal_plan["ExternalFiles"].add(self)

        for er_xput in itertools.chain(self.execrecordins.all(), self.execrecordouts.all()):
            curr_ER = er_xput.execrecord
            if curr_ER not in removal_plan["ExecRecords"]:
                metadata.models.update_removal_plan(removal_plan, curr_ER.build_removal_plan(removal_plan))

        return removal_plan

    @transaction.atomic
    def remove(self):
        removal_plan = self.build_removal_plan()
        metadata.models.remove_helper(removal_plan)

    @classmethod
    def purge(cls,
              max_storage=settings.DATASET_MAX_STORAGE,
              target=settings.DATASET_TARGET_STORAGE):

        files = []  # [(date, path, filesize)]
        start_path = os.path.join(settings.MEDIA_ROOT, cls.UPLOAD_DIR)
        total_size = 0
        skipped_count = 0
        for dirpath, _dirnames, filenames in os.walk(start_path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                filedate = os.path.getmtime(filepath)
                filesize = os.path.getsize(filepath)
                relpath = os.path.relpath(filepath, settings.MEDIA_ROOT)
                total_size += filesize
                heapq.heappush(files, (filedate, relpath, filesize))
        if total_size >= max_storage:
            cls.logger.info('Dataset purge triggered at %s over %d files.',
                            filesizeformat(total_size),
                            len(files))
            active_runs = archive.models.Run.objects.filter(end_time=None)

            def active_datasets(active_runs):
                for run in active_runs:
                    for component in run.get_all_atomic_runcomponents():
                        execrecord = component.execrecord
                        if execrecord is not None:
                            for eri in execrecord.execrecordins.all():
                                yield eri.dataset
                            for ero in execrecord.execrecordouts.all():
                                yield ero.dataset
            active_dataset_ids = set(d.id for d in active_datasets(active_runs))
            cls.logger.debug('Found %d active datasets.',
                             len(active_dataset_ids))
            while total_size > target and files:
                filedate, relpath, filesize = heapq.heappop(files)
                dataset = Dataset.objects.filter(dataset_file=relpath).first()
                if dataset is None:
                    filepath = os.path.join(settings.MEDIA_ROOT, relpath)
                    filedate = os.path.getmtime(filepath)
                    file_age = datetime.now() - datetime.fromtimestamp(filedate)
                    if file_age < timedelta(hours=1):
                        skipped_count += 1
                    else:
                        cls.logger.warn('No dataset matches file %r, deleting it.',
                                        relpath)
                        os.remove(filepath)
                        total_size -= filesize
                else:
                    if dataset.file_source is None:
                        is_skipped = True  # it was uploaded, not created
                    else:
                        # Check to see if it's being used by an active run.
                        is_skipped = dataset.id in active_dataset_ids

                    if is_skipped:
                        skipped_count += 1
                    else:
                        dataset.dataset_file.delete(save=True)
                        total_size -= filesize

            remaining = 'Leaving {} over {} files.'.format(
                filesizeformat(total_size),
                len(files) + skipped_count)
            if total_size > max_storage:
                target = max_storage
                log_method = cls.logger.error
            elif total_size > target:
                target = target
                log_method = cls.logger.warn
            else:
                target = None
                log_method = cls.logger.info
            if target:
                message = 'Cannot purge datasets below {}. {}'.format(
                    filesizeformat(target),
                    remaining)
            else:
                message = 'Dataset purge finished. ' + remaining
            message = message.replace('\xa0', ' ')
            log_method(message)
            if log_method == cls.logger.error:
                raise RuntimeError(message)

    def increase_permissions_from_json(self, permissions_json):
        """
        Grant permission to all users and groups specified in the parameter.

        The permissions_json parameter should be a JSON string formatted as it would
        be by the permissions widget used in the UI.
        """
        self.grant_from_json(permissions_json)

        for ic in self.integrity_checks.all():
            if ic.is_fail():
                ic.usurper.conflicting_dataset.increase_permissions_from_json(permissions_json)


class DatasetStructure(models.Model):
    """
    Data with a Shipyard-compliant structure: a CSV file with a header.
    Encodes the CDT, and the transformation output generating this data.

    PRECONDITION
    Any Dataset that represents a CSV file has to have confirmed using
    summarize_CSV() that the CSV file is coherent.
    """
    # Note: previously we were tracking the exact TransformationOutput
    # this came from (both for its Run and its RunStep) but this is
    # now done more cleanly using ExecRecord.
    dataset = models.OneToOneField(Dataset, related_name="structure")
    compounddatatype = models.ForeignKey("metadata.CompoundDatatype", related_name="conforming_datasets")

    # A value of -1 means the file is missing or num rows has never been counted
    num_rows = models.IntegerField("number of rows", validators=[MinValueValidator(-1)], default=-1)

    def clean(self):
        self.dataset.validate_restrict_access([self.compounddatatype])


@python_2_unicode_compatible
class ExecRecord(models.Model):
    """
    Record of a previous execution of a Pipeline component.
    """
    generator = models.OneToOneField("archive.ExecLog", related_name="execrecord")

    # FIXME exactly one of these must be non-null

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """Unicode representation of this ExecRecord."""
        inputs_list = [unicode(eri) for eri in self.execrecordins.all()]
        outputs_list = [unicode(ero) for ero in self.execrecordouts.all()]

        if type(self.general_transf()) == method.models.Method:
            return "{}({}) = ({})".format(
                    self.general_transf(),
                    ", ".join(inputs_list),
                    ", ".join(outputs_list))
        else:
            # Return a representation for a cable.
            return ("{}".format(", ".join(inputs_list)) +
                    " ={" + "{}".format(self.general_transf()) + "}=> " +
                    "{}".format(", ".join(outputs_list)))

    @property
    def execrecordins_in_order(self):
        return sorted(self.execrecordins.all(), key=lambda e: e.generic_input.definite.dataset_idx)

    @property
    def execrecordouts_in_order(self):
        return sorted(self.execrecordouts.all(), key=lambda e: e.generic_output.definite.dataset_idx)

    @classmethod
    @transaction.atomic
    def create(cls, generator, component, input_SDs, output_SDs):
        """Create a complete ExecRecord, including inputs and outputs.

        INPUTS
        generator       ExecLog generating this ExecRecord
        component       Pipeline component the ExecRecord is for (a
                        PipelineStep, PipelineOutputCable, or
                        PipelineStepInputCable)
        input_SDs       list of Datasets input to the component
                        during execution, in order of their index
        output_SDs      list of Datasets output by the component
                        during execution, in order of their index
        """
        execrecord = cls(generator=generator)
        # execrecord.clean()
        execrecord.save()
        for i, component_input in enumerate(component.inputs):
            execrecord.execrecordins.create(generic_input=component_input, dataset=input_SDs[i])
        for i, component_output in enumerate(component.outputs):
            execrecord.execrecordouts.create(generic_output=component_output, dataset=output_SDs[i])
        execrecord.complete_clean()
        return execrecord

    def get_execrecordout(self, xput):
        """Get the ExecRecordOut for a TransformationXput.

        INPUTS
        xput        TransformationXput to get ExecRecordOut for
        """
        try:
            return self.execrecordouts.get(generic_output=xput)
        except ExecRecordOut.DoesNotExist:
            return None

    def clean(self):
        """
        Checks coherence of the ExecRecord.

        Calls clean on all of the in/outputs.  (Multiple quenching is
        checked via a uniqueness condition and does not need to be
        coded here.)

        If this ER represents a trivial cable, then the single ERI and
        ERO should have the same Dataset.
        """
        eris = self.execrecordins.all()
        eros = self.execrecordouts.all()

        for eri in eris:
            eri.clean()
        for ero in eros:
            ero.clean()

        # Check that the permissions on the generating Run do not exceed those of the inputs.
        # (That the output permissions are the same as the generating Run will be checked
        # by the output Datasets themselves.)
        input_SDs = [x.dataset for x in self.execrecordouts.all()]
        self.generating_run.validate_restrict_access(input_SDs)

        if not isinstance(self.general_transf(), method.models.Method):
            # If the cable is quenched:
            if eris.exists() and eros.exists():

                # If the cable is trivial, then the ERI and ERO should
                # have the same Dataset (if they both exist).
                if self.general_transf().is_trivial():
                    if eris[0].dataset != eros[0].dataset:
                        raise ValidationError(('ExecRecord "{}" represents a trivial cable but its input and output '
                                               'do not match').format(self))

                # From this point on we can't proceed if the ExecRecord is redacted.
                if self.is_redacted():
                    return

                # If the cable is not trivial and both sides have
                # data, then the column *Datatypes* on the destination
                # side are the same as the corresponding column on the
                # source side.  For example, if a CDT like (DNA col1,
                # int col2) is fed through a cable that maps col1 to
                # produce (string foo), then the actual Datatype of
                # the column in the corresponding Dataset would be
                # DNA.

                # Note that because the ERI and ERO are both clean,
                # and because we checked general_transf is not
                # trivial, we know that both have well-defined
                # DatasetStructures.
                elif not self.general_transf().is_trivial():
                    cable_wires = self.general_transf().custom_wires.all()

                    source_CDT = eris[0].dataset.structure.compounddatatype
                    dest_CDT = eros[0].dataset.structure.compounddatatype

                    for wire in cable_wires:
                        source_idx = wire.source_pin.column_idx
                        dest_idx = wire.dest_pin.column_idx

                        dest_dt = dest_CDT.members.get(column_idx=dest_idx).datatype
                        source_dt = source_CDT.members.get(column_idx=source_idx).datatype

                        if source_dt != dest_dt:
                            raise ValidationError(
                                ('ExecRecord "{}" represents a cable, but the Datatype '
                                 'of its destination column, "{}", does not match the Datatype '
                                 'of its source column, "{}"').format(self, dest_dt, source_dt)
                            )

    def complete_clean(self):
        """
        Checks completeness of the ExecRecord.

        Calls clean, and then checks that all in/outputs of the
        Method/POC/PSIC are quenched.
        """
        self.clean()

        # Because we know that each ERI is clean (and therefore each
        # one maps to a valid input of our Method/POC/PSIC), and
        # because there is no multiple quenching (due to a uniqueness
        # constraint), all we have to do is check the number of ERIs
        # to make sure everything is quenched.
        if type(self.general_transf()) in (
                pipeline.models.PipelineOutputCable,
                pipeline.models.PipelineStepInputCable
                ):
            # In this case we check that there is an input and an output.
            if not self.execrecordins.all().exists():
                raise ValidationError(
                    "Input to ExecRecord \"{}\" is not quenched".format(self))
            if not self.execrecordouts.all().exists():
                raise ValidationError(
                    "Output of ExecRecord \"{}\" is not quenched".format(self))

        else:
            if self.execrecordins.count() != self.general_transf().inputs.count():
                raise ValidationError(
                    "Input(s) to ExecRecord \"{}\" are not quenched".format(self))

            # Similar for EROs.
            if self.execrecordouts.count() != self.general_transf().outputs.count():
                raise ValidationError(
                    "Output(s) of ExecRecord \"{}\" are not quenched".format(self))

    def general_transf(self):
        """Returns the Method/POC/PSIC represented by this ExecRecord."""
        if self.generator.record.is_cable:
            return self.generator.record.component
        else:
            # This is a Method.
            return self.generator.record.component.transformation.definite

    @property
    def generating_run(self):
        return self.generator.record.top_level_run

    def provides_outputs(self, outputs):
        """
        Checks whether this ER has existent data for these outputs.
        outputs: an iterable of TOs we want the ER to have real data for.

        PRE
        1) outputs must be TransformationOutputs of the Transformation associated
        with the RunStep/RunSIC/RunOutputCable associated with this ExecRecord
        (they cannot be arbitrary TransformationOutputs).
        """
        # Load each TO in outputs
        for curr_output in outputs:
            corresp_ero = self.execrecordouts.get(generic_output=curr_output)

            if not corresp_ero.has_data():
                self.logger.debug(
                    "corresponding ERO doesn't have data - ER doesn't have existent data for all TOs requested")
                return False

        self.logger.debug("all outputs needed have corresponding EROs with data")
        return True

    def outputs_OK(self):
        """Checks whether all of the EROs of this ER are OK."""
        return all([ero.is_OK() for ero in self.execrecordouts.all()])

    def outputs_failed_any_checks(self):
        """
        Checks whether any of the EROs of this ER have ever failed any checks.
        """
        return any([ero.dataset.any_failed_checks() for ero in self.execrecordouts.all()])

    def has_ever_failed(self, use_cache=True):
        """Has any execution of this ExecRecord ever failed?"""
        # Go through all RunSteps using this ExecRecord.
        run_components = self.used_by_components.exclude(
            reused=True).filter(runstep__isnull=False)
        for component_using_this in run_components:
            if not component_using_this.runstep.is_successful(use_cache=use_cache):
                return True
        return False

    def is_redacted(self):
        for eri in self.execrecordins.all().select_related("dataset"):
            if eri.dataset.is_redacted():
                return True
        for ero in self.execrecordouts.all().select_related("dataset"):
            if ero.dataset.is_redacted():
                return True

        return self.generator.is_redacted()

    @transaction.atomic
    def build_redaction_plan(self, redaction_accumulator=None):
        redaction_plan = redaction_accumulator or archive.models.empty_redaction_plan()
        assert self not in redaction_plan["ExecRecords"]
        if self.is_redacted():
            return redaction_plan
        redaction_plan["ExecRecords"].add(self)

        metadata.models.update_removal_plan(redaction_plan, self.generator.build_redaction_plan())

        for ero in self.execrecordouts.exclude(dataset___redacted=True).select_related("dataset"):
            # If any of these are already redacted, this call will simply do nothing.
            if ero.dataset not in redaction_plan["Datasets"]:
                metadata.models.update_removal_plan(
                    redaction_plan, ero.dataset.build_redaction_plan(redaction_plan)
                )

        return redaction_plan

    @transaction.atomic
    def redact_this(self):
        # Redact components that used this ExecRecord, and purge any sandboxes that still exist.
        runs_to_purge = set()
        for rc in self.used_by_components.all():
            rc.redact()
            if rc.top_level_run not in runs_to_purge:
                runs_to_purge.add(rc.top_level_run)

        for run in runs_to_purge:
            try:
                if not run.purged:
                    run.collect_garbage()
            except ObjectDoesNotExist:
                # No RunToProcess exists.
                pass
            except archive.exceptions.SandboxActiveException as e:
                # The Run never started or hasn't finished.
                self.logger.warning(e)
            except OSError as e:
                # The sandbox could not be removed.
                self.logger.warning(e)

    @transaction.atomic
    def redact(self):
        """
        "Hollow out" this ExecRecord.

        This may be triggered by an input Dataset or by the ExecLog.
        """
        redaction_plan = self.build_redaction_plan()
        archive.models.redact_helper(redaction_plan)

    @transaction.atomic
    def build_removal_plan(self, removal_accumulator=None):
        """
        Creates a manifest of objects that will be removed if this ExecRecord is removed.
        """
        removal_plan = removal_accumulator or metadata.models.empty_removal_plan()
        assert self not in removal_plan["ExecRecords"]
        removal_plan["ExecRecords"].add(self)

        if not (self.generator.record.is_cable and self.general_transf().is_trivial()):
            for ero in self.execrecordouts.exclude(
                    dataset__in=removal_plan["Datasets"]).select_related("dataset"):
                if ero.dataset not in removal_plan["Datasets"]:
                    metadata.models.update_removal_plan(
                        removal_plan, ero.dataset.build_removal_plan(removal_plan)
                    )

        for rc in self.used_by_components.all():
            if rc.top_level_run not in removal_plan["Runs"]:
                metadata.models.update_removal_plan(removal_plan, rc.top_level_run.build_removal_plan(removal_plan))

        return removal_plan

    @transaction.atomic
    def remove(self):
        removal_plan = self.build_removal_plan()
        metadata.models.remove_helper(removal_plan)

    def quarantine_runcomponents(self):
        """
        Quarantine RunComponents that used this ExecRecord.
        """
        for rc in self.used_by_components.filter(_state__pk=runcomponentstates.SUCCESSFUL_PK):
            rc.quarantine()


@python_2_unicode_compatible
class ExecRecordIn(models.Model):
    """
    Denotes an input fed to the Method/POC/PSIC in the parent ExecRecord.

    The input may map to deleted data, e.g. if it was a deleted output
    of a previous step in a pipeline.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord", related_name="execrecordins")
    dataset = models.ForeignKey(Dataset, help_text="Dataset fed to this input",
                                related_name="execrecordins")

    # For a Method/Pipeline, this denotes the input that this ERI refers to;
    # for a cable, this denotes the thing that "feeds" it.
    generic_input = models.ForeignKey(transformation.models.TransformationXput)

    class Meta:
        unique_together = ("execrecord", "generic_input")

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """
        Unicode representation.

        If this ERI represents the source of a POC/PSIC, then it looks like
        [dataset]
        If it represents a TI, then it looks like
        [dataset]=>[transformation (raw) input name]

        Examples:
        S552
        S552=>foo_bar

        PRE: the parent ER must exist and be clean.
        """
        if (type(self.execrecord.general_transf()) in
                (pipeline.models.PipelineOutputCable,
                 pipeline.models.PipelineStepInputCable)):
            return str(self.dataset)
        else:
            dest_name = self.generic_input.definite.dataset_name
            return "{}=>{}".format(self.dataset, dest_name)

    def clean(self):
        """
        Checks coherence of this ExecRecordIn.

        Checks that generic_input is appropriate for the parent
        ExecRecord's Method/POC/PSIC.
        - If execrecord is for a POC, then generic_input should be the TO that
          feeds it (i.e. the PipelineStep TO that is cabled to a Pipeline output).
        - If execrecord is for a PSIC, then generic_input should be the TO or TI
          that feeds it (TO if it's from a previous step; TI if it's from a Pipeline
          input).
        - If execrecord is for a Method, then generic_input is the TI
          that this ERI represents.

        Also, if dataset refers to existent data, check that it
        is compatible with the input represented.
        """
        # Check that the input is accessible by the generating run.
        self.execrecord.generating_run.validate_restrict_access([self.dataset])

        parent_transf = self.execrecord.general_transf()

        # If ER links to POC, ERI must link to TO which the outcable runs from.
        if type(parent_transf) == pipeline.models.PipelineOutputCable:
            if self.generic_input.definite != parent_transf.source.definite:
                raise ValidationError(
                    'ExecRecordIn "{}" does not denote the TO that feeds the parent ExecRecord POC'.
                    format(self))
        # Similarly for a PSIC.
        elif type(parent_transf) == pipeline.models.PipelineStepInputCable:
            if self.generic_input.definite != parent_transf.source.definite:
                raise ValidationError(
                    'ExecRecordIn "{}" does not denote the TO/TI that feeds the parent ExecRecord PSIC'.
                    format(self))

        else:
            # The ER represents a Method (not a cable).  Therefore the
            # ERI must refer to a TI of the parent ER's Method.
            if type(self.generic_input) == transformation.models.TransformationOutput:
                raise ValidationError(
                    'ExecRecordIn "{}" must refer to a TI of the Method of the parent ExecRecord'.
                    format(self))

            transf_inputs = parent_transf.inputs
            if not transf_inputs.filter(pk=self.generic_input.pk).exists():
                raise ValidationError(
                    'Input "{}" does not belong to Method of ExecRecord "{}"'.
                    format(self.generic_input, self.execrecord))

        # If the SD is redacted, we return -- the rest is not applicable.
        if self.dataset.is_redacted():
            return

        # The ERI's Dataset raw/unraw state must match the
        # raw/unraw state of the generic_input that feeds it (if ER is a cable)
        # or that it is fed into (if ER is a Method).
        # self.dataset = librarian.models.Dataset.objects.get(pk=self.dataset.pk)
        # self.generic_input = transformation.models.TransformationXput.objects.get(pk=self.generic_input.pk)
        if self.generic_input.is_raw() != self.dataset.is_raw():
            sd_raw_str = "raw" if self.dataset.is_raw() else "non-raw"
            gi_raw_str = "raw" if self.generic_input.is_raw() else "non-raw"
            raise ValidationError(
                'Dataset "{}" ({}) cannot feed source "{}" ({})'.
                format(self.dataset, sd_raw_str, self.generic_input, gi_raw_str))

        if not self.dataset.is_raw():
            transf_xput_used = self.generic_input
            cdt_needed = self.generic_input.get_cdt()
            input_SD = self.dataset

            # CDT of input_SD must be a restriction of cdt_needed,
            # i.e. we can feed it into cdt_needed.
            if not input_SD.structure.compounddatatype.is_restriction(
                    cdt_needed):
                raise ValidationError(
                    'CDT of Dataset "{}" is not a restriction of the required CDT'.
                    format(input_SD))

            # Check row constraints.
            if (transf_xput_used.get_min_row() is not None and
                    input_SD.num_rows() < transf_xput_used.get_min_row()):
                error_str = ""
                if type(self.generic_input) == transformation.models.TransformationOutput:
                    error_str = 'Dataset "{}" has too few rows to have come from TransformationOutput "{}"'
                else:
                    error_str = 'Dataset "{}" has too few rows for TransformationInput "{}"'
                raise ValidationError(error_str.format(input_SD, transf_xput_used))

            if (transf_xput_used.get_max_row() != None and
                    input_SD.num_rows() > transf_xput_used.get_max_row()):
                error_str = ""
                if type(self.generic_input) == transformation.models.TransformationOutput:
                    error_str = 'Dataset "{}" has too many rows to have come from TransformationOutput "{}"'
                else:
                    error_str = 'Dataset "{}" has too many rows for TransformationInput "{}"'
                raise ValidationError(error_str.format(input_SD, transf_xput_used))

    def is_OK(self):
        """Checks if the associated Dataset is OK."""
        return self.dataset.is_OK()


@python_2_unicode_compatible
class ExecRecordOut(models.Model):
    """
    Denotes an output from the Method/PSIC/POC in the parent ExecRecord.

    The output may map to deleted data, i.e. if it was deleted after
    being generated.
    """
    execrecord = models.ForeignKey(ExecRecord, help_text="Parent ExecRecord",
                                   related_name="execrecordouts")

    dataset = models.ForeignKey(
        Dataset,
        help_text="Dataset coming from this output",
        related_name="execrecordouts"
    )

    # For a Method/Pipeline this represents the TO that produces this output.
    # For a cable, this represents the TO (for a POC) or TI (for a PSIC) that
    # this cable feeds into.
    generic_output = models.ForeignKey(transformation.models.TransformationXput,
                                       related_name="execrecordouts_referencing")

    class Meta:
        unique_together = ("execrecord", "generic_output")

    def __init__(self, *args, **kwargs):
        super(self.__class__, self).__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)

    def __str__(self):
        """
        Unicode representation of this ExecRecordOut.

        If this ERO represented the output of a PipelineOutputCable, then this looks like
        [dataset]
        If it represents the input that a PSIC feeds into, then it looks like
        [dataset]
        Otherwise, it represents a TransformationOutput, and this looks like
        [TO name]=>[dataset]
        e.g.
        S458
        output_one=>S458
        """
        if (type(self.execrecord.general_transf()) in
                (pipeline.models.PipelineOutputCable,
                 pipeline.models.PipelineStepInputCable)):
            return str(self.dataset)
        else:
            return "{}=>{}".format(self.generic_output.definite.dataset_name, self.dataset)

    def clean(self):
        """
        - If the ExecRecord represents a PipelineOutputCable, check
          that the output is the one defined by the PipelineOutputCable.
        - If the ExecRecord represents a PipelineStepInputCable, check
          that the output is the TransformationInput that the cable feeds.
        - If the ExecRecord is not for a cable, check that the output
          belongs to the ExecRecord's Method.
        - The Dataset is compatible with generic_output. (??)
        """
        # Runs must not increase permissions on an input.
        self.execrecord.generating_run.validate_restrict_access(
            [self.dataset])

        # If the parent ER is linked with POC, the corresponding ERO TO must be coherent
        if isinstance(self.execrecord.general_transf(), pipeline.models.PipelineOutputCable):
            parent_er_outcable = self.execrecord.general_transf()

            # ERO TO must belong to the same pipeline as the ER POC
            if self.generic_output.definite.transformation.definite != parent_er_outcable.pipeline:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not belong to the same pipeline as its parent ExecRecord POC".
                    format(self))

            # And the POC defined output name must match the pipeline TO name
            if parent_er_outcable.output_name != self.generic_output.definite.dataset_name:
                raise ValidationError(
                    "ExecRecordOut \"{}\" does not represent the same output as its parent ExecRecord POC".
                    format(self))

        # Second case: parent ER represents a PSIC.
        elif isinstance(self.execrecord.general_transf(), pipeline.models.PipelineStepInputCable):
            # This ERO must point to a TI.
            if not self.generic_output.is_input:
                raise ValidationError(
                    "Parent of ExecRecordOut \"{}\" represents a PSIC; ERO must be a TransformationInput".
                    format(self))

        # Else the parent ER is linked with a method
        else:
            query_for_outs = self.execrecord.general_transf().outputs

            # The ERO output TO must be a member of the ER's method/pipeline
            if not query_for_outs.filter(pk=self.generic_output.pk).exists():
                raise ValidationError(
                    "Output \"{}\" does not belong to Method/Pipeline of ExecRecord \"{}\"".
                    format(self.generic_output, self.execrecord))

        if self.dataset.is_redacted():
            # A redacted SD is fine -- the rest of the checks are inapplicable.
            return

        # Check that the SD is compatible with generic_output.

        self.logger.debug("ERO SD '{}' is raw? {}".format(
            self.dataset,
            self.dataset.is_raw()))
        self.logger.debug("ERO generic_output '{}' {} is raw? {}".format(
            self.generic_output,
            type(self.generic_output),
            self.generic_output.is_raw()))

        # If SD is raw, the ERO output TO must also be raw
        # Refresh dataset and generic_output to make sure we get the right information.
        self.dataset = librarian.models.Dataset.objects.get(pk=self.dataset.pk)
        self.generic_output = transformation.models.TransformationXput.objects.get(pk=self.generic_output.pk)
        if self.dataset.is_raw() != self.generic_output.is_raw():
            sd_raw_str = "raw" if self.dataset.is_raw() else "non-raw"
            go_raw_str = "raw" if self.generic_output.is_raw() else "non-raw"
            if type(self.generic_output) == pipeline.models.PipelineStepInputCable:
                raise ValidationError(
                    'Dataset "{}" ({}) cannot feed input "{}" ({})'.
                    format(self.dataset, sd_raw_str, self.generic_output, go_raw_str))
            else:
                raise ValidationError(
                    'Dataset "{}" ({}) cannot have come from output "{}" ({})'.
                    format(self.dataset, sd_raw_str, self.generic_output, go_raw_str))

        # SD must satisfy the CDT / row constraints of the producing TO (Methods/Pipelines/POCs)
        # or of the TI fed (PSIC case)
        if not self.dataset.is_raw():
            input_SD = self.dataset

            # If this execrecord refers to a Method, the SD CDT
            # must *exactly* be generic_output's CDT since it was
            # generated by this Method.

            if isinstance(self.execrecord.general_transf(), method.models.Method):
                if input_SD.structure.compounddatatype != self.generic_output.get_cdt():
                    raise ValidationError(
                        ('CDT of Dataset "{}" is not the CDT of the '
                         'TransformationOutput "{}" of the generating Method').
                        format(input_SD, self.generic_output))

            # For POCs, ERO SD's CDT must be >>identical<< to generic_output's CDT, because it was generated either
            # by this POC or by a compatible one.
            # FIXME: self.generic_output.get_cdt().is_restriction(self.dataset.structure.compounddatatype)

            elif isinstance(self.execrecord.general_transf(), pipeline.models.PipelineOutputCable):
                if not self.dataset.structure.compounddatatype.is_identical(self.generic_output.get_cdt()):
                    raise ValidationError(
                        "CDT of Dataset \"{}\" is not identical to the "
                        "CDT of the TransformationOutput \"{}\" of the "
                        "generating Pipeline".format(input_SD,
                                                     self.generic_output))

            # If it refers to a PSIC, then SD CDT must be a
            # restriction of generic_output's CDT.
            else:
                if not input_SD.structure.compounddatatype.is_restriction(self.generic_output.get_cdt()):
                    raise ValidationError(
                        'CDT of Dataset "{}" is not a restriction of '
                        'the CDT of the fed TransformationInput "{}"'.format(
                            input_SD,
                            self.generic_output))

            # If the input SD has a number of rows, then check that it is coherent.  (If it is -1,
            # then we can't check this.)
            if input_SD.num_rows() != -1:
                if (self.generic_output.get_min_row() != None and
                        input_SD.num_rows() < self.generic_output.get_min_row()):
                    if isinstance(self.execrecord.general_transf(), pipeline.models.PipelineStepInputCable):
                        raise ValidationError(
                            "Dataset \"{}\" feeds TransformationInput \"{}\" but has too few rows".
                            format(input_SD, self.generic_output))
                    else:
                        raise ValidationError(
                            "Dataset \"{}\" was produced by TransformationOutput \"{}\" but has too few rows".
                            format(input_SD, self.generic_output))

                if (self.generic_output.get_max_row() != None and
                        input_SD.num_rows() > self.generic_output.get_max_row()):
                    if isinstance(self.execrecord.general_transf(), pipeline.models.PipelineStepInputCable):
                        raise ValidationError(
                            "Dataset \"{}\" feeds TransformationInput \"{}\" but has too many rows".
                            format(input_SD, self.generic_output))
                    else:
                        raise ValidationError(
                            "Dataset \"{}\" was produced by TransformationOutput \"{}\" but has too many rows".
                            format(input_SD, self.generic_output))

        self.logger.debug("ERO is clean")

    def has_data(self):
        """True if associated Dataset has data; False otherwise."""
        return self.dataset.has_data()

    def is_OK(self):
        """Checks if the associated Dataset is OK."""
        return self.dataset.is_OK()


# Register signals.
post_delete.connect(librarian.signals.dataset_post_delete, sender=Dataset)
